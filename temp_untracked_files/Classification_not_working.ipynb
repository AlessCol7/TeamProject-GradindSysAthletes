{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling data and JSON files\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# For splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# For data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For pose detection\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# For softmax\n",
    "from scipy.special import softmax\n",
    "\n",
    "# For counting classes\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 2  # 17 keypoints with x, y coordinates\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < 51:  # 17 keypoints * 3 (x, y, visibility)\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract x, y coordinates only and flatten\n",
    "            normalized_keypoints = np.array(keypoints).reshape(-1, 3)[:, :2].flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Path to folder containing JSON files\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91       458\n",
      "           1       0.88      0.88      0.88       396\n",
      "           2       0.92      0.93      0.93       345\n",
      "           3       0.90      0.90      0.90       424\n",
      "           4       0.97      0.98      0.97       406\n",
      "           5       0.94      0.92      0.93       445\n",
      "           6       0.96      0.94      0.95       504\n",
      "           7       0.89      0.83      0.86       259\n",
      "           8       0.89      0.90      0.89       296\n",
      "\n",
      "    accuracy                           0.92      3533\n",
      "   macro avg       0.91      0.91      0.91      3533\n",
      "weighted avg       0.92      0.92      0.92      3533\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Split data into training and testing sets first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(exercise_labels.keys()))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping of exercise labels\n",
    "exercise_labels_inv = {v: k for k, v in exercise_labels.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_and_predict(frame, model, clf, scaler):\n",
    "    # Run the YOLO pose model\n",
    "    results = model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        return None, None  # Skip if no keypoints detected\n",
    "    \n",
    "    # Initialize variables to store the keypoints of the largest person\n",
    "    largest_area = 0\n",
    "    selected_keypoints = None\n",
    "\n",
    "    for result in results:\n",
    "        if result.keypoints is not None and result.boxes is not None and len(result.boxes) > 0:\n",
    "            keypoints = result.keypoints.xy.cpu().numpy().reshape(-1, 2)  # x, y coordinates\n",
    "            bbox = result.boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            bbox_width = x_max - x_min\n",
    "            bbox_height = y_max - y_min\n",
    "            area = bbox_width * bbox_height\n",
    "\n",
    "            if area > largest_area:\n",
    "                largest_area = area\n",
    "                selected_keypoints = keypoints.flatten()\n",
    "\n",
    "    if selected_keypoints is None:\n",
    "        return None, None\n",
    "\n",
    "    # Check if the number of keypoints matches the expected number\n",
    "    expected_num_features = scaler.n_features_in_\n",
    "    if selected_keypoints.size != expected_num_features:\n",
    "        print(f\"Expected {expected_num_features} features, but got {selected_keypoints.size} features\")\n",
    "        return None, None\n",
    "\n",
    "    # Normalize keypoints\n",
    "    normalized_keypoints = scaler.transform([selected_keypoints])\n",
    "\n",
    "    # Predict class and probabilities\n",
    "    exercise_class = clf.predict(normalized_keypoints)\n",
    "    exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "\n",
    "    # Apply softmax to the confidence scores if not already probabilities\n",
    "    exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "\n",
    "    return exercise_class[0], exercise_class_proba[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 123.1ms\n",
      "Speed: 4.2ms preprocess, 123.1ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.1ms\n",
      "Speed: 2.1ms preprocess, 140.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.7ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 0.7ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.0ms\n",
      "Speed: 0.7ms preprocess, 88.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 2.3ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 0.6ms preprocess, 79.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 0.7ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.6ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 0.9ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 110.8ms\n",
      "Speed: 0.6ms preprocess, 110.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.4ms\n",
      "Speed: 0.7ms preprocess, 82.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.4ms\n",
      "Speed: 0.6ms preprocess, 89.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.4ms\n",
      "Speed: 0.8ms preprocess, 85.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 0.6ms preprocess, 74.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.6ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 0.6ms preprocess, 77.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 0.5ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.9ms preprocess, 78.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.7ms preprocess, 71.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 0.7ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.5ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.2ms\n",
      "Speed: 0.7ms preprocess, 107.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.9ms\n",
      "Speed: 0.8ms preprocess, 98.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.5ms\n",
      "Speed: 1.0ms preprocess, 80.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 91.7ms\n",
      "Speed: 1.2ms preprocess, 91.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 95.0ms\n",
      "Speed: 0.7ms preprocess, 95.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 94.5ms\n",
      "Speed: 0.7ms preprocess, 94.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.6ms\n",
      "Speed: 0.6ms preprocess, 88.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 0.7ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.6ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.8ms\n",
      "Speed: 0.5ms preprocess, 88.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 0.7ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.8ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.7ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 0.7ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 0.6ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.8ms preprocess, 69.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.7ms\n",
      "Speed: 0.9ms preprocess, 85.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.9ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.6ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.8ms\n",
      "Speed: 0.7ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.7ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.6ms preprocess, 70.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.7ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.6ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.7ms preprocess, 75.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.6ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.9ms preprocess, 73.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 97.2ms\n",
      "Speed: 0.7ms preprocess, 97.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.9ms\n",
      "Speed: 0.6ms preprocess, 73.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 0.8ms preprocess, 74.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 0.7ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.2ms\n",
      "Speed: 0.6ms preprocess, 81.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 0.8ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.5ms\n",
      "Speed: 0.7ms preprocess, 84.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.0ms\n",
      "Speed: 0.7ms preprocess, 89.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 0.8ms preprocess, 79.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.7ms preprocess, 75.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.8ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.9ms preprocess, 73.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.7ms preprocess, 72.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.7ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.6ms\n",
      "Speed: 0.7ms preprocess, 80.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 0.6ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.7ms preprocess, 69.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.6ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.6ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.6ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.8ms\n",
      "Speed: 1.0ms preprocess, 88.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.1ms\n",
      "Speed: 0.9ms preprocess, 78.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 0.7ms preprocess, 79.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.7ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.6ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.6ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.6ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 0.6ms preprocess, 80.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.4ms\n",
      "Speed: 0.7ms preprocess, 83.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 128.7ms\n",
      "Speed: 0.8ms preprocess, 128.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.8ms\n",
      "Speed: 0.9ms preprocess, 80.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.2ms\n",
      "Speed: 0.7ms preprocess, 77.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.9ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.6ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.9ms\n",
      "Speed: 0.8ms preprocess, 83.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.6ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.6ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.6ms preprocess, 76.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.6ms preprocess, 79.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.3ms\n",
      "Speed: 0.6ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 119.0ms\n",
      "Speed: 0.6ms preprocess, 119.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 0.7ms preprocess, 140.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.0ms\n",
      "Speed: 0.7ms preprocess, 82.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 0.8ms preprocess, 77.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 0.9ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 0.8ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.8ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.9ms\n",
      "Speed: 0.8ms preprocess, 88.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.7ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 126.2ms\n",
      "Speed: 0.7ms preprocess, 126.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.6ms\n",
      "Speed: 0.8ms preprocess, 107.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.6ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.7ms preprocess, 75.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 0.7ms preprocess, 76.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.3ms\n",
      "Speed: 0.7ms preprocess, 87.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 0.7ms preprocess, 76.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.2ms\n",
      "Speed: 0.6ms preprocess, 77.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 0.6ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.9ms\n",
      "Speed: 0.8ms preprocess, 85.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 0.6ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.6ms preprocess, 76.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.4ms\n",
      "Speed: 0.7ms preprocess, 82.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 0.8ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.6ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.7ms preprocess, 70.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.6ms preprocess, 69.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.2ms\n",
      "Speed: 0.7ms preprocess, 79.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.8ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.7ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.7ms preprocess, 69.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.7ms preprocess, 70.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.6ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.6ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.6ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 0.6ms preprocess, 71.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 0.7ms preprocess, 79.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.6ms\n",
      "Speed: 0.7ms preprocess, 68.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.2ms\n",
      "Speed: 0.7ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.6ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.8ms\n",
      "Speed: 0.7ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.6ms\n",
      "Speed: 0.6ms preprocess, 68.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 0.6ms preprocess, 71.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.7ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.7ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.8ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.7ms preprocess, 71.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.6ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.7ms preprocess, 73.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.6ms preprocess, 76.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.7ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.6ms\n",
      "Speed: 0.6ms preprocess, 68.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.7ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.7ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 0.6ms preprocess, 73.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.7ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.6ms preprocess, 70.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.7ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.8ms preprocess, 70.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.6ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.6ms\n",
      "Speed: 0.6ms preprocess, 81.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.7ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.6ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.6ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.9ms preprocess, 78.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.8ms preprocess, 69.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.6ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.5ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.8ms\n",
      "Speed: 0.6ms preprocess, 99.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.4ms\n",
      "Speed: 0.6ms preprocess, 88.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.7ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.6ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.7ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.9ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.8ms preprocess, 73.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.8ms preprocess, 70.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.8ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.6ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.8ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.7ms preprocess, 73.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.6ms preprocess, 72.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 0.6ms preprocess, 76.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.6ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.8ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.7ms preprocess, 72.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.7ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.6ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 0.6ms preprocess, 79.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 1.4ms preprocess, 71.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.6ms preprocess, 69.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.6ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.6ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.6ms preprocess, 68.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.6ms preprocess, 76.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.6ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.6ms preprocess, 68.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.7ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.7ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.8ms preprocess, 69.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.5ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.6ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 0.9ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.8ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 0.8ms preprocess, 68.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 0.6ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.6ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.9ms\n",
      "Speed: 0.8ms preprocess, 83.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.8ms preprocess, 68.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.7ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.7ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.6ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.8ms preprocess, 70.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.6ms preprocess, 75.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.7ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 0.6ms preprocess, 75.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.6ms preprocess, 71.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.6ms preprocess, 72.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.6ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.9ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.5ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.6ms\n",
      "Speed: 0.7ms preprocess, 87.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.7ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.6ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.6ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.7ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.7ms preprocess, 68.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.6ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.7ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.7ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 0.9ms preprocess, 71.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.7ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.6ms preprocess, 71.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 0.7ms preprocess, 71.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.8ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.8ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.6ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.9ms\n",
      "Speed: 0.6ms preprocess, 78.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.6ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.7ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.7ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.6ms\n",
      "Speed: 0.6ms preprocess, 68.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.6ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.8ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.5ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.7ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 0.7ms preprocess, 75.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.1ms\n",
      "Speed: 0.7ms preprocess, 88.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.6ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 0.7ms preprocess, 71.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.8ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.6ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.9ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 66.9ms\n",
      "Speed: 0.6ms preprocess, 66.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.6ms preprocess, 68.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.7ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.6ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.7ms preprocess, 77.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.7ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 0.6ms preprocess, 74.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 0.6ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.6ms preprocess, 70.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.7ms preprocess, 68.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 0.6ms preprocess, 82.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 124.8ms\n",
      "Speed: 0.7ms preprocess, 124.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 2.1ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.7ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.6ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 0.6ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.2ms\n",
      "Speed: 0.6ms preprocess, 82.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 1.0ms preprocess, 78.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 0.6ms preprocess, 77.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.7ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.6ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.6ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.7ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 0.6ms preprocess, 72.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.7ms preprocess, 69.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.6ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.6ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.7ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.6ms preprocess, 68.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.6ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 97.8ms\n",
      "Speed: 0.7ms preprocess, 97.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 0.7ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.6ms\n",
      "Speed: 0.7ms preprocess, 73.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.6ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.9ms\n",
      "Speed: 0.7ms preprocess, 71.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.0ms\n",
      "Speed: 0.8ms preprocess, 68.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 2.4ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.6ms preprocess, 71.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.7ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.6ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.7ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.7ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.7ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.8ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 1.1ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.8ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.8ms preprocess, 75.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.7ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 5.9ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.6ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.8ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.6ms preprocess, 70.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 0.7ms preprocess, 68.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.7ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.6ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 2.8ms preprocess, 72.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.6ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.9ms\n",
      "Speed: 0.8ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 0.7ms preprocess, 68.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.7ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.7ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.9ms\n",
      "Speed: 0.7ms preprocess, 90.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.7ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 1.0ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.7ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.7ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 0.8ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 0.6ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 0.6ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 0.8ms preprocess, 68.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.9ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.7ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 0.7ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 0.6ms preprocess, 68.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.6ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.7ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.9ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 2.2ms preprocess, 70.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.7ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.8ms preprocess, 73.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.5ms\n",
      "Speed: 0.6ms preprocess, 81.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.6ms preprocess, 70.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.8ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.6ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.7ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.8ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.8ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.8ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 130.4ms\n",
      "Speed: 0.7ms preprocess, 130.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 0.7ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.4ms\n",
      "Speed: 0.7ms preprocess, 92.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 100.5ms\n",
      "Speed: 0.7ms preprocess, 100.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 0.7ms preprocess, 82.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 0.7ms preprocess, 77.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 0.6ms preprocess, 75.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 0.7ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.6ms\n",
      "Speed: 7.7ms preprocess, 85.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.6ms\n",
      "Speed: 1.0ms preprocess, 87.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 0.9ms preprocess, 80.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 1.5ms preprocess, 81.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 1.1ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 102.1ms\n",
      "Speed: 0.6ms preprocess, 102.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 116.9ms\n",
      "Speed: 0.7ms preprocess, 116.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 0.7ms preprocess, 74.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 0.9ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.9ms\n",
      "Speed: 0.6ms preprocess, 82.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.0ms\n",
      "Speed: 0.7ms preprocess, 88.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.1ms\n",
      "Speed: 0.7ms preprocess, 78.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 1.4ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.8ms\n",
      "Speed: 0.7ms preprocess, 84.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 105.8ms\n",
      "Speed: 4.5ms preprocess, 105.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.0ms\n",
      "Speed: 0.6ms preprocess, 86.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 169.5ms\n",
      "Speed: 1.5ms preprocess, 169.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 0.9ms preprocess, 81.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.7ms preprocess, 73.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.7ms preprocess, 78.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 122.4ms\n",
      "Speed: 0.9ms preprocess, 122.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.2ms\n",
      "Speed: 0.7ms preprocess, 76.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.7ms\n",
      "Speed: 0.6ms preprocess, 79.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.6ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 0.7ms preprocess, 80.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.6ms\n",
      "Speed: 0.7ms preprocess, 92.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 79.9ms\n",
      "Speed: 0.8ms preprocess, 79.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.0ms\n",
      "Speed: 0.7ms preprocess, 74.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.2ms\n",
      "Speed: 1.9ms preprocess, 74.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.6ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.7ms preprocess, 73.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 0.8ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 0.7ms preprocess, 71.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 76.0ms\n",
      "Speed: 0.7ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 106.2ms\n",
      "Speed: 0.7ms preprocess, 106.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 97.5ms\n",
      "Speed: 0.7ms preprocess, 97.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 166.6ms\n",
      "Speed: 0.8ms preprocess, 166.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 1 person, 110.4ms\n",
      "Speed: 0.7ms preprocess, 110.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 108.8ms\n",
      "Speed: 1.0ms preprocess, 108.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 94.1ms\n",
      "Speed: 0.9ms preprocess, 94.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 94.6ms\n",
      "Speed: 0.9ms preprocess, 94.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 101.0ms\n",
      "Speed: 0.6ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 103.9ms\n",
      "Speed: 0.9ms preprocess, 103.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 117.7ms\n",
      "Speed: 1.1ms preprocess, 117.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 91.8ms\n",
      "Speed: 0.8ms preprocess, 91.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 88.0ms\n",
      "Speed: 0.8ms preprocess, 88.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 86.9ms\n",
      "Speed: 0.8ms preprocess, 86.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 85.8ms\n",
      "Speed: 0.7ms preprocess, 85.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 89.5ms\n",
      "Speed: 0.8ms preprocess, 89.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 109.2ms\n",
      "Speed: 0.8ms preprocess, 109.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 0.6ms preprocess, 79.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.8ms\n",
      "Speed: 0.7ms preprocess, 84.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 93.1ms\n",
      "Speed: 0.8ms preprocess, 93.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Class Percentages (Based on Frame Count):\n",
      "Hoogspringen.json: 100.00%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 9.77%\n",
      "Estafette.json: 9.94%\n",
      "Hoogspringen.json: 22.21%\n",
      "Hordenlopen.json: 10.01%\n",
      "Kogelstoten.json: 9.57%\n",
      "Speerwerpen.json: 9.52%\n",
      "sprint_start.json: 9.71%\n",
      "sprint.json: 9.69%\n",
      "Verspringen.json: 9.59%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "model = YOLO(\"yolov8s-pose.pt\")  # Replace with your model path if different\n",
    "# Path to the video\n",
    "video_path = '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hoogspringen/segment_000001.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "class_counts = Counter()\n",
    "class_probabilities = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        exercise_class, exercise_class_proba = extract_keypoints_and_predict(frame, model, clf, scaler)\n",
    "        if exercise_class is not None:\n",
    "            frame_count += 1\n",
    "            class_counts[exercise_class] += 1\n",
    "            class_probabilities.append(exercise_class_proba)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping frame due to error: {e}\")\n",
    "        continue  # Skip problematic frames\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Calculate overall percentages based on frame count\n",
    "class_labels = clf.classes_\n",
    "total_frames = sum(class_counts.values())\n",
    "\n",
    "if total_frames == 0:\n",
    "    print(\"No frames were classified.\")\n",
    "else:\n",
    "    class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "    \n",
    "    # Calculate average probabilities across all frames\n",
    "    if class_probabilities:\n",
    "        average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "        probability_percentages = {exercise_labels_inv[i]: p * 100 for i, p in enumerate(average_probabilities)}\n",
    "    else:\n",
    "        probability_percentages = {}\n",
    "\n",
    "    # Output results\n",
    "    print(\"Class Percentages (Based on Frame Count):\")\n",
    "    for cls, pct in class_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "    for cls, pct in probability_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Percentages (Based on Frame Count):\n",
      "Hoogspringen.json: 100.00%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 9.77%\n",
      "Estafette.json: 9.94%\n",
      "Hoogspringen.json: 22.21%\n",
      "Hordenlopen.json: 10.01%\n",
      "Kogelstoten.json: 9.57%\n",
      "Speerwerpen.json: 9.52%\n",
      "sprint_start.json: 9.71%\n",
      "sprint.json: 9.69%\n",
      "Verspringen.json: 9.59%\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Class Percentages (Based on Frame Count):\")\n",
    "for cls, pct in class_percentages.items():\n",
    "    print(f\"{cls}: {pct:.2f}%\")\n",
    "\n",
    "print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "for cls, pct in probability_percentages.items():\n",
    "    print(f\"{cls}: {pct:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the directory to save the files\n",
    "save_dir = \"Testing\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the classifier\n",
    "joblib.dump(clf, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\"))\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\"))\n",
    "\n",
    "# Save the label mappings\n",
    "joblib.dump(exercise_labels_inv, os.path.join(save_dir, \"exercise_labels.pkl\"))\n",
    "\n",
    "print(\"Model, scaler, and labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in dataset: {np.int64(0): np.int64(2377), np.int64(1): np.int64(1904), np.int64(2): np.int64(1740), np.int64(3): np.int64(2099), np.int64(4): np.int64(1969), np.int64(5): np.int64(2121), np.int64(6): np.int64(2665), np.int64(7): np.int64(1284), np.int64(8): np.int64(1503)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler saved as 'rf_model.pkl' and 'scaler.pkl'.\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       475\n",
      "           1       0.87      0.87      0.87       381\n",
      "           2       0.94      0.91      0.93       348\n",
      "           3       0.90      0.90      0.90       420\n",
      "           4       0.96      0.97      0.97       394\n",
      "           5       0.92      0.95      0.94       424\n",
      "           6       0.95      0.95      0.95       533\n",
      "           7       0.89      0.85      0.87       257\n",
      "           8       0.92      0.90      0.91       301\n",
      "\n",
      "    accuracy                           0.92      3533\n",
      "   macro avg       0.92      0.91      0.92      3533\n",
      "weighted avg       0.92      0.92      0.92      3533\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Class distribution in dataset:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler for future use\n",
    "joblib.dump(clf, \"rf_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler_model.pkl\")\n",
    "print(\"Model and scaler saved as 'rf_model.pkl' and 'scaler.pkl'.\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping of exercise labels\n",
    "exercise_labels_inv = {v: k for k, v in exercise_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_and_predict(frame, model, clf, scaler):\n",
    "    # Run the YOLO pose model\n",
    "    results = model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        return None, None  # Skip if no keypoints detected\n",
    "    \n",
    "    # Initialize variables to store the keypoints of the largest person\n",
    "    largest_area = 0\n",
    "    selected_keypoints = None\n",
    "\n",
    "    for result in results:\n",
    "        if result.keypoints is not None and result.boxes is not None and len(result.boxes) > 0:\n",
    "            keypoints = result.keypoints.xy.cpu().numpy().reshape(-1, 2)  # x, y coordinates\n",
    "            bbox = result.boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            bbox_width = x_max - x_min\n",
    "            bbox_height = y_max - y_min\n",
    "            area = bbox_width * bbox_height\n",
    "\n",
    "            if area > largest_area:\n",
    "                largest_area = area\n",
    "                selected_keypoints = keypoints.flatten()\n",
    "\n",
    "    if selected_keypoints is None:\n",
    "        return None, None\n",
    "\n",
    "    # Check if the number of keypoints matches the expected number\n",
    "    expected_num_features = scaler.n_features_in_\n",
    "    if selected_keypoints.size != expected_num_features:\n",
    "        print(f\"Expected {expected_num_features} features, but got {selected_keypoints.size} features\")\n",
    "        return None, None\n",
    "\n",
    "    # Normalize keypoints\n",
    "    normalized_keypoints = scaler.transform([selected_keypoints])\n",
    "\n",
    "    # Predict class and probabilities\n",
    "    exercise_class = clf.predict(normalized_keypoints)\n",
    "    exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "\n",
    "    # Apply softmax to the confidence scores if not already probabilities\n",
    "    exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "\n",
    "    return exercise_class[0], exercise_class_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 0.8ms preprocess, 80.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.6ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 1.2ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 0.7ms preprocess, 73.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 0.6ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.6ms preprocess, 71.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 0.6ms preprocess, 74.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.3ms\n",
      "Speed: 0.6ms preprocess, 67.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.6ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 0.8ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.6ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.9ms\n",
      "Speed: 0.7ms preprocess, 67.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.9ms\n",
      "Speed: 0.6ms preprocess, 85.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.8ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.6ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.7ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.4ms\n",
      "Speed: 0.7ms preprocess, 80.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 0.8ms preprocess, 74.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.6ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.7ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.6ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 0.6ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.5ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.6ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.7ms preprocess, 78.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 0.6ms preprocess, 79.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.7ms preprocess, 73.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.7ms preprocess, 69.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.7ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 0.6ms preprocess, 71.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.6ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.6ms preprocess, 72.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.6ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.6ms preprocess, 69.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.9ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.8ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 0.8ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.8ms\n",
      "Speed: 0.8ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.6ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.8ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.8ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.7ms\n",
      "Speed: 0.6ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.6ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.6ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.6ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.8ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.5ms preprocess, 71.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.7ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.7ms\n",
      "Speed: 0.6ms preprocess, 80.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.8ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.2ms\n",
      "Speed: 0.6ms preprocess, 77.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 0.7ms preprocess, 81.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.6ms preprocess, 75.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.6ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 0.6ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 0.6ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.8ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.8ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.7ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.8ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 1.5ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.8ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.5ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 104.7ms\n",
      "Speed: 0.6ms preprocess, 104.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 0.6ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.9ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.8ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 0.6ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 0.6ms preprocess, 74.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 0.6ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.6ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.9ms\n",
      "Speed: 0.7ms preprocess, 87.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 0.6ms preprocess, 81.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.7ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.6ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 0.7ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.6ms preprocess, 71.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.8ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.8ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.6ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 0.6ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.9ms preprocess, 73.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.6ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.1ms\n",
      "Speed: 3.0ms preprocess, 78.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.6ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.6ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 0.7ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.6ms preprocess, 75.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.8ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.7ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.7ms\n",
      "Speed: 0.6ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 0.6ms preprocess, 72.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.7ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 0.6ms preprocess, 73.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 0.6ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.6ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 0.7ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.8ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.6ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.6ms preprocess, 71.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.8ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.9ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.7ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.6ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.5ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.8ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 0.6ms preprocess, 73.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.6ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.8ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.6ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 0.8ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.6ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.6ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.6ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.8ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.6ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.6ms\n",
      "Speed: 0.8ms preprocess, 73.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 0.7ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.9ms preprocess, 70.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.6ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.5ms\n",
      "Speed: 0.6ms preprocess, 80.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.6ms preprocess, 75.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.8ms\n",
      "Speed: 0.6ms preprocess, 76.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.1ms\n",
      "Speed: 0.8ms preprocess, 81.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.8ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.6ms\n",
      "Speed: 0.7ms preprocess, 79.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.5ms\n",
      "Speed: 1.0ms preprocess, 88.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 0.9ms preprocess, 75.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.0ms\n",
      "Speed: 1.0ms preprocess, 77.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 0.6ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 1.1ms preprocess, 75.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 1.0ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 0.6ms preprocess, 77.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.6ms preprocess, 75.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 1.0ms preprocess, 75.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.3ms\n",
      "Speed: 0.7ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 163.4ms\n",
      "Speed: 0.6ms preprocess, 163.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.5ms\n",
      "Speed: 0.6ms preprocess, 83.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 2.0ms preprocess, 79.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.3ms\n",
      "Speed: 1.1ms preprocess, 84.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 0.7ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.6ms\n",
      "Speed: 0.9ms preprocess, 84.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.0ms\n",
      "Speed: 0.7ms preprocess, 92.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.0ms\n",
      "Speed: 1.1ms preprocess, 90.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.3ms\n",
      "Speed: 2.0ms preprocess, 107.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 0.9ms preprocess, 83.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 95.3ms\n",
      "Speed: 1.2ms preprocess, 95.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.2ms\n",
      "Speed: 0.6ms preprocess, 90.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.3ms\n",
      "Speed: 1.0ms preprocess, 87.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.7ms\n",
      "Speed: 0.9ms preprocess, 90.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 1.0ms preprocess, 77.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.4ms\n",
      "Speed: 1.0ms preprocess, 80.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.0ms\n",
      "Speed: 0.6ms preprocess, 83.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.6ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 114.7ms\n",
      "Speed: 0.9ms preprocess, 114.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.7ms\n",
      "Speed: 3.0ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.8ms preprocess, 77.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.0ms\n",
      "Speed: 1.0ms preprocess, 88.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.3ms\n",
      "Speed: 0.6ms preprocess, 90.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.9ms\n",
      "Speed: 1.3ms preprocess, 89.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 0.9ms preprocess, 78.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 0.9ms preprocess, 81.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.3ms\n",
      "Speed: 0.5ms preprocess, 84.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 0.8ms preprocess, 83.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.9ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.9ms\n",
      "Speed: 1.0ms preprocess, 80.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 0.6ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.9ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 114.7ms\n",
      "Speed: 0.9ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 1.1ms preprocess, 83.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.4ms\n",
      "Speed: 0.6ms preprocess, 84.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.6ms preprocess, 75.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 1.0ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 1.0ms preprocess, 77.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 0.6ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 0.9ms preprocess, 75.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 1.2ms preprocess, 76.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.1ms\n",
      "Speed: 0.9ms preprocess, 88.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 1.1ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.8ms\n",
      "Speed: 1.2ms preprocess, 82.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "Speed: 0.7ms preprocess, 76.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.2ms\n",
      "Speed: 1.4ms preprocess, 90.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 0.9ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.7ms preprocess, 78.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.5ms\n",
      "Speed: 0.6ms preprocess, 80.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.4ms\n",
      "Speed: 1.1ms preprocess, 76.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 96.2ms\n",
      "Speed: 1.1ms preprocess, 96.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 95.1ms\n",
      "Speed: 1.1ms preprocess, 95.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 0.7ms preprocess, 80.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.7ms\n",
      "Speed: 1.0ms preprocess, 85.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.5ms\n",
      "Speed: 1.1ms preprocess, 85.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 1.3ms preprocess, 75.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 1.1ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 0.9ms preprocess, 78.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 103.1ms\n",
      "Speed: 1.0ms preprocess, 103.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 1.1ms preprocess, 79.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.4ms\n",
      "Speed: 1.0ms preprocess, 84.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 1.1ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.1ms\n",
      "Speed: 0.7ms preprocess, 88.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.0ms\n",
      "Speed: 1.4ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 0.9ms preprocess, 77.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 1.0ms preprocess, 83.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 0.6ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.8ms\n",
      "Speed: 0.9ms preprocess, 92.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.3ms\n",
      "Speed: 0.7ms preprocess, 84.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.9ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.2ms\n",
      "Speed: 1.0ms preprocess, 86.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 0.8ms preprocess, 74.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.7ms preprocess, 75.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 1.1ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.5ms\n",
      "Speed: 0.8ms preprocess, 90.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.2ms\n",
      "Speed: 0.9ms preprocess, 87.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 1.1ms preprocess, 77.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.1ms\n",
      "Speed: 1.1ms preprocess, 80.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.5ms\n",
      "Speed: 0.9ms preprocess, 81.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 0.9ms preprocess, 80.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 102.8ms\n",
      "Speed: 0.6ms preprocess, 102.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 94.6ms\n",
      "Speed: 0.7ms preprocess, 94.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.0ms\n",
      "Speed: 1.2ms preprocess, 86.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 91.3ms\n",
      "Speed: 0.6ms preprocess, 91.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.0ms\n",
      "Speed: 0.7ms preprocess, 87.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.2ms\n",
      "Speed: 1.1ms preprocess, 85.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 0.9ms preprocess, 81.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.5ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 0.7ms preprocess, 75.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.3ms\n",
      "Speed: 1.1ms preprocess, 85.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.4ms\n",
      "Speed: 1.0ms preprocess, 88.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.8ms\n",
      "Speed: 0.7ms preprocess, 85.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.0ms\n",
      "Speed: 0.6ms preprocess, 92.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 0.7ms preprocess, 75.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 1.0ms preprocess, 79.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.7ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.1ms\n",
      "Speed: 0.6ms preprocess, 86.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 115.1ms\n",
      "Speed: 1.0ms preprocess, 115.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.7ms preprocess, 78.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.7ms\n",
      "Speed: 0.6ms preprocess, 80.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.1ms\n",
      "Speed: 0.9ms preprocess, 79.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 1.2ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.8ms preprocess, 79.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.4ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 1.0ms preprocess, 80.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.8ms\n",
      "Speed: 0.6ms preprocess, 82.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.7ms\n",
      "Speed: 0.6ms preprocess, 78.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.0ms\n",
      "Speed: 0.9ms preprocess, 82.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.2ms\n",
      "Speed: 1.1ms preprocess, 82.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.2ms\n",
      "Speed: 0.6ms preprocess, 76.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 0.7ms preprocess, 79.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 0.6ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.0ms\n",
      "Speed: 1.3ms preprocess, 77.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.1ms\n",
      "Speed: 1.2ms preprocess, 78.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 0.7ms preprocess, 75.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 1.2ms preprocess, 73.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.7ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.1ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 1.0ms preprocess, 71.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.8ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.9ms preprocess, 76.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 1.3ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 1.2ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 0.9ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 1.2ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 0.9ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.1ms\n",
      "Speed: 0.9ms preprocess, 76.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 1.0ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 1.1ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.6ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 1.0ms preprocess, 78.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 1.1ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 0.7ms preprocess, 77.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.8ms\n",
      "Speed: 0.6ms preprocess, 76.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.4ms\n",
      "Speed: 0.9ms preprocess, 90.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 0.6ms preprocess, 78.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.1ms\n",
      "Speed: 1.0ms preprocess, 92.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 101.7ms\n",
      "Speed: 3.9ms preprocess, 101.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 1.1ms preprocess, 76.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.7ms preprocess, 77.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.5ms\n",
      "Speed: 1.4ms preprocess, 83.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 0.9ms preprocess, 76.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 0.7ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 0.7ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.7ms preprocess, 75.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 0.8ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.9ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.2ms\n",
      "Speed: 2.1ms preprocess, 84.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 0.6ms preprocess, 74.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.4ms\n",
      "Speed: 0.6ms preprocess, 76.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 129.8ms\n",
      "Speed: 0.6ms preprocess, 129.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 117.6ms\n",
      "Speed: 0.8ms preprocess, 117.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.9ms\n",
      "Speed: 0.5ms preprocess, 81.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.2ms\n",
      "Speed: 0.6ms preprocess, 79.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.4ms\n",
      "Speed: 1.4ms preprocess, 82.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.5ms\n",
      "Speed: 0.6ms preprocess, 89.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 0.9ms preprocess, 76.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.9ms preprocess, 75.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.0ms\n",
      "Speed: 0.6ms preprocess, 77.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.6ms preprocess, 74.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.9ms preprocess, 74.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 0.9ms preprocess, 79.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 0.9ms preprocess, 77.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 1.0ms preprocess, 77.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.8ms\n",
      "Speed: 0.9ms preprocess, 82.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.6ms\n",
      "Speed: 1.5ms preprocess, 86.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.7ms\n",
      "Speed: 0.7ms preprocess, 82.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.8ms\n",
      "Speed: 1.1ms preprocess, 89.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.8ms\n",
      "Speed: 1.8ms preprocess, 80.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.4ms\n",
      "Speed: 0.7ms preprocess, 81.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 1.1ms preprocess, 76.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.0ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.2ms\n",
      "Speed: 0.9ms preprocess, 89.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 1.0ms preprocess, 76.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.8ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.9ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.9ms preprocess, 79.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 180.7ms\n",
      "Speed: 1.4ms preprocess, 180.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.7ms\n",
      "Speed: 1.0ms preprocess, 84.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.9ms\n",
      "Speed: 0.5ms preprocess, 80.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 0.8ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.8ms\n",
      "Speed: 0.6ms preprocess, 76.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 1.0ms preprocess, 80.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 212.1ms\n",
      "Speed: 2.4ms preprocess, 212.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 161.7ms\n",
      "Speed: 1.1ms preprocess, 161.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 155.7ms\n",
      "Speed: 1.3ms preprocess, 155.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 115.8ms\n",
      "Speed: 0.6ms preprocess, 115.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.7ms\n",
      "Speed: 30.8ms preprocess, 145.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 147.7ms\n",
      "Speed: 1.1ms preprocess, 147.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 230.8ms\n",
      "Speed: 0.6ms preprocess, 230.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 222.3ms\n",
      "Speed: 1.1ms preprocess, 222.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 186.1ms\n",
      "Speed: 1.5ms preprocess, 186.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 232.4ms\n",
      "Speed: 2.6ms preprocess, 232.4ms inference, 16.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 441.2ms\n",
      "Speed: 1.3ms preprocess, 441.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 274.8ms\n",
      "Speed: 0.9ms preprocess, 274.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.8ms\n",
      "Speed: 0.7ms preprocess, 98.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 162.9ms\n",
      "Speed: 1.2ms preprocess, 162.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 152.2ms\n",
      "Speed: 1.8ms preprocess, 152.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.8ms\n",
      "Speed: 0.9ms preprocess, 99.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.0ms\n",
      "Speed: 1.0ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 103.5ms\n",
      "Speed: 1.0ms preprocess, 103.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 182.7ms\n",
      "Speed: 1.0ms preprocess, 182.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 120.6ms\n",
      "Speed: 0.6ms preprocess, 120.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 97.3ms\n",
      "Speed: 0.8ms preprocess, 97.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.7ms\n",
      "Speed: 0.9ms preprocess, 98.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.4ms\n",
      "Speed: 0.6ms preprocess, 86.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.9ms\n",
      "Speed: 0.6ms preprocess, 83.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.6ms preprocess, 77.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.7ms\n",
      "Speed: 0.9ms preprocess, 82.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.9ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 0.6ms preprocess, 74.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 101.4ms\n",
      "Speed: 0.9ms preprocess, 101.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.1ms\n",
      "Speed: 0.6ms preprocess, 84.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.6ms\n",
      "Speed: 0.7ms preprocess, 84.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.7ms\n",
      "Speed: 1.0ms preprocess, 85.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.4ms\n",
      "Speed: 2.0ms preprocess, 145.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.8ms\n",
      "Speed: 0.7ms preprocess, 84.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.4ms\n",
      "Speed: 1.0ms preprocess, 80.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.1ms\n",
      "Speed: 1.0ms preprocess, 80.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.8ms\n",
      "Speed: 0.9ms preprocess, 82.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.0ms preprocess, 75.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.7ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 1.0ms preprocess, 75.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.7ms\n",
      "Speed: 0.7ms preprocess, 82.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.5ms\n",
      "Speed: 1.0ms preprocess, 83.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.5ms\n",
      "Speed: 0.6ms preprocess, 107.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.3ms\n",
      "Speed: 1.5ms preprocess, 88.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 1.1ms preprocess, 75.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.9ms preprocess, 75.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 86.1ms\n",
      "Speed: 1.0ms preprocess, 86.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.7ms\n",
      "Speed: 0.9ms preprocess, 83.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 0.6ms preprocess, 76.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 1.3ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 0.9ms preprocess, 75.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.6ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 0.9ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 0.9ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 1.1ms preprocess, 80.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.9ms preprocess, 74.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.7ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.7ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.9ms preprocess, 74.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.2ms\n",
      "Speed: 0.5ms preprocess, 84.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.4ms\n",
      "Speed: 1.2ms preprocess, 76.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 0.6ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.9ms preprocess, 74.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.8ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.1ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.6ms preprocess, 75.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.7ms preprocess, 75.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 1.1ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 0.8ms preprocess, 74.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 0.9ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.7ms preprocess, 71.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.6ms preprocess, 72.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 0.7ms preprocess, 72.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 1.0ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 104.5ms\n",
      "Speed: 0.6ms preprocess, 104.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "Speed: 1.4ms preprocess, 76.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.0ms\n",
      "Speed: 1.1ms preprocess, 77.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.7ms\n",
      "Speed: 0.7ms preprocess, 72.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 0.9ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 0.7ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 0.8ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 0.6ms preprocess, 78.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.6ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 1.2ms preprocess, 72.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 0.5ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.8ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 0.8ms preprocess, 74.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.7ms\n",
      "Speed: 0.6ms preprocess, 72.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 0.9ms preprocess, 74.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.9ms\n",
      "Speed: 0.8ms preprocess, 73.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 81.5ms\n",
      "Speed: 0.6ms preprocess, 81.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 1.0ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 1.2ms preprocess, 77.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 0.6ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 0.9ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.7ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.8ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 0.8ms preprocess, 71.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.9ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 0.6ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.8ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 0.9ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.6ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 0.9ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 1.2ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.8ms\n",
      "Speed: 0.6ms preprocess, 75.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 0.8ms preprocess, 72.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 1.0ms preprocess, 77.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.7ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.7ms preprocess, 71.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 0.9ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 172.0ms\n",
      "Speed: 1.2ms preprocess, 172.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.0ms preprocess, 77.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.0ms\n",
      "Speed: 0.8ms preprocess, 81.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.0ms\n",
      "Speed: 0.9ms preprocess, 74.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 71.9ms\n",
      "Speed: 0.7ms preprocess, 71.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 102.9ms\n",
      "Speed: 1.1ms preprocess, 102.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 102.2ms\n",
      "Speed: 0.7ms preprocess, 102.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 192.9ms\n",
      "Speed: 1.0ms preprocess, 192.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 1.7ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 0.7ms preprocess, 73.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 74.3ms\n",
      "Speed: 0.6ms preprocess, 74.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 93.9ms\n",
      "Speed: 0.9ms preprocess, 93.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 74.0ms\n",
      "Speed: 0.9ms preprocess, 74.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 2 persons, 75.7ms\n",
      "Speed: 0.7ms preprocess, 75.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Expected 34 features, but got 68 features\n",
      "\n",
      "0: 384x640 1 person, 76.1ms\n",
      "Speed: 0.9ms preprocess, 76.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 0.8ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.8ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 1.1ms preprocess, 71.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.9ms preprocess, 77.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 0.6ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.6ms preprocess, 70.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 1.4ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.5ms\n",
      "Speed: 1.1ms preprocess, 75.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.6ms\n",
      "Speed: 0.8ms preprocess, 69.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 73.0ms\n",
      "Speed: 1.0ms preprocess, 73.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.3ms\n",
      "Speed: 0.6ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.6ms\n",
      "Speed: 0.8ms preprocess, 85.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.7ms preprocess, 75.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 0.9ms preprocess, 72.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 1.0ms preprocess, 72.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Class Percentages (Based on Frame Count):\n",
      "Hoogspringen.json: 97.00%\n",
      "Discurweper.json: 0.64%\n",
      "Estafette.json: 0.64%\n",
      "sprint.json: 1.72%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 9.90%\n",
      "Estafette.json: 10.03%\n",
      "Hoogspringen.json: 21.56%\n",
      "Hordenlopen.json: 10.19%\n",
      "Kogelstoten.json: 9.59%\n",
      "Speerwerpen.json: 9.56%\n",
      "sprint_start.json: 9.75%\n",
      "sprint.json: 9.79%\n",
      "Verspringen.json: 9.62%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "model = YOLO(\"yolov8s-pose.pt\")  # Replace with your model path if different\n",
    "# Path to the video\n",
    "video_path = '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hoogspringen/segment_000001.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "class_counts = Counter()\n",
    "class_probabilities = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        exercise_class, exercise_class_proba = extract_keypoints_and_predict(frame, model, clf, scaler)\n",
    "        if exercise_class is not None:\n",
    "            frame_count += 1\n",
    "            class_counts[exercise_class] += 1\n",
    "            class_probabilities.append(exercise_class_proba)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping frame due to error: {e}\")\n",
    "        continue  # Skip problematic frames\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Calculate overall percentages based on frame count\n",
    "class_labels = clf.classes_\n",
    "total_frames = sum(class_counts.values())\n",
    "\n",
    "if total_frames == 0:\n",
    "    print(\"No frames were classified.\")\n",
    "else:\n",
    "    class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "    \n",
    "    # Calculate average probabilities across all frames\n",
    "    if class_probabilities:\n",
    "        average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "        probability_percentages = {exercise_labels_inv[i]: p * 100 for i, p in enumerate(average_probabilities)}\n",
    "    else:\n",
    "        probability_percentages = {}\n",
    "\n",
    "    # Output results\n",
    "    print(\"Class Percentages (Based on Frame Count):\")\n",
    "    for cls, pct in class_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "    for cls, pct in probability_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Percentages (Based on Frame Count):\n",
      "Hoogspringen.json: 97.00%\n",
      "Discurweper.json: 0.64%\n",
      "Estafette.json: 0.64%\n",
      "sprint.json: 1.72%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 9.90%\n",
      "Estafette.json: 10.03%\n",
      "Hoogspringen.json: 21.56%\n",
      "Hordenlopen.json: 10.19%\n",
      "Kogelstoten.json: 9.59%\n",
      "Speerwerpen.json: 9.56%\n",
      "sprint_start.json: 9.75%\n",
      "sprint.json: 9.79%\n",
      "Verspringen.json: 9.62%\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Class Percentages (Based on Frame Count):\")\n",
    "for cls, pct in class_percentages.items():\n",
    "    print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "for cls, pct in probability_percentages.items():\n",
    "    print(f\"{cls}: {pct:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'df' is already prepared\n",
    "# X = df[['width', 'height', 'aspect_ratio', 'area', 'avg_distance']].values\n",
    "# y = df['score'].values\n",
    "\n",
    "# # Normalize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the DNN model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test).flatten()\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Test Mean Squared Error: {mse}\")\n",
    "\n",
    "# # Plot training history\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751us/step - loss: 4.1868 - mae: 1.6104 - val_loss: 1.3185 - val_mae: 0.8965\n",
      "Epoch 2/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1.6519 - mae: 1.0044 - val_loss: 1.2854 - val_mae: 0.8798\n",
      "Epoch 3/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 1.5211 - mae: 0.9585 - val_loss: 1.2580 - val_mae: 0.8671\n",
      "Epoch 4/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1.4495 - mae: 0.9323 - val_loss: 1.2601 - val_mae: 0.8685\n",
      "Epoch 5/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 1.4558 - mae: 0.9331 - val_loss: 1.2482 - val_mae: 0.8657\n",
      "Epoch 6/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1.4655 - mae: 0.9382 - val_loss: 1.2372 - val_mae: 0.8590\n",
      "Epoch 7/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 1.3954 - mae: 0.9122 - val_loss: 1.2587 - val_mae: 0.8692\n",
      "Epoch 8/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 1.4571 - mae: 0.9328 - val_loss: 1.2628 - val_mae: 0.8780\n",
      "Epoch 9/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.4037 - mae: 0.9164 - val_loss: 1.2339 - val_mae: 0.8559\n",
      "Epoch 10/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 1.3674 - mae: 0.9013 - val_loss: 1.2260 - val_mae: 0.8504\n",
      "Epoch 11/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 1.3652 - mae: 0.9063 - val_loss: 1.2254 - val_mae: 0.8564\n",
      "Epoch 12/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 1.3342 - mae: 0.8895 - val_loss: 1.2339 - val_mae: 0.8619\n",
      "Epoch 13/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 1.3335 - mae: 0.8883 - val_loss: 1.2177 - val_mae: 0.8518\n",
      "Epoch 14/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 1.2810 - mae: 0.8691 - val_loss: 1.2363 - val_mae: 0.8667\n",
      "Epoch 15/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 1.3456 - mae: 0.8986 - val_loss: 1.2088 - val_mae: 0.8491\n",
      "Epoch 16/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 1.3213 - mae: 0.8862 - val_loss: 1.2106 - val_mae: 0.8504\n",
      "Epoch 17/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 1.2961 - mae: 0.8714 - val_loss: 1.2047 - val_mae: 0.8465\n",
      "Epoch 18/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 1.3261 - mae: 0.8898 - val_loss: 1.1955 - val_mae: 0.8371\n",
      "Epoch 19/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 1.2922 - mae: 0.8731 - val_loss: 1.2089 - val_mae: 0.8493\n",
      "Epoch 20/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 1.2874 - mae: 0.8710 - val_loss: 1.1888 - val_mae: 0.8399\n",
      "Epoch 21/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.3166 - mae: 0.8876 - val_loss: 1.1866 - val_mae: 0.8377\n",
      "Epoch 22/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 1.2844 - mae: 0.8708 - val_loss: 1.2020 - val_mae: 0.8510\n",
      "Epoch 23/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.2797 - mae: 0.8737 - val_loss: 1.1824 - val_mae: 0.8291\n",
      "Epoch 24/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 1.2459 - mae: 0.8563 - val_loss: 1.1903 - val_mae: 0.8473\n",
      "Epoch 25/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 1.2589 - mae: 0.8622 - val_loss: 1.1805 - val_mae: 0.8344\n",
      "Epoch 26/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 1.2746 - mae: 0.8657 - val_loss: 1.1710 - val_mae: 0.8298\n",
      "Epoch 27/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 1.2404 - mae: 0.8562 - val_loss: 1.1741 - val_mae: 0.8245\n",
      "Epoch 28/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 1.2707 - mae: 0.8675 - val_loss: 1.1660 - val_mae: 0.8316\n",
      "Epoch 29/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 1.2465 - mae: 0.8587 - val_loss: 1.1701 - val_mae: 0.8331\n",
      "Epoch 30/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 1.2401 - mae: 0.8530 - val_loss: 1.1578 - val_mae: 0.8250\n",
      "Epoch 31/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.2325 - mae: 0.8514 - val_loss: 1.1737 - val_mae: 0.8400\n",
      "Epoch 32/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 1.2462 - mae: 0.8580 - val_loss: 1.1612 - val_mae: 0.8288\n",
      "Epoch 33/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 1.2228 - mae: 0.8493 - val_loss: 1.1598 - val_mae: 0.8268\n",
      "Epoch 34/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.2516 - mae: 0.8603 - val_loss: 1.1605 - val_mae: 0.8296\n",
      "Epoch 35/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 1.1851 - mae: 0.8352 - val_loss: 1.1632 - val_mae: 0.8310\n",
      "Epoch 36/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 1.2218 - mae: 0.8507 - val_loss: 1.1511 - val_mae: 0.8220\n",
      "Epoch 37/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 1.1911 - mae: 0.8407 - val_loss: 1.1583 - val_mae: 0.8220\n",
      "Epoch 38/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 1.1919 - mae: 0.8397 - val_loss: 1.1525 - val_mae: 0.8286\n",
      "Epoch 39/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.2057 - mae: 0.8467 - val_loss: 1.1457 - val_mae: 0.8258\n",
      "Epoch 40/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 1.2098 - mae: 0.8494 - val_loss: 1.1442 - val_mae: 0.8196\n",
      "Epoch 41/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.1959 - mae: 0.8436 - val_loss: 1.1442 - val_mae: 0.8267\n",
      "Epoch 42/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 1.1931 - mae: 0.8432 - val_loss: 1.1439 - val_mae: 0.8152\n",
      "Epoch 43/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.1768 - mae: 0.8336 - val_loss: 1.1495 - val_mae: 0.8299\n",
      "Epoch 44/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.2207 - mae: 0.8507 - val_loss: 1.1385 - val_mae: 0.8224\n",
      "Epoch 45/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.1828 - mae: 0.8382 - val_loss: 1.1350 - val_mae: 0.8188\n",
      "Epoch 46/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.2007 - mae: 0.8426 - val_loss: 1.1386 - val_mae: 0.8159\n",
      "Epoch 47/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 1.1663 - mae: 0.8323 - val_loss: 1.1323 - val_mae: 0.8174\n",
      "Epoch 48/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.2050 - mae: 0.8422 - val_loss: 1.1285 - val_mae: 0.8137\n",
      "Epoch 49/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.1750 - mae: 0.8291 - val_loss: 1.1327 - val_mae: 0.8127\n",
      "Epoch 50/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 1.2216 - mae: 0.8490 - val_loss: 1.1290 - val_mae: 0.8158\n",
      "Epoch 51/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 1.1614 - mae: 0.8297 - val_loss: 1.1277 - val_mae: 0.8151\n",
      "Epoch 52/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 1.1620 - mae: 0.8293 - val_loss: 1.1282 - val_mae: 0.8174\n",
      "Epoch 53/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.1595 - mae: 0.8290 - val_loss: 1.1339 - val_mae: 0.8204\n",
      "Epoch 54/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.1680 - mae: 0.8315 - val_loss: 1.1289 - val_mae: 0.8100\n",
      "Epoch 55/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 1.1763 - mae: 0.8313 - val_loss: 1.1259 - val_mae: 0.8069\n",
      "Epoch 56/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 1.1829 - mae: 0.8364 - val_loss: 1.1271 - val_mae: 0.8159\n",
      "Epoch 57/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 1.1431 - mae: 0.8210 - val_loss: 1.1269 - val_mae: 0.8150\n",
      "Epoch 58/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 1.1654 - mae: 0.8320 - val_loss: 1.1315 - val_mae: 0.8123\n",
      "Epoch 59/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.1650 - mae: 0.8234 - val_loss: 1.1253 - val_mae: 0.8143\n",
      "Epoch 60/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1.1683 - mae: 0.8272 - val_loss: 1.1262 - val_mae: 0.8172\n",
      "Epoch 61/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1.1330 - mae: 0.8180 - val_loss: 1.1227 - val_mae: 0.8053\n",
      "Epoch 62/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 1.1752 - mae: 0.8302 - val_loss: 1.1217 - val_mae: 0.8052\n",
      "Epoch 63/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 1.1625 - mae: 0.8257 - val_loss: 1.1242 - val_mae: 0.8080\n",
      "Epoch 64/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 1.1353 - mae: 0.8140 - val_loss: 1.1279 - val_mae: 0.8054\n",
      "Epoch 65/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.1535 - mae: 0.8269 - val_loss: 1.1222 - val_mae: 0.8036\n",
      "Epoch 66/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 1.1412 - mae: 0.8161 - val_loss: 1.1219 - val_mae: 0.8137\n",
      "Epoch 67/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 1.1772 - mae: 0.8335 - val_loss: 1.1165 - val_mae: 0.8033\n",
      "Epoch 68/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 1.1218 - mae: 0.8129 - val_loss: 1.1204 - val_mae: 0.8061\n",
      "Epoch 69/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 1.1360 - mae: 0.8237 - val_loss: 1.1185 - val_mae: 0.7990\n",
      "Epoch 70/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.1533 - mae: 0.8207 - val_loss: 1.1045 - val_mae: 0.7986\n",
      "Epoch 71/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 1.1762 - mae: 0.8300 - val_loss: 1.1025 - val_mae: 0.7925\n",
      "Epoch 72/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 1.1816 - mae: 0.8294 - val_loss: 1.1157 - val_mae: 0.7998\n",
      "Epoch 73/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 1.1609 - mae: 0.8278 - val_loss: 1.1287 - val_mae: 0.8083\n",
      "Epoch 74/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 1.1419 - mae: 0.8142 - val_loss: 1.1103 - val_mae: 0.8011\n",
      "Epoch 75/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 1.1275 - mae: 0.8128 - val_loss: 1.1096 - val_mae: 0.8011\n",
      "Epoch 76/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 1.1677 - mae: 0.8309 - val_loss: 1.1070 - val_mae: 0.7996\n",
      "Epoch 77/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 1.1411 - mae: 0.8193 - val_loss: 1.0980 - val_mae: 0.7955\n",
      "Epoch 78/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 1.1318 - mae: 0.8101 - val_loss: 1.1055 - val_mae: 0.8013\n",
      "Epoch 79/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 1.1271 - mae: 0.8171 - val_loss: 1.0961 - val_mae: 0.8013\n",
      "Epoch 80/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 1.1491 - mae: 0.8201 - val_loss: 1.1001 - val_mae: 0.7959\n",
      "Epoch 81/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 1.1358 - mae: 0.8153 - val_loss: 1.1105 - val_mae: 0.8041\n",
      "Epoch 82/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 1.1250 - mae: 0.8175 - val_loss: 1.0998 - val_mae: 0.7891\n",
      "Epoch 83/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.1115 - mae: 0.8086 - val_loss: 1.1055 - val_mae: 0.8052\n",
      "Epoch 84/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 1.1358 - mae: 0.8176 - val_loss: 1.1092 - val_mae: 0.8003\n",
      "Epoch 85/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.1494 - mae: 0.8200 - val_loss: 1.0980 - val_mae: 0.7894\n",
      "Epoch 86/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 1.1327 - mae: 0.8115 - val_loss: 1.0961 - val_mae: 0.7949\n",
      "Epoch 87/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.1346 - mae: 0.8168 - val_loss: 1.0985 - val_mae: 0.7947\n",
      "Epoch 88/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 1.0839 - mae: 0.7924 - val_loss: 1.1005 - val_mae: 0.7937\n",
      "Epoch 89/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 1.1613 - mae: 0.8254 - val_loss: 1.1046 - val_mae: 0.7972\n",
      "Epoch 90/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 1.1382 - mae: 0.8159 - val_loss: 1.1017 - val_mae: 0.8006\n",
      "Epoch 91/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 1.1334 - mae: 0.8180 - val_loss: 1.1100 - val_mae: 0.8012\n",
      "Epoch 92/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 1.1301 - mae: 0.8115 - val_loss: 1.0899 - val_mae: 0.7934\n",
      "Epoch 93/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 1.1403 - mae: 0.8181 - val_loss: 1.0909 - val_mae: 0.7870\n",
      "Epoch 94/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 1.1328 - mae: 0.8106 - val_loss: 1.0877 - val_mae: 0.7911\n",
      "Epoch 95/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 1.1420 - mae: 0.8196 - val_loss: 1.0893 - val_mae: 0.7892\n",
      "Epoch 96/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 1.1242 - mae: 0.8115 - val_loss: 1.0888 - val_mae: 0.7845\n",
      "Epoch 97/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 1.1502 - mae: 0.8157 - val_loss: 1.0900 - val_mae: 0.7964\n",
      "Epoch 98/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 1.1364 - mae: 0.8176 - val_loss: 1.0815 - val_mae: 0.7888\n",
      "Epoch 99/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 1.1369 - mae: 0.8143 - val_loss: 1.0869 - val_mae: 0.7843\n",
      "Epoch 100/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.1238 - mae: 0.8093 - val_loss: 1.0872 - val_mae: 0.7934\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 1.08715966739003\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_dir = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore\"\n",
    "\n",
    "# Function to extract features from a single JSON file\n",
    "def extract_features(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Check if 'segments' key exists\n",
    "    if 'segments' not in data:\n",
    "        print(f\"'segments' key not found in file: {json_path}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame for this file\n",
    "\n",
    "    rows = []\n",
    "    for segment, details in data['segments'].items():\n",
    "        score = details.get('score', 0)  # Default score to 0 if missing\n",
    "        for annotation in details.get('annotations', []):  # Default to empty list if missing\n",
    "            bbox = annotation.get('bbox', [0, 0, 0, 0])  # Default bbox to zeros\n",
    "            area = annotation.get('area', 0)  # Default area to 0\n",
    "            keypoints = np.array(annotation.get('keypoints', [])).reshape(-1, 3) if annotation.get('keypoints') else []\n",
    "\n",
    "            # Compute features\n",
    "            width, height = bbox[2], bbox[3]\n",
    "            aspect_ratio = width / height if height != 0 else 0\n",
    "            distances = [\n",
    "                np.linalg.norm(keypoints[i][:2] - keypoints[j][:2])\n",
    "                for i in range(len(keypoints))\n",
    "                for j in range(i + 1, len(keypoints))\n",
    "                if keypoints[i][2] > 0 and keypoints[j][2] > 0\n",
    "            ]\n",
    "            avg_distance = np.mean(distances) if distances else 0\n",
    "\n",
    "            # Append row\n",
    "            rows.append({\n",
    "                'segment': segment,\n",
    "                'score': score,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'area': area,\n",
    "                'avg_distance': avg_distance,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Aggregate all data\n",
    "all_data = pd.DataFrame()\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_dir, file_name)\n",
    "        df = extract_features(file_path)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# Prepare the dataset\n",
    "X = all_data[['width', 'height', 'aspect_ratio', 'area', 'avg_distance']].values\n",
    "y = all_data['score'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scoring_scaler.pkl\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the DNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error: {mse}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"scoring_model.h5\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - loss: 2.1351 - mae: 2.6014 - val_loss: 0.5075 - val_mae: 0.8859\n",
      "Epoch 2/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.7557 - mae: 1.1648 - val_loss: 0.4912 - val_mae: 0.8701\n",
      "Epoch 3/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.6315 - mae: 1.0261 - val_loss: 0.4736 - val_mae: 0.8452\n",
      "Epoch 4/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.5748 - mae: 0.9591 - val_loss: 0.4726 - val_mae: 0.8421\n",
      "Epoch 5/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.5605 - mae: 0.9434 - val_loss: 0.4760 - val_mae: 0.8460\n",
      "Epoch 6/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.5471 - mae: 0.9284 - val_loss: 0.4693 - val_mae: 0.8372\n",
      "Epoch 7/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 0.5307 - mae: 0.9073 - val_loss: 0.4691 - val_mae: 0.8417\n",
      "Epoch 8/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.5254 - mae: 0.8998 - val_loss: 0.4702 - val_mae: 0.8410\n",
      "Epoch 9/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.5027 - mae: 0.8743 - val_loss: 0.4674 - val_mae: 0.8388\n",
      "Epoch 10/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.5162 - mae: 0.8913 - val_loss: 0.4711 - val_mae: 0.8335\n",
      "Epoch 11/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.5145 - mae: 0.8881 - val_loss: 0.4660 - val_mae: 0.8368\n",
      "Epoch 12/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.4978 - mae: 0.8714 - val_loss: 0.4637 - val_mae: 0.8327\n",
      "Epoch 13/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.5001 - mae: 0.8727 - val_loss: 0.4651 - val_mae: 0.8343\n",
      "Epoch 14/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.4998 - mae: 0.8713 - val_loss: 0.4752 - val_mae: 0.8490\n",
      "Epoch 15/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.5001 - mae: 0.8729 - val_loss: 0.4634 - val_mae: 0.8298\n",
      "Epoch 16/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.4988 - mae: 0.8688 - val_loss: 0.4655 - val_mae: 0.8390\n",
      "Epoch 17/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 0.4909 - mae: 0.8635 - val_loss: 0.4635 - val_mae: 0.8274\n",
      "Epoch 18/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.4992 - mae: 0.8709 - val_loss: 0.4631 - val_mae: 0.8294\n",
      "Epoch 19/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.4949 - mae: 0.8646 - val_loss: 0.4623 - val_mae: 0.8261\n",
      "Epoch 20/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.4877 - mae: 0.8580 - val_loss: 0.4620 - val_mae: 0.8255\n",
      "Epoch 21/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 0.4814 - mae: 0.8499 - val_loss: 0.4626 - val_mae: 0.8251\n",
      "Epoch 22/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 0.4885 - mae: 0.8586 - val_loss: 0.4598 - val_mae: 0.8270\n",
      "Epoch 23/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.4870 - mae: 0.8582 - val_loss: 0.4604 - val_mae: 0.8251\n",
      "Epoch 24/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.4899 - mae: 0.8568 - val_loss: 0.4624 - val_mae: 0.8260\n",
      "Epoch 25/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.4796 - mae: 0.8503 - val_loss: 0.4584 - val_mae: 0.8275\n",
      "Epoch 26/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.4861 - mae: 0.8560 - val_loss: 0.4630 - val_mae: 0.8265\n",
      "Epoch 27/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.4919 - mae: 0.8624 - val_loss: 0.4566 - val_mae: 0.8228\n",
      "Epoch 28/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.4819 - mae: 0.8523 - val_loss: 0.4554 - val_mae: 0.8208\n",
      "Epoch 29/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.4858 - mae: 0.8561 - val_loss: 0.4574 - val_mae: 0.8234\n",
      "Epoch 30/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.4846 - mae: 0.8534 - val_loss: 0.4526 - val_mae: 0.8184\n",
      "Epoch 31/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.4871 - mae: 0.8545 - val_loss: 0.4540 - val_mae: 0.8218\n",
      "Epoch 32/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.4837 - mae: 0.8522 - val_loss: 0.4583 - val_mae: 0.8198\n",
      "Epoch 33/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.4916 - mae: 0.8605 - val_loss: 0.4547 - val_mae: 0.8234\n",
      "Epoch 34/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.4799 - mae: 0.8465 - val_loss: 0.4546 - val_mae: 0.8208\n",
      "Epoch 35/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.4754 - mae: 0.8419 - val_loss: 0.4516 - val_mae: 0.8178\n",
      "Epoch 36/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.4804 - mae: 0.8496 - val_loss: 0.4522 - val_mae: 0.8134\n",
      "Epoch 37/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.4619 - mae: 0.8287 - val_loss: 0.4516 - val_mae: 0.8108\n",
      "Epoch 38/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.4694 - mae: 0.8362 - val_loss: 0.4510 - val_mae: 0.8139\n",
      "Epoch 39/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.4729 - mae: 0.8400 - val_loss: 0.4600 - val_mae: 0.8235\n",
      "Epoch 40/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.4671 - mae: 0.8329 - val_loss: 0.4496 - val_mae: 0.8118\n",
      "Epoch 41/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.4588 - mae: 0.8214 - val_loss: 0.4483 - val_mae: 0.8135\n",
      "Epoch 42/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.4756 - mae: 0.8427 - val_loss: 0.4465 - val_mae: 0.8085\n",
      "Epoch 43/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.4811 - mae: 0.8473 - val_loss: 0.4486 - val_mae: 0.8105\n",
      "Epoch 44/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.4605 - mae: 0.8235 - val_loss: 0.4469 - val_mae: 0.8052\n",
      "Epoch 45/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.4614 - mae: 0.8231 - val_loss: 0.4490 - val_mae: 0.8099\n",
      "Epoch 46/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.4589 - mae: 0.8228 - val_loss: 0.4466 - val_mae: 0.8097\n",
      "Epoch 47/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.4665 - mae: 0.8327 - val_loss: 0.4434 - val_mae: 0.8003\n",
      "Epoch 48/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.4663 - mae: 0.8285 - val_loss: 0.4438 - val_mae: 0.8043\n",
      "Epoch 49/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.4629 - mae: 0.8228 - val_loss: 0.4475 - val_mae: 0.8110\n",
      "Epoch 50/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 0.4671 - mae: 0.8326 - val_loss: 0.4453 - val_mae: 0.8063\n",
      "Epoch 51/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4653 - mae: 0.8273 - val_loss: 0.4474 - val_mae: 0.8061\n",
      "Epoch 52/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.4658 - mae: 0.8292 - val_loss: 0.4455 - val_mae: 0.8032\n",
      "Epoch 53/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.4698 - mae: 0.8345 - val_loss: 0.4459 - val_mae: 0.8072\n",
      "Epoch 54/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 0.4670 - mae: 0.8300 - val_loss: 0.4456 - val_mae: 0.8064\n",
      "Epoch 55/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.4569 - mae: 0.8181 - val_loss: 0.4446 - val_mae: 0.8043\n",
      "Epoch 56/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.4679 - mae: 0.8303 - val_loss: 0.4412 - val_mae: 0.8012\n",
      "Epoch 57/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.4604 - mae: 0.8230 - val_loss: 0.4496 - val_mae: 0.8068\n",
      "Epoch 58/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.4608 - mae: 0.8231 - val_loss: 0.4434 - val_mae: 0.8063\n",
      "Epoch 59/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4595 - mae: 0.8217 - val_loss: 0.4428 - val_mae: 0.8049\n",
      "Epoch 60/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.4591 - mae: 0.8229 - val_loss: 0.4525 - val_mae: 0.8133\n",
      "Epoch 61/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.4635 - mae: 0.8273 - val_loss: 0.4430 - val_mae: 0.8008\n",
      "Epoch 62/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.4569 - mae: 0.8227 - val_loss: 0.4410 - val_mae: 0.7957\n",
      "Epoch 63/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.4583 - mae: 0.8187 - val_loss: 0.4444 - val_mae: 0.8076\n",
      "Epoch 64/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4702 - mae: 0.8368 - val_loss: 0.4428 - val_mae: 0.7988\n",
      "Epoch 65/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.4560 - mae: 0.8168 - val_loss: 0.4413 - val_mae: 0.8001\n",
      "Epoch 66/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.4652 - mae: 0.8284 - val_loss: 0.4408 - val_mae: 0.7984\n",
      "Epoch 67/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.4693 - mae: 0.8352 - val_loss: 0.4388 - val_mae: 0.7937\n",
      "Epoch 68/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.4679 - mae: 0.8310 - val_loss: 0.4420 - val_mae: 0.8009\n",
      "Epoch 69/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.4644 - mae: 0.8267 - val_loss: 0.4437 - val_mae: 0.7991\n",
      "Epoch 70/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.4589 - mae: 0.8210 - val_loss: 0.4424 - val_mae: 0.7986\n",
      "Epoch 71/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.4758 - mae: 0.8407 - val_loss: 0.4408 - val_mae: 0.8010\n",
      "Epoch 72/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.4661 - mae: 0.8280 - val_loss: 0.4421 - val_mae: 0.7968\n",
      "Epoch 73/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.4628 - mae: 0.8271 - val_loss: 0.4406 - val_mae: 0.7984\n",
      "Epoch 74/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.4643 - mae: 0.8270 - val_loss: 0.4409 - val_mae: 0.7965\n",
      "Epoch 75/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.4497 - mae: 0.8102 - val_loss: 0.4391 - val_mae: 0.7951\n",
      "Epoch 76/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.4554 - mae: 0.8131 - val_loss: 0.4389 - val_mae: 0.7966\n",
      "Epoch 77/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.4682 - mae: 0.8301 - val_loss: 0.4401 - val_mae: 0.7994\n",
      "Epoch 78/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4518 - mae: 0.8124 - val_loss: 0.4409 - val_mae: 0.7987\n",
      "Epoch 79/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4511 - mae: 0.8129 - val_loss: 0.4394 - val_mae: 0.7970\n",
      "Epoch 80/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.4608 - mae: 0.8231 - val_loss: 0.4383 - val_mae: 0.7929\n",
      "Epoch 81/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.4609 - mae: 0.8208 - val_loss: 0.4386 - val_mae: 0.7948\n",
      "Epoch 82/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.4565 - mae: 0.8168 - val_loss: 0.4386 - val_mae: 0.7926\n",
      "Epoch 83/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.4496 - mae: 0.8096 - val_loss: 0.4367 - val_mae: 0.7946\n",
      "Epoch 84/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.4513 - mae: 0.8088 - val_loss: 0.4398 - val_mae: 0.7972\n",
      "Epoch 85/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.4576 - mae: 0.8202 - val_loss: 0.4405 - val_mae: 0.7978\n",
      "Epoch 86/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.4486 - mae: 0.8071 - val_loss: 0.4385 - val_mae: 0.7977\n",
      "Epoch 87/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.4453 - mae: 0.8048 - val_loss: 0.4415 - val_mae: 0.8006\n",
      "Epoch 88/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.4528 - mae: 0.8145 - val_loss: 0.4393 - val_mae: 0.7946\n",
      "Epoch 89/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.4552 - mae: 0.8166 - val_loss: 0.4417 - val_mae: 0.7989\n",
      "Epoch 90/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.4533 - mae: 0.8174 - val_loss: 0.4394 - val_mae: 0.7948\n",
      "Epoch 91/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.4673 - mae: 0.8317 - val_loss: 0.4410 - val_mae: 0.7936\n",
      "Epoch 92/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 0.4562 - mae: 0.8171 - val_loss: 0.4393 - val_mae: 0.7919\n",
      "Epoch 93/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.4510 - mae: 0.8114 - val_loss: 0.4405 - val_mae: 0.7978\n",
      "Epoch 94/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.4478 - mae: 0.8085 - val_loss: 0.4366 - val_mae: 0.7957\n",
      "Epoch 95/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4388 - mae: 0.7974 - val_loss: 0.4332 - val_mae: 0.7890\n",
      "Epoch 96/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.4578 - mae: 0.8212 - val_loss: 0.4339 - val_mae: 0.7883\n",
      "Epoch 97/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.4499 - mae: 0.8104 - val_loss: 0.4364 - val_mae: 0.7903\n",
      "Epoch 98/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.4615 - mae: 0.8221 - val_loss: 0.4332 - val_mae: 0.7878\n",
      "Epoch 99/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.4534 - mae: 0.8151 - val_loss: 0.4352 - val_mae: 0.7911\n",
      "Epoch 100/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.4503 - mae: 0.8115 - val_loss: 0.4324 - val_mae: 0.7848\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error (rounded): 1.179190340909091\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_dir = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore\"\n",
    "\n",
    "# Function to extract features from a single JSON file\n",
    "# Function to extract features from a single JSON file\n",
    "def extract_features(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    rows = []\n",
    "    if 'segments' in data:\n",
    "        for segment, details in data['segments'].items():\n",
    "            score = details['score']\n",
    "            for annotation in details['annotations']:\n",
    "                bbox = annotation['bbox']\n",
    "                area = annotation['area']\n",
    "                keypoints = np.array(annotation['keypoints']).reshape(-1, 3)\n",
    "\n",
    "                # Compute features\n",
    "                width, height = bbox[2], bbox[3]\n",
    "                aspect_ratio = width / height\n",
    "                distances = [\n",
    "                    np.linalg.norm(keypoints[i][:2] - keypoints[j][:2])\n",
    "                    for i in range(len(keypoints))\n",
    "                    for j in range(i + 1, len(keypoints))\n",
    "                    if keypoints[i][2] > 0 and keypoints[j][2] > 0\n",
    "                ]\n",
    "                avg_distance = np.mean(distances) if distances else 0\n",
    "\n",
    "                # Append row\n",
    "                rows.append({\n",
    "                    'segment': segment,\n",
    "                    'score': score,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'aspect_ratio': aspect_ratio,\n",
    "                    'area': area,\n",
    "                    'avg_distance': avg_distance,\n",
    "                })\n",
    "    else:\n",
    "        print(f\"Key 'segments' not found in {json_path}\")\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Aggregate all data\n",
    "all_data = pd.DataFrame()\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_dir, file_name)\n",
    "        df = extract_features(file_path)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# Prepare the dataset\n",
    "X = all_data[['width', 'height', 'aspect_ratio', 'area', 'avg_distance']].values\n",
    "y = all_data['score'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scoring_scaler.pkl\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the DNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "def round_to_nearest_half(x):\n",
    "    return round(x * 2) / 2\n",
    "\n",
    "\n",
    "# Round predictions to the nearest 0.5\n",
    "y_pred_rounded = [round_to_nearest_half(score) for score in y_pred]\n",
    "\n",
    "# Calculate MSE for the rounded predictions\n",
    "mse = mean_squared_error(y_test, y_pred_rounded)  # Corrected line\n",
    "print(f\"Test Mean Squared Error (rounded): {mse}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"scoring_model2.h5\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing video: 'Sequential' object has no attribute 'detect_keypoints'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib  # Use joblib for loading the pickle model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the classification model (Random Forest)\n",
    "classification_model = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "\n",
    "# Define custom objects if needed (for metrics, loss functions, etc.)\n",
    "custom_objects = {\n",
    "    'mse': tf.keras.metrics.MeanSquaredError()  # Make sure to specify custom metrics if needed\n",
    "}\n",
    "\n",
    "# Load the scoring model with custom objects\n",
    "scoring_model = tf.keras.models.load_model(\n",
    "    \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\",\n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "# Load the scaler used for feature scaling\n",
    "scaler = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "\n",
    "# Function to extract features from a video\n",
    "def extract_features_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints_list = []\n",
    "    bounding_boxes = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        def extract_keypoints_and_bbox(frame):\n",
    "            # Implement keypoint extraction and bounding box logic here\n",
    "            # Example:\n",
    "            keypoints = model.detect_keypoints(frame)  # Replace with your actual model or logic\n",
    "            bbox = model.detect_bbox(frame)  # Replace with your actual bounding box detection logic\n",
    "            return keypoints, bbox\n",
    "\n",
    "        # Process frame (example: extract keypoints using your keypoint detection logic)\n",
    "        keypoints, bbox = extract_keypoints_and_bbox(frame)\n",
    "        if keypoints is not None and bbox is not None:\n",
    "            keypoints_list.append(keypoints)\n",
    "            bounding_boxes.append(bbox)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Aggregate features\n",
    "    if not keypoints_list or not bounding_boxes:\n",
    "        raise ValueError(\"No keypoints or bounding boxes extracted from the video.\")\n",
    "\n",
    "    # Calculate features\n",
    "    width = np.mean([bbox[2] for bbox in bounding_boxes])\n",
    "    height = np.mean([bbox[3] for bbox in bounding_boxes])\n",
    "    aspect_ratio = width / height\n",
    "    area = np.mean([bbox[2] * bbox[3] for bbox in bounding_boxes])\n",
    "\n",
    "    distances = []\n",
    "    for keypoints in keypoints_list:\n",
    "        keypoints = np.array(keypoints).reshape(-1, 3)\n",
    "        dist = [\n",
    "            np.linalg.norm(keypoints[i][:2] - keypoints[j][:2])\n",
    "            for i in range(len(keypoints))\n",
    "            for j in range(i + 1, len(keypoints))\n",
    "            if keypoints[i][2] > 0 and keypoints[j][2] > 0\n",
    "        ]\n",
    "        distances.append(np.mean(dist) if dist else 0)\n",
    "\n",
    "    avg_distance = np.mean(distances)\n",
    "\n",
    "    return np.array([[width, height, aspect_ratio, area, avg_distance]])\n",
    "\n",
    "# Function to classify and score a video\n",
    "def classify_and_score_video(video_path):\n",
    "    # Extract features\n",
    "    features = extract_features_from_video(video_path)\n",
    "    features_scaled = scaler.transform(features)  # Scale features using the same scaler\n",
    "\n",
    "    # Predict exercise type using the Random Forest model\n",
    "    exercise_class = classification_model.predict(features_scaled)[0]  # Direct prediction\n",
    "    print(f\"Classified Exercise: {exercise_class}\")\n",
    "\n",
    "    # Predict score using the scoring model\n",
    "    score = scoring_model.predict(features_scaled)[0][0]\n",
    "    print(f\"Predicted Score: {score}\")\n",
    "\n",
    "    return exercise_class, score\n",
    "\n",
    "# Example usage\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_001557.mp4\"\n",
    "try:\n",
    "    exercise_class, predicted_score = classify_and_score_video(video_path)\n",
    "    print(f\"Exercise Class: {exercise_class}, Score: {predicted_score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing video: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 170.8ms\n",
      "Speed: 5.1ms preprocess, 170.8ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 5.143880844116211, 'inference': 170.84312438964844, 'postprocess': 8.874177932739258}]\n",
      "Bounding Box: tensor([[ 64.0000, 204.5000,  52.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5892, 0.6988, 0.0800, 0.9533, 0.1159, 0.9978, 0.9628, 0.9902, 0.6096, 0.9449, 0.4077, 0.9987, 0.9948, 0.9975, 0.9886, 0.9871, 0.9665]])\n",
      "data: tensor([[[5.5682e+01, 1.4894e+02, 5.8918e-01],\n",
      "         [5.6697e+01, 1.4610e+02, 6.9878e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.9999e-02],\n",
      "         [6.1248e+01, 1.4660e+02, 9.5333e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1590e-01],\n",
      "         [6.5869e+01, 1.6264e+02, 9.9781e-01],\n",
      "         [7.1959e+01, 1.6144e+02, 9.6283e-01],\n",
      "         [6.3668e+01, 1.8504e+02, 9.9024e-01],\n",
      "         [7.1887e+01, 1.8035e+02, 6.0962e-01],\n",
      "         [4.9227e+01, 1.8169e+02, 9.4485e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0774e-01],\n",
      "         [7.1173e+01, 1.9969e+02, 9.9868e-01],\n",
      "         [7.6250e+01, 1.9811e+02, 9.9483e-01],\n",
      "         [6.4185e+01, 2.2750e+02, 9.9750e-01],\n",
      "         [7.1773e+01, 2.2604e+02, 9.8865e-01],\n",
      "         [6.8204e+01, 2.6212e+02, 9.8708e-01],\n",
      "         [8.4200e+01, 2.5136e+02, 9.6646e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 55.6816, 148.9438],\n",
      "         [ 56.6967, 146.1016],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 61.2478, 146.5974],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.8686, 162.6371],\n",
      "         [ 71.9591, 161.4367],\n",
      "         [ 63.6683, 185.0444],\n",
      "         [ 71.8870, 180.3481],\n",
      "         [ 49.2270, 181.6852],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.1733, 199.6940],\n",
      "         [ 76.2498, 198.1144],\n",
      "         [ 64.1854, 227.5009],\n",
      "         [ 71.7728, 226.0366],\n",
      "         [ 68.2041, 262.1180],\n",
      "         [ 84.2002, 251.3628]]])\n",
      "xyn: tensor([[[0.1547, 0.4137],\n",
      "         [0.1575, 0.4058],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1701, 0.4072],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1830, 0.4518],\n",
      "         [0.1999, 0.4484],\n",
      "         [0.1769, 0.5140],\n",
      "         [0.1997, 0.5010],\n",
      "         [0.1367, 0.5047],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1977, 0.5547],\n",
      "         [0.2118, 0.5503],\n",
      "         [0.1783, 0.6319],\n",
      "         [0.1994, 0.6279],\n",
      "         [0.1895, 0.7281],\n",
      "         [0.2339, 0.6982]]])\n",
      "\n",
      "0: 640x640 1 person, 136.0ms\n",
      "Speed: 1.3ms preprocess, 136.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2993812561035156, 'inference': 135.99586486816406, 'postprocess': 0.6546974182128906}]\n",
      "Bounding Box: tensor([[ 67.5000, 204.5000,  47.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5550, 0.6733, 0.0756, 0.9323, 0.0929, 0.9961, 0.9338, 0.9855, 0.5284, 0.9395, 0.3898, 0.9982, 0.9930, 0.9968, 0.9859, 0.9844, 0.9598]])\n",
      "data: tensor([[[5.7368e+01, 1.4844e+02, 5.5502e-01],\n",
      "         [5.8444e+01, 1.4565e+02, 6.7326e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.5599e-02],\n",
      "         [6.3091e+01, 1.4619e+02, 9.3234e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.2899e-02],\n",
      "         [6.6052e+01, 1.6255e+02, 9.9607e-01],\n",
      "         [7.5022e+01, 1.6144e+02, 9.3378e-01],\n",
      "         [6.2561e+01, 1.8291e+02, 9.8554e-01],\n",
      "         [7.6432e+01, 1.7941e+02, 5.2836e-01],\n",
      "         [5.1877e+01, 1.8033e+02, 9.3949e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8978e-01],\n",
      "         [7.2824e+01, 1.9884e+02, 9.9818e-01],\n",
      "         [7.9925e+01, 1.9719e+02, 9.9297e-01],\n",
      "         [6.4571e+01, 2.2705e+02, 9.9683e-01],\n",
      "         [7.2984e+01, 2.2506e+02, 9.8590e-01],\n",
      "         [6.8025e+01, 2.6125e+02, 9.8443e-01],\n",
      "         [8.4185e+01, 2.5181e+02, 9.5976e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 57.3676, 148.4400],\n",
      "         [ 58.4437, 145.6467],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.0914, 146.1946],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.0519, 162.5489],\n",
      "         [ 75.0224, 161.4368],\n",
      "         [ 62.5607, 182.9099],\n",
      "         [ 76.4316, 179.4098],\n",
      "         [ 51.8773, 180.3266],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.8237, 198.8372],\n",
      "         [ 79.9249, 197.1871],\n",
      "         [ 64.5714, 227.0464],\n",
      "         [ 72.9841, 225.0635],\n",
      "         [ 68.0255, 261.2510],\n",
      "         [ 84.1853, 251.8057]]])\n",
      "xyn: tensor([[[0.1594, 0.4123],\n",
      "         [0.1623, 0.4046],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1753, 0.4061],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1835, 0.4515],\n",
      "         [0.2084, 0.4484],\n",
      "         [0.1738, 0.5081],\n",
      "         [0.2123, 0.4984],\n",
      "         [0.1441, 0.5009],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2023, 0.5523],\n",
      "         [0.2220, 0.5477],\n",
      "         [0.1794, 0.6307],\n",
      "         [0.2027, 0.6252],\n",
      "         [0.1890, 0.7257],\n",
      "         [0.2338, 0.6995]]])\n",
      "\n",
      "0: 640x640 1 person, 121.1ms\n",
      "Speed: 1.4ms preprocess, 121.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3878345489501953, 'inference': 121.10471725463867, 'postprocess': 0.43702125549316406}]\n",
      "Bounding Box: tensor([[ 69.0000, 203.5000,  44.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5567, 0.6708, 0.0757, 0.9361, 0.1002, 0.9965, 0.9443, 0.9858, 0.5518, 0.9389, 0.3993, 0.9984, 0.9939, 0.9973, 0.9881, 0.9866, 0.9659]])\n",
      "data: tensor([[[5.8324e+01, 1.4829e+02, 5.5672e-01],\n",
      "         [5.9456e+01, 1.4561e+02, 6.7081e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.5677e-02],\n",
      "         [6.3968e+01, 1.4641e+02, 9.3615e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0024e-01],\n",
      "         [6.6233e+01, 1.6246e+02, 9.9647e-01],\n",
      "         [7.6631e+01, 1.6138e+02, 9.4431e-01],\n",
      "         [6.4130e+01, 1.8388e+02, 9.8581e-01],\n",
      "         [7.8977e+01, 1.8031e+02, 5.5182e-01],\n",
      "         [5.4183e+01, 1.8120e+02, 9.3894e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9929e-01],\n",
      "         [7.3461e+01, 1.9832e+02, 9.9838e-01],\n",
      "         [8.1734e+01, 1.9671e+02, 9.9394e-01],\n",
      "         [6.3843e+01, 2.2592e+02, 9.9727e-01],\n",
      "         [7.5064e+01, 2.2420e+02, 9.8808e-01],\n",
      "         [6.7755e+01, 2.6103e+02, 9.8659e-01],\n",
      "         [8.5501e+01, 2.5018e+02, 9.6587e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 58.3238, 148.2884],\n",
      "         [ 59.4556, 145.6109],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.9679, 146.4090],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.2325, 162.4557],\n",
      "         [ 76.6311, 161.3772],\n",
      "         [ 64.1296, 183.8818],\n",
      "         [ 78.9765, 180.3081],\n",
      "         [ 54.1828, 181.1976],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.4608, 198.3219],\n",
      "         [ 81.7339, 196.7129],\n",
      "         [ 63.8433, 225.9184],\n",
      "         [ 75.0642, 224.2031],\n",
      "         [ 67.7545, 261.0320],\n",
      "         [ 85.5013, 250.1751]]])\n",
      "xyn: tensor([[[0.1620, 0.4119],\n",
      "         [0.1652, 0.4045],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1777, 0.4067],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1840, 0.4513],\n",
      "         [0.2129, 0.4483],\n",
      "         [0.1781, 0.5108],\n",
      "         [0.2194, 0.5009],\n",
      "         [0.1505, 0.5033],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2041, 0.5509],\n",
      "         [0.2270, 0.5464],\n",
      "         [0.1773, 0.6276],\n",
      "         [0.2085, 0.6228],\n",
      "         [0.1882, 0.7251],\n",
      "         [0.2375, 0.6949]]])\n",
      "\n",
      "0: 640x640 1 person, 129.4ms\n",
      "Speed: 1.2ms preprocess, 129.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2269020080566406, 'inference': 129.35709953308105, 'postprocess': 0.48613548278808594}]\n",
      "Bounding Box: tensor([[ 69., 204.,  46., 138.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4435, 0.5635, 0.0513, 0.9298, 0.1040, 0.9966, 0.9437, 0.9862, 0.5392, 0.9335, 0.3644, 0.9987, 0.9948, 0.9978, 0.9898, 0.9889, 0.9707]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.4355e-01],\n",
      "         [6.1122e+01, 1.4595e+02, 5.6351e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.1259e-02],\n",
      "         [6.4690e+01, 1.4705e+02, 9.2983e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0397e-01],\n",
      "         [6.6152e+01, 1.6252e+02, 9.9656e-01],\n",
      "         [7.8049e+01, 1.6129e+02, 9.4368e-01],\n",
      "         [6.4297e+01, 1.8290e+02, 9.8621e-01],\n",
      "         [7.9609e+01, 1.7960e+02, 5.3922e-01],\n",
      "         [5.4758e+01, 1.8143e+02, 9.3352e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6438e-01],\n",
      "         [7.3761e+01, 1.9685e+02, 9.9866e-01],\n",
      "         [8.3258e+01, 1.9525e+02, 9.9484e-01],\n",
      "         [6.3817e+01, 2.2461e+02, 9.9776e-01],\n",
      "         [7.6240e+01, 2.2314e+02, 9.8980e-01],\n",
      "         [6.7260e+01, 2.6071e+02, 9.8894e-01],\n",
      "         [8.5656e+01, 2.4989e+02, 9.7068e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 61.1220, 145.9511],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.6901, 147.0508],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.1517, 162.5183],\n",
      "         [ 78.0489, 161.2915],\n",
      "         [ 64.2974, 182.9020],\n",
      "         [ 79.6091, 179.5952],\n",
      "         [ 54.7583, 181.4350],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.7614, 196.8471],\n",
      "         [ 83.2576, 195.2470],\n",
      "         [ 63.8170, 224.6137],\n",
      "         [ 76.2398, 223.1391],\n",
      "         [ 67.2597, 260.7116],\n",
      "         [ 85.6561, 249.8917]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1698, 0.4054],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1797, 0.4085],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1838, 0.4514],\n",
      "         [0.2168, 0.4480],\n",
      "         [0.1786, 0.5081],\n",
      "         [0.2211, 0.4989],\n",
      "         [0.1521, 0.5040],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2049, 0.5468],\n",
      "         [0.2313, 0.5424],\n",
      "         [0.1773, 0.6239],\n",
      "         [0.2118, 0.6198],\n",
      "         [0.1868, 0.7242],\n",
      "         [0.2379, 0.6941]]])\n",
      "\n",
      "0: 640x640 1 person, 146.3ms\n",
      "Speed: 1.4ms preprocess, 146.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [171, 155, 140],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4128684997558594, 'inference': 146.29197120666504, 'postprocess': 0.8022785186767578}]\n",
      "Bounding Box: tensor([[ 70.0000, 204.5000,  42.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4226, 0.5380, 0.0466, 0.9360, 0.1108, 0.9970, 0.9517, 0.9877, 0.5648, 0.9317, 0.3612, 0.9987, 0.9949, 0.9978, 0.9901, 0.9901, 0.9736]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.2260e-01],\n",
      "         [6.2253e+01, 1.4590e+02, 5.3802e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6612e-02],\n",
      "         [6.5701e+01, 1.4703e+02, 9.3604e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1084e-01],\n",
      "         [6.6082e+01, 1.6209e+02, 9.9700e-01],\n",
      "         [8.0287e+01, 1.6095e+02, 9.5172e-01],\n",
      "         [6.4400e+01, 1.8242e+02, 9.8766e-01],\n",
      "         [8.1155e+01, 1.7912e+02, 5.6477e-01],\n",
      "         [5.5283e+01, 1.8180e+02, 9.3175e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6116e-01],\n",
      "         [7.2864e+01, 1.9852e+02, 9.9869e-01],\n",
      "         [8.3854e+01, 1.9705e+02, 9.9489e-01],\n",
      "         [6.3265e+01, 2.2530e+02, 9.9784e-01],\n",
      "         [7.8884e+01, 2.2410e+02, 9.9013e-01],\n",
      "         [6.7321e+01, 2.6142e+02, 9.9006e-01],\n",
      "         [8.5702e+01, 2.5005e+02, 9.7360e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 62.2527, 145.9021],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.7009, 147.0258],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.0818, 162.0880],\n",
      "         [ 80.2865, 160.9473],\n",
      "         [ 64.4004, 182.4163],\n",
      "         [ 81.1549, 179.1193],\n",
      "         [ 55.2825, 181.7981],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.8643, 198.5164],\n",
      "         [ 83.8539, 197.0527],\n",
      "         [ 63.2650, 225.2989],\n",
      "         [ 78.8837, 224.0968],\n",
      "         [ 67.3212, 261.4192],\n",
      "         [ 85.7022, 250.0523]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1729, 0.4053],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1825, 0.4084],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1836, 0.4502],\n",
      "         [0.2230, 0.4471],\n",
      "         [0.1789, 0.5067],\n",
      "         [0.2254, 0.4976],\n",
      "         [0.1536, 0.5050],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2024, 0.5514],\n",
      "         [0.2329, 0.5474],\n",
      "         [0.1757, 0.6258],\n",
      "         [0.2191, 0.6225],\n",
      "         [0.1870, 0.7262],\n",
      "         [0.2381, 0.6946]]])\n",
      "\n",
      "0: 640x640 1 person, 133.1ms\n",
      "Speed: 1.3ms preprocess, 133.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [171, 155, 140],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [172, 157, 141],\n",
      "        [171, 155, 140]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2841224670410156, 'inference': 133.09025764465332, 'postprocess': 0.6101131439208984}]\n",
      "Bounding Box: tensor([[ 69.5000, 203.5000,  43.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3409, 0.4535, 0.0335, 0.9392, 0.1186, 0.9974, 0.9580, 0.9883, 0.5717, 0.9247, 0.3338, 0.9989, 0.9957, 0.9980, 0.9909, 0.9904, 0.9741]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.4087e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5346e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3485e-02],\n",
      "         [6.6066e+01, 1.4717e+02, 9.3924e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1862e-01],\n",
      "         [6.6385e+01, 1.6174e+02, 9.9740e-01],\n",
      "         [8.1554e+01, 1.6080e+02, 9.5805e-01],\n",
      "         [6.4172e+01, 1.8169e+02, 9.8834e-01],\n",
      "         [8.0534e+01, 1.7865e+02, 5.7168e-01],\n",
      "         [5.4587e+01, 1.8013e+02, 9.2474e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3378e-01],\n",
      "         [7.2684e+01, 1.9768e+02, 9.9889e-01],\n",
      "         [8.4459e+01, 1.9640e+02, 9.9566e-01],\n",
      "         [6.3267e+01, 2.2446e+02, 9.9802e-01],\n",
      "         [8.0009e+01, 2.2411e+02, 9.9087e-01],\n",
      "         [6.6602e+01, 2.6052e+02, 9.9040e-01],\n",
      "         [8.5590e+01, 2.5050e+02, 9.7410e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.0662, 147.1737],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.3855, 161.7391],\n",
      "         [ 81.5543, 160.7996],\n",
      "         [ 64.1715, 181.6900],\n",
      "         [ 80.5336, 178.6516],\n",
      "         [ 54.5874, 180.1302],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.6844, 197.6812],\n",
      "         [ 84.4594, 196.4041],\n",
      "         [ 63.2669, 224.4584],\n",
      "         [ 80.0085, 224.1073],\n",
      "         [ 66.6017, 260.5171],\n",
      "         [ 85.5901, 250.4999]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1835, 0.4088],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1844, 0.4493],\n",
      "         [0.2265, 0.4467],\n",
      "         [0.1783, 0.5047],\n",
      "         [0.2237, 0.4963],\n",
      "         [0.1516, 0.5004],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2019, 0.5491],\n",
      "         [0.2346, 0.5456],\n",
      "         [0.1757, 0.6235],\n",
      "         [0.2222, 0.6225],\n",
      "         [0.1850, 0.7237],\n",
      "         [0.2378, 0.6958]]])\n",
      "\n",
      "0: 640x640 1 person, 125.5ms\n",
      "Speed: 1.3ms preprocess, 125.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[159, 175, 202],\n",
      "        [159, 175, 202],\n",
      "        [159, 175, 202],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [171, 155, 140],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [172, 157, 141],\n",
      "        [171, 155, 140]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.264810562133789, 'inference': 125.45609474182129, 'postprocess': 0.42176246643066406}]\n",
      "Bounding Box: tensor([[ 70., 203.,  44., 136.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3276, 0.4378, 0.0342, 0.9335, 0.1179, 0.9966, 0.9554, 0.9835, 0.5588, 0.9017, 0.3201, 0.9988, 0.9957, 0.9981, 0.9923, 0.9919, 0.9800]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.2764e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3781e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4161e-02],\n",
      "         [6.6925e+01, 1.4708e+02, 9.3350e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1789e-01],\n",
      "         [6.6346e+01, 1.6124e+02, 9.9659e-01],\n",
      "         [8.3309e+01, 1.6063e+02, 9.5542e-01],\n",
      "         [6.4235e+01, 1.8027e+02, 9.8350e-01],\n",
      "         [8.3859e+01, 1.7869e+02, 5.5884e-01],\n",
      "         [5.5799e+01, 1.7944e+02, 9.0165e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2005e-01],\n",
      "         [7.2424e+01, 1.9604e+02, 9.9876e-01],\n",
      "         [8.5468e+01, 1.9490e+02, 9.9569e-01],\n",
      "         [6.2713e+01, 2.2358e+02, 9.9811e-01],\n",
      "         [8.1430e+01, 2.2350e+02, 9.9234e-01],\n",
      "         [6.5648e+01, 2.5975e+02, 9.9192e-01],\n",
      "         [8.5983e+01, 2.5038e+02, 9.7998e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.9249, 147.0789],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.3460, 161.2447],\n",
      "         [ 83.3087, 160.6290],\n",
      "         [ 64.2353, 180.2670],\n",
      "         [ 83.8589, 178.6897],\n",
      "         [ 55.7994, 179.4413],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.4244, 196.0420],\n",
      "         [ 85.4681, 194.8962],\n",
      "         [ 62.7132, 223.5813],\n",
      "         [ 81.4299, 223.5019],\n",
      "         [ 65.6481, 259.7473],\n",
      "         [ 85.9826, 250.3835]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1859, 0.4086],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1843, 0.4479],\n",
      "         [0.2314, 0.4462],\n",
      "         [0.1784, 0.5007],\n",
      "         [0.2329, 0.4964],\n",
      "         [0.1550, 0.4984],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2012, 0.5446],\n",
      "         [0.2374, 0.5414],\n",
      "         [0.1742, 0.6211],\n",
      "         [0.2262, 0.6208],\n",
      "         [0.1824, 0.7215],\n",
      "         [0.2388, 0.6955]]])\n",
      "\n",
      "0: 640x640 1 person, 118.2ms\n",
      "Speed: 1.3ms preprocess, 118.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [171, 155, 140],\n",
      "        [171, 155, 140]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [171, 155, 140],\n",
      "        [172, 157, 141]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [172, 157, 141],\n",
      "        [173, 158, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3082027435302734, 'inference': 118.18432807922363, 'postprocess': 0.4229545593261719}]\n",
      "Bounding Box: tensor([[ 71.0000, 202.5000,  44.0000, 135.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3074, 0.4064, 0.0393, 0.9286, 0.1530, 0.9950, 0.9568, 0.9681, 0.5475, 0.8229, 0.2805, 0.9981, 0.9947, 0.9972, 0.9912, 0.9901, 0.9794]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.0742e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0644e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9270e-02],\n",
      "         [6.7542e+01, 1.4704e+02, 9.2858e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5303e-01],\n",
      "         [6.6503e+01, 1.6154e+02, 9.9503e-01],\n",
      "         [8.5064e+01, 1.6053e+02, 9.5685e-01],\n",
      "         [6.4919e+01, 1.8096e+02, 9.6808e-01],\n",
      "         [8.9013e+01, 1.7955e+02, 5.4754e-01],\n",
      "         [5.8071e+01, 1.7952e+02, 8.2286e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8050e-01],\n",
      "         [7.2489e+01, 1.9697e+02, 9.9805e-01],\n",
      "         [8.6321e+01, 1.9573e+02, 9.9469e-01],\n",
      "         [6.1897e+01, 2.2325e+02, 9.9719e-01],\n",
      "         [8.2230e+01, 2.2320e+02, 9.9124e-01],\n",
      "         [6.5841e+01, 2.5888e+02, 9.9007e-01],\n",
      "         [8.5780e+01, 2.4990e+02, 9.7939e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5417, 147.0376],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.5030, 161.5392],\n",
      "         [ 85.0640, 160.5335],\n",
      "         [ 64.9185, 180.9587],\n",
      "         [ 89.0133, 179.5470],\n",
      "         [ 58.0711, 179.5156],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.4892, 196.9740],\n",
      "         [ 86.3206, 195.7281],\n",
      "         [ 61.8969, 223.2483],\n",
      "         [ 82.2297, 223.2032],\n",
      "         [ 65.8407, 258.8809],\n",
      "         [ 85.7800, 249.8972]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1876, 0.4084],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1847, 0.4487],\n",
      "         [0.2363, 0.4459],\n",
      "         [0.1803, 0.5027],\n",
      "         [0.2473, 0.4987],\n",
      "         [0.1613, 0.4987],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2014, 0.5472],\n",
      "         [0.2398, 0.5437],\n",
      "         [0.1719, 0.6201],\n",
      "         [0.2284, 0.6200],\n",
      "         [0.1829, 0.7191],\n",
      "         [0.2383, 0.6942]]])\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.2ms preprocess, 116.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2428760528564453, 'inference': 116.05405807495117, 'postprocess': 0.43702125549316406}]\n",
      "Bounding Box: tensor([[ 72.0000, 202.5000,  44.0000, 135.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2439, 0.3268, 0.0329, 0.9207, 0.1666, 0.9939, 0.9602, 0.9535, 0.5562, 0.7472, 0.2555, 0.9981, 0.9955, 0.9973, 0.9929, 0.9918, 0.9847]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4392e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2679e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2905e-02],\n",
      "         [6.7995e+01, 1.4719e+02, 9.2070e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6656e-01],\n",
      "         [6.6770e+01, 1.6153e+02, 9.9389e-01],\n",
      "         [8.5620e+01, 1.6051e+02, 9.6015e-01],\n",
      "         [6.5717e+01, 1.8123e+02, 9.5345e-01],\n",
      "         [9.0662e+01, 1.8041e+02, 5.5624e-01],\n",
      "         [6.0838e+01, 1.8015e+02, 7.4717e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5548e-01],\n",
      "         [7.2792e+01, 1.9828e+02, 9.9806e-01],\n",
      "         [8.6685e+01, 1.9720e+02, 9.9551e-01],\n",
      "         [6.1413e+01, 2.2279e+02, 9.9730e-01],\n",
      "         [8.2873e+01, 2.2351e+02, 9.9292e-01],\n",
      "         [6.6402e+01, 2.5788e+02, 9.9181e-01],\n",
      "         [8.5527e+01, 2.5030e+02, 9.8474e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.9948, 147.1877],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.7696, 161.5279],\n",
      "         [ 85.6198, 160.5056],\n",
      "         [ 65.7165, 181.2272],\n",
      "         [ 90.6620, 180.4115],\n",
      "         [ 60.8382, 180.1495],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.7920, 198.2801],\n",
      "         [ 86.6851, 197.2042],\n",
      "         [ 61.4128, 222.7891],\n",
      "         [ 82.8730, 223.5060],\n",
      "         [ 66.4019, 257.8765],\n",
      "         [ 85.5274, 250.3040]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1889, 0.4089],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1855, 0.4487],\n",
      "         [0.2378, 0.4458],\n",
      "         [0.1825, 0.5034],\n",
      "         [0.2518, 0.5011],\n",
      "         [0.1690, 0.5004],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2022, 0.5508],\n",
      "         [0.2408, 0.5478],\n",
      "         [0.1706, 0.6189],\n",
      "         [0.2302, 0.6209],\n",
      "         [0.1844, 0.7163],\n",
      "         [0.2376, 0.6953]]])\n",
      "\n",
      "0: 640x640 1 person, 138.5ms\n",
      "Speed: 1.3ms preprocess, 138.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2829303741455078, 'inference': 138.49210739135742, 'postprocess': 0.6661415100097656}]\n",
      "Bounding Box: tensor([[ 74., 202.,  46., 134.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3224, 0.3273, 0.0480, 0.9205, 0.2699, 0.9969, 0.9880, 0.9767, 0.8310, 0.8599, 0.5456, 0.9992, 0.9985, 0.9983, 0.9965, 0.9933, 0.9891]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.2245e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2733e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7988e-02],\n",
      "         [6.8526e+01, 1.4739e+02, 9.2049e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6990e-01],\n",
      "         [6.7410e+01, 1.6104e+02, 9.9693e-01],\n",
      "         [8.7239e+01, 1.6058e+02, 9.8803e-01],\n",
      "         [6.5095e+01, 1.7911e+02, 9.7669e-01],\n",
      "         [9.3885e+01, 1.8135e+02, 8.3104e-01],\n",
      "         [5.5907e+01, 1.7688e+02, 8.5994e-01],\n",
      "         [8.6544e+01, 1.9247e+02, 5.4555e-01],\n",
      "         [7.3548e+01, 1.9806e+02, 9.9919e-01],\n",
      "         [8.7710e+01, 1.9720e+02, 9.9852e-01],\n",
      "         [6.1060e+01, 2.2066e+02, 9.9834e-01],\n",
      "         [8.3112e+01, 2.2268e+02, 9.9651e-01],\n",
      "         [6.7118e+01, 2.5571e+02, 9.9327e-01],\n",
      "         [8.5533e+01, 2.4984e+02, 9.8915e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.5256, 147.3905],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.4101, 161.0378],\n",
      "         [ 87.2390, 160.5839],\n",
      "         [ 65.0954, 179.1123],\n",
      "         [ 93.8854, 181.3505],\n",
      "         [ 55.9070, 176.8768],\n",
      "         [ 86.5437, 192.4737],\n",
      "         [ 73.5476, 198.0561],\n",
      "         [ 87.7098, 197.1984],\n",
      "         [ 61.0598, 220.6626],\n",
      "         [ 83.1125, 222.6844],\n",
      "         [ 67.1180, 255.7122],\n",
      "         [ 85.5325, 249.8384]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1903, 0.4094],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1873, 0.4473],\n",
      "         [0.2423, 0.4461],\n",
      "         [0.1808, 0.4975],\n",
      "         [0.2608, 0.5038],\n",
      "         [0.1553, 0.4913],\n",
      "         [0.2404, 0.5346],\n",
      "         [0.2043, 0.5502],\n",
      "         [0.2436, 0.5478],\n",
      "         [0.1696, 0.6130],\n",
      "         [0.2309, 0.6186],\n",
      "         [0.1864, 0.7103],\n",
      "         [0.2376, 0.6940]]])\n",
      "\n",
      "0: 640x640 1 person, 121.1ms\n",
      "Speed: 1.4ms preprocess, 121.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139],\n",
      "        [171, 155, 140]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3570785522460938, 'inference': 121.06919288635254, 'postprocess': 0.4191398620605469}]\n",
      "Bounding Box: tensor([[ 77.0000, 200.5000,  46.0000, 129.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2343, 0.2210, 0.0459, 0.8358, 0.2502, 0.9885, 0.9809, 0.9112, 0.8099, 0.6866, 0.5113, 0.9983, 0.9981, 0.9966, 0.9956, 0.9883, 0.9866]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3430e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2101e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5896e-02],\n",
      "         [6.8670e+01, 1.4732e+02, 8.3581e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5021e-01],\n",
      "         [6.7564e+01, 1.6090e+02, 9.8847e-01],\n",
      "         [8.8130e+01, 1.6015e+02, 9.8088e-01],\n",
      "         [6.4328e+01, 1.7988e+02, 9.1121e-01],\n",
      "         [9.6442e+01, 1.8040e+02, 8.0994e-01],\n",
      "         [5.8008e+01, 1.7761e+02, 6.8658e-01],\n",
      "         [9.1231e+01, 1.9152e+02, 5.1127e-01],\n",
      "         [7.4749e+01, 1.9719e+02, 9.9832e-01],\n",
      "         [8.9215e+01, 1.9631e+02, 9.9806e-01],\n",
      "         [6.2520e+01, 2.2012e+02, 9.9659e-01],\n",
      "         [8.3329e+01, 2.2206e+02, 9.9560e-01],\n",
      "         [6.7979e+01, 2.5408e+02, 9.8827e-01],\n",
      "         [8.6855e+01, 2.5037e+02, 9.8661e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.6703, 147.3157],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5638, 160.8952],\n",
      "         [ 88.1302, 160.1475],\n",
      "         [ 64.3276, 179.8806],\n",
      "         [ 96.4416, 180.4036],\n",
      "         [ 58.0075, 177.6145],\n",
      "         [ 91.2310, 191.5181],\n",
      "         [ 74.7485, 197.1940],\n",
      "         [ 89.2152, 196.3058],\n",
      "         [ 62.5204, 220.1212],\n",
      "         [ 83.3293, 222.0564],\n",
      "         [ 67.9795, 254.0820],\n",
      "         [ 86.8555, 250.3730]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1908, 0.4092],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1877, 0.4469],\n",
      "         [0.2448, 0.4449],\n",
      "         [0.1787, 0.4997],\n",
      "         [0.2679, 0.5011],\n",
      "         [0.1611, 0.4934],\n",
      "         [0.2534, 0.5320],\n",
      "         [0.2076, 0.5478],\n",
      "         [0.2478, 0.5453],\n",
      "         [0.1737, 0.6114],\n",
      "         [0.2315, 0.6168],\n",
      "         [0.1888, 0.7058],\n",
      "         [0.2413, 0.6955]]])\n",
      "\n",
      "0: 640x640 1 person, 131.2ms\n",
      "Speed: 1.2ms preprocess, 131.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190],\n",
      "        [147, 164, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139],\n",
      "        [168, 153, 138]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [167, 152, 137],\n",
      "        [168, 153, 138],\n",
      "        [168, 153, 138]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [168, 153, 138],\n",
      "        [169, 154, 139],\n",
      "        [169, 154, 139]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2471675872802734, 'inference': 131.20293617248535, 'postprocess': 0.43582916259765625}]\n",
      "Bounding Box: tensor([[ 80., 199.,  48., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2246, 0.1814, 0.0457, 0.8479, 0.3037, 0.9905, 0.9911, 0.9064, 0.9010, 0.6529, 0.6324, 0.9989, 0.9991, 0.9978, 0.9979, 0.9931, 0.9936]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2460e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8138e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5675e-02],\n",
      "         [6.8585e+01, 1.4780e+02, 8.4790e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0371e-01],\n",
      "         [6.6665e+01, 1.6120e+02, 9.9045e-01],\n",
      "         [8.8059e+01, 1.6003e+02, 9.9110e-01],\n",
      "         [6.4363e+01, 1.7868e+02, 9.0639e-01],\n",
      "         [9.7038e+01, 1.7768e+02, 9.0097e-01],\n",
      "         [6.1989e+01, 1.7903e+02, 6.5289e-01],\n",
      "         [1.0014e+02, 1.8770e+02, 6.3242e-01],\n",
      "         [7.3966e+01, 1.9772e+02, 9.9891e-01],\n",
      "         [8.8849e+01, 1.9702e+02, 9.9907e-01],\n",
      "         [6.1588e+01, 2.1910e+02, 9.9781e-01],\n",
      "         [8.3128e+01, 2.2217e+02, 9.9794e-01],\n",
      "         [6.9583e+01, 2.5094e+02, 9.9307e-01],\n",
      "         [8.5718e+01, 2.4957e+02, 9.9359e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.5845, 147.7991],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.6651, 161.2033],\n",
      "         [ 88.0586, 160.0309],\n",
      "         [ 64.3626, 178.6797],\n",
      "         [ 97.0380, 177.6764],\n",
      "         [ 61.9891, 179.0319],\n",
      "         [100.1445, 187.6997],\n",
      "         [ 73.9664, 197.7157],\n",
      "         [ 88.8491, 197.0220],\n",
      "         [ 61.5885, 219.0995],\n",
      "         [ 83.1282, 222.1672],\n",
      "         [ 69.5832, 250.9445],\n",
      "         [ 85.7184, 249.5712]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1905, 0.4106],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1852, 0.4478],\n",
      "         [0.2446, 0.4445],\n",
      "         [0.1788, 0.4963],\n",
      "         [0.2696, 0.4935],\n",
      "         [0.1722, 0.4973],\n",
      "         [0.2782, 0.5214],\n",
      "         [0.2055, 0.5492],\n",
      "         [0.2468, 0.5473],\n",
      "         [0.1711, 0.6086],\n",
      "         [0.2309, 0.6171],\n",
      "         [0.1933, 0.6971],\n",
      "         [0.2381, 0.6933]]])\n",
      "\n",
      "0: 640x640 1 person, 113.4ms\n",
      "Speed: 1.3ms preprocess, 113.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[159, 175, 202],\n",
      "        [159, 175, 202],\n",
      "        [159, 175, 202],\n",
      "        ...,\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [166, 151, 136],\n",
      "        [167, 152, 137],\n",
      "        [169, 154, 139]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [165, 150, 134],\n",
      "        [166, 151, 136],\n",
      "        [167, 152, 137]],\n",
      "\n",
      "       [[159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        [159, 146, 127],\n",
      "        ...,\n",
      "        [165, 150, 134],\n",
      "        [165, 150, 134],\n",
      "        [166, 151, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2540817260742188, 'inference': 113.44003677368164, 'postprocess': 0.4611015319824219}]\n",
      "Bounding Box: tensor([[ 82.5000, 198.5000,  51.0000, 127.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1597, 0.1081, 0.0376, 0.7988, 0.3422, 0.9858, 0.9931, 0.8426, 0.9278, 0.5150, 0.6646, 0.9988, 0.9992, 0.9974, 0.9983, 0.9923, 0.9944]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.5973e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0810e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7619e-02],\n",
      "         [6.8782e+01, 1.4723e+02, 7.9879e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4216e-01],\n",
      "         [6.6453e+01, 1.6086e+02, 9.8577e-01],\n",
      "         [8.8356e+01, 1.6013e+02, 9.9308e-01],\n",
      "         [6.4560e+01, 1.7886e+02, 8.4257e-01],\n",
      "         [9.6968e+01, 1.7777e+02, 9.2779e-01],\n",
      "         [6.8401e+01, 1.8421e+02, 5.1501e-01],\n",
      "         [1.0372e+02, 1.8739e+02, 6.6458e-01],\n",
      "         [7.3454e+01, 1.9615e+02, 9.9876e-01],\n",
      "         [8.8786e+01, 1.9569e+02, 9.9924e-01],\n",
      "         [6.3048e+01, 2.1897e+02, 9.9745e-01],\n",
      "         [8.3455e+01, 2.2142e+02, 9.9830e-01],\n",
      "         [7.0443e+01, 2.5111e+02, 9.9228e-01],\n",
      "         [8.5502e+01, 2.4955e+02, 9.9442e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.7820, 147.2260],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.4525, 160.8646],\n",
      "         [ 88.3564, 160.1346],\n",
      "         [ 64.5603, 178.8646],\n",
      "         [ 96.9682, 177.7743],\n",
      "         [ 68.4012, 184.2130],\n",
      "         [103.7215, 187.3889],\n",
      "         [ 73.4538, 196.1488],\n",
      "         [ 88.7857, 195.6892],\n",
      "         [ 63.0485, 218.9701],\n",
      "         [ 83.4554, 221.4192],\n",
      "         [ 70.4432, 251.1109],\n",
      "         [ 85.5016, 249.5546]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1911, 0.4090],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1846, 0.4468],\n",
      "         [0.2454, 0.4448],\n",
      "         [0.1793, 0.4968],\n",
      "         [0.2694, 0.4938],\n",
      "         [0.1900, 0.5117],\n",
      "         [0.2881, 0.5205],\n",
      "         [0.2040, 0.5449],\n",
      "         [0.2466, 0.5436],\n",
      "         [0.1751, 0.6083],\n",
      "         [0.2318, 0.6151],\n",
      "         [0.1957, 0.6975],\n",
      "         [0.2375, 0.6932]]])\n",
      "\n",
      "0: 640x640 1 person, 111.8ms\n",
      "Speed: 1.3ms preprocess, 111.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 147, 134],\n",
      "        [169, 148, 136],\n",
      "        [171, 150, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 147, 134],\n",
      "        [168, 147, 134],\n",
      "        [169, 148, 136]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [168, 147, 134],\n",
      "        [171, 150, 137],\n",
      "        [172, 151, 138]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3070106506347656, 'inference': 111.82832717895508, 'postprocess': 0.42891502380371094}]\n",
      "Bounding Box: tensor([[ 85., 197.,  52., 124.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1833, 0.1025, 0.0516, 0.6992, 0.3858, 0.9812, 0.9925, 0.8529, 0.9495, 0.5944, 0.7799, 0.9985, 0.9991, 0.9956, 0.9972, 0.9841, 0.9889]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.8334e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0248e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.1564e-02],\n",
      "         [6.9035e+01, 1.4687e+02, 6.9915e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8582e-01],\n",
      "         [6.7174e+01, 1.6060e+02, 9.8115e-01],\n",
      "         [8.8568e+01, 1.5994e+02, 9.9250e-01],\n",
      "         [6.4500e+01, 1.7853e+02, 8.5286e-01],\n",
      "         [9.8244e+01, 1.7659e+02, 9.4947e-01],\n",
      "         [6.7015e+01, 1.8678e+02, 5.9435e-01],\n",
      "         [1.0703e+02, 1.8753e+02, 7.7989e-01],\n",
      "         [7.5009e+01, 1.9651e+02, 9.9845e-01],\n",
      "         [8.9505e+01, 1.9627e+02, 9.9911e-01],\n",
      "         [6.4558e+01, 2.1775e+02, 9.9555e-01],\n",
      "         [8.2445e+01, 2.2121e+02, 9.9723e-01],\n",
      "         [7.2704e+01, 2.4846e+02, 9.8411e-01],\n",
      "         [8.4905e+01, 2.4995e+02, 9.8887e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0354, 146.8727],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.1736, 160.6013],\n",
      "         [ 88.5682, 159.9414],\n",
      "         [ 64.4999, 178.5343],\n",
      "         [ 98.2436, 176.5929],\n",
      "         [ 67.0152, 186.7762],\n",
      "         [107.0301, 187.5347],\n",
      "         [ 75.0092, 196.5124],\n",
      "         [ 89.5047, 196.2685],\n",
      "         [ 64.5577, 217.7515],\n",
      "         [ 82.4447, 221.2145],\n",
      "         [ 72.7037, 248.4644],\n",
      "         [ 84.9053, 249.9452]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1918, 0.4080],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1866, 0.4461],\n",
      "         [0.2460, 0.4443],\n",
      "         [0.1792, 0.4959],\n",
      "         [0.2729, 0.4905],\n",
      "         [0.1862, 0.5188],\n",
      "         [0.2973, 0.5209],\n",
      "         [0.2084, 0.5459],\n",
      "         [0.2486, 0.5452],\n",
      "         [0.1793, 0.6049],\n",
      "         [0.2290, 0.6145],\n",
      "         [0.2020, 0.6902],\n",
      "         [0.2358, 0.6943]]])\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.3ms preprocess, 114.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 147, 134],\n",
      "        [169, 148, 136],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 148, 136],\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [172, 151, 138],\n",
      "        [171, 150, 137],\n",
      "        [169, 148, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2590885162353516, 'inference': 114.18914794921875, 'postprocess': 0.39696693420410156}]\n",
      "Bounding Box: tensor([[ 87.0000, 196.5000,  52.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2589, 0.1446, 0.0871, 0.6649, 0.3624, 0.9738, 0.9882, 0.7937, 0.9348, 0.5383, 0.7635, 0.9965, 0.9981, 0.9901, 0.9944, 0.9652, 0.9769]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5886e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4463e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.7120e-02],\n",
      "         [6.9471e+01, 1.4690e+02, 6.6490e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6241e-01],\n",
      "         [6.7889e+01, 1.6076e+02, 9.7379e-01],\n",
      "         [8.8793e+01, 1.5994e+02, 9.8816e-01],\n",
      "         [6.5413e+01, 1.7873e+02, 7.9369e-01],\n",
      "         [9.8778e+01, 1.7672e+02, 9.3480e-01],\n",
      "         [6.9544e+01, 1.8520e+02, 5.3825e-01],\n",
      "         [1.0824e+02, 1.8838e+02, 7.6351e-01],\n",
      "         [7.5597e+01, 1.9701e+02, 9.9648e-01],\n",
      "         [8.9503e+01, 1.9655e+02, 9.9806e-01],\n",
      "         [6.7191e+01, 2.1732e+02, 9.9006e-01],\n",
      "         [8.3309e+01, 2.2090e+02, 9.9438e-01],\n",
      "         [7.4516e+01, 2.4638e+02, 9.6518e-01],\n",
      "         [8.6148e+01, 2.4917e+02, 9.7692e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.4712, 146.8985],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.8891, 160.7620],\n",
      "         [ 88.7934, 159.9365],\n",
      "         [ 65.4132, 178.7299],\n",
      "         [ 98.7785, 176.7172],\n",
      "         [ 69.5435, 185.1967],\n",
      "         [108.2433, 188.3823],\n",
      "         [ 75.5973, 197.0137],\n",
      "         [ 89.5031, 196.5541],\n",
      "         [ 67.1907, 217.3154],\n",
      "         [ 83.3087, 220.8985],\n",
      "         [ 74.5161, 246.3834],\n",
      "         [ 86.1477, 249.1712]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1930, 0.4081],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1886, 0.4466],\n",
      "         [0.2466, 0.4443],\n",
      "         [0.1817, 0.4965],\n",
      "         [0.2744, 0.4909],\n",
      "         [0.1932, 0.5144],\n",
      "         [0.3007, 0.5233],\n",
      "         [0.2100, 0.5473],\n",
      "         [0.2486, 0.5460],\n",
      "         [0.1866, 0.6037],\n",
      "         [0.2314, 0.6136],\n",
      "         [0.2070, 0.6844],\n",
      "         [0.2393, 0.6921]]])\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [173, 152, 139],\n",
      "        [169, 148, 136]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2507438659667969, 'inference': 116.47534370422363, 'postprocess': 0.396728515625}]\n",
      "Bounding Box: tensor([[ 88.5000, 197.0000,  55.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2269, 0.1082, 0.0814, 0.5954, 0.3931, 0.9714, 0.9902, 0.7923, 0.9534, 0.5495, 0.8162, 0.9971, 0.9986, 0.9915, 0.9957, 0.9696, 0.9814]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2689e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0820e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.1368e-02],\n",
      "         [6.9358e+01, 1.4728e+02, 5.9538e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9308e-01],\n",
      "         [6.7572e+01, 1.6081e+02, 9.7140e-01],\n",
      "         [8.9041e+01, 1.6019e+02, 9.9023e-01],\n",
      "         [6.4836e+01, 1.7824e+02, 7.9230e-01],\n",
      "         [1.0002e+02, 1.7753e+02, 9.5335e-01],\n",
      "         [6.7358e+01, 1.8163e+02, 5.4953e-01],\n",
      "         [1.1041e+02, 1.8978e+02, 8.1622e-01],\n",
      "         [7.5915e+01, 1.9685e+02, 9.9709e-01],\n",
      "         [8.9858e+01, 1.9646e+02, 9.9859e-01],\n",
      "         [6.8061e+01, 2.1702e+02, 9.9151e-01],\n",
      "         [8.3470e+01, 2.2061e+02, 9.9575e-01],\n",
      "         [7.6970e+01, 2.4621e+02, 9.6958e-01],\n",
      "         [8.6254e+01, 2.4916e+02, 9.8141e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3578, 147.2786],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5716, 160.8062],\n",
      "         [ 89.0409, 160.1914],\n",
      "         [ 64.8355, 178.2432],\n",
      "         [100.0188, 177.5330],\n",
      "         [ 67.3581, 181.6292],\n",
      "         [110.4113, 189.7832],\n",
      "         [ 75.9147, 196.8535],\n",
      "         [ 89.8581, 196.4552],\n",
      "         [ 68.0609, 217.0182],\n",
      "         [ 83.4704, 220.6089],\n",
      "         [ 76.9702, 246.2128],\n",
      "         [ 86.2543, 249.1590]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1927, 0.4091],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1877, 0.4467],\n",
      "         [0.2473, 0.4450],\n",
      "         [0.1801, 0.4951],\n",
      "         [0.2778, 0.4931],\n",
      "         [0.1871, 0.5045],\n",
      "         [0.3067, 0.5272],\n",
      "         [0.2109, 0.5468],\n",
      "         [0.2496, 0.5457],\n",
      "         [0.1891, 0.6028],\n",
      "         [0.2319, 0.6128],\n",
      "         [0.2138, 0.6839],\n",
      "         [0.2396, 0.6921]]])\n",
      "\n",
      "0: 640x640 1 person, 117.6ms\n",
      "Speed: 1.3ms preprocess, 117.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [173, 152, 139],\n",
      "        [169, 148, 136]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2569427490234375, 'inference': 117.58804321289062, 'postprocess': 0.43082237243652344}]\n",
      "Bounding Box: tensor([[ 88.5000, 197.0000,  57.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1837, 0.0734, 0.0656, 0.5110, 0.4063, 0.9735, 0.9914, 0.8360, 0.9668, 0.6208, 0.8657, 0.9980, 0.9991, 0.9937, 0.9969, 0.9749, 0.9846]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.8375e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.3391e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.5645e-02],\n",
      "         [6.9115e+01, 1.4742e+02, 5.1101e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0634e-01],\n",
      "         [6.7510e+01, 1.6044e+02, 9.7348e-01],\n",
      "         [8.8479e+01, 1.5986e+02, 9.9143e-01],\n",
      "         [6.4329e+01, 1.7831e+02, 8.3601e-01],\n",
      "         [9.9540e+01, 1.7750e+02, 9.6680e-01],\n",
      "         [6.5296e+01, 1.8411e+02, 6.2083e-01],\n",
      "         [1.1116e+02, 1.8973e+02, 8.6565e-01],\n",
      "         [7.5956e+01, 1.9516e+02, 9.9804e-01],\n",
      "         [8.9392e+01, 1.9479e+02, 9.9905e-01],\n",
      "         [6.9352e+01, 2.1712e+02, 9.9373e-01],\n",
      "         [8.4001e+01, 2.2021e+02, 9.9686e-01],\n",
      "         [7.8233e+01, 2.4571e+02, 9.7487e-01],\n",
      "         [8.6232e+01, 2.4905e+02, 9.8456e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.1146, 147.4198],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5102, 160.4420],\n",
      "         [ 88.4791, 159.8625],\n",
      "         [ 64.3289, 178.3147],\n",
      "         [ 99.5403, 177.5040],\n",
      "         [ 65.2959, 184.1071],\n",
      "         [111.1553, 189.7322],\n",
      "         [ 75.9565, 195.1649],\n",
      "         [ 89.3922, 194.7922],\n",
      "         [ 69.3519, 217.1153],\n",
      "         [ 84.0010, 220.2101],\n",
      "         [ 78.2333, 245.7069],\n",
      "         [ 86.2322, 249.0491]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1920, 0.4095],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1875, 0.4457],\n",
      "         [0.2458, 0.4441],\n",
      "         [0.1787, 0.4953],\n",
      "         [0.2765, 0.4931],\n",
      "         [0.1814, 0.5114],\n",
      "         [0.3088, 0.5270],\n",
      "         [0.2110, 0.5421],\n",
      "         [0.2483, 0.5411],\n",
      "         [0.1926, 0.6031],\n",
      "         [0.2333, 0.6117],\n",
      "         [0.2173, 0.6825],\n",
      "         [0.2395, 0.6918]]])\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.3ms preprocess, 114.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [173, 152, 139],\n",
      "        [169, 148, 136]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3110637664794922, 'inference': 114.1669750213623, 'postprocess': 0.4200935363769531}]\n",
      "Bounding Box: tensor([[ 89.0000, 196.5000,  60.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2500, 0.1159, 0.0767, 0.6061, 0.3951, 0.9854, 0.9912, 0.9170, 0.9626, 0.7715, 0.8692, 0.9985, 0.9990, 0.9949, 0.9964, 0.9759, 0.9810]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4998e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1586e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.6731e-02],\n",
      "         [6.9527e+01, 1.4716e+02, 6.0614e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9508e-01],\n",
      "         [6.7990e+01, 1.6014e+02, 9.8539e-01],\n",
      "         [8.8472e+01, 1.5949e+02, 9.9115e-01],\n",
      "         [6.4392e+01, 1.7768e+02, 9.1698e-01],\n",
      "         [9.8844e+01, 1.7731e+02, 9.6262e-01],\n",
      "         [6.2152e+01, 1.8289e+02, 7.7150e-01],\n",
      "         [1.1145e+02, 1.8905e+02, 8.6918e-01],\n",
      "         [7.5779e+01, 1.9461e+02, 9.9855e-01],\n",
      "         [8.8838e+01, 1.9415e+02, 9.9901e-01],\n",
      "         [7.0107e+01, 2.1716e+02, 9.9493e-01],\n",
      "         [8.4805e+01, 2.1974e+02, 9.9639e-01],\n",
      "         [7.8752e+01, 2.4601e+02, 9.7587e-01],\n",
      "         [8.6065e+01, 2.4856e+02, 9.8105e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.5273, 147.1624],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.9900, 160.1357],\n",
      "         [ 88.4718, 159.4862],\n",
      "         [ 64.3918, 177.6785],\n",
      "         [ 98.8441, 177.3080],\n",
      "         [ 62.1517, 182.8883],\n",
      "         [111.4500, 189.0522],\n",
      "         [ 75.7786, 194.6123],\n",
      "         [ 88.8377, 194.1474],\n",
      "         [ 70.1067, 217.1644],\n",
      "         [ 84.8046, 219.7364],\n",
      "         [ 78.7517, 246.0072],\n",
      "         [ 86.0650, 248.5577]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1931, 0.4088],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1889, 0.4448],\n",
      "         [0.2458, 0.4430],\n",
      "         [0.1789, 0.4936],\n",
      "         [0.2746, 0.4925],\n",
      "         [0.1726, 0.5080],\n",
      "         [0.3096, 0.5251],\n",
      "         [0.2105, 0.5406],\n",
      "         [0.2468, 0.5393],\n",
      "         [0.1947, 0.6032],\n",
      "         [0.2356, 0.6104],\n",
      "         [0.2188, 0.6834],\n",
      "         [0.2391, 0.6904]]])\n",
      "\n",
      "0: 640x640 1 person, 113.9ms\n",
      "Speed: 1.2ms preprocess, 113.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [173, 152, 139],\n",
      "        [169, 148, 136]],\n",
      "\n",
      "       [[164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        [164, 151, 130],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2428760528564453, 'inference': 113.8620376586914, 'postprocess': 0.41604042053222656}]\n",
      "Bounding Box: tensor([[ 89.5000, 196.5000,  59.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2072, 0.0908, 0.0663, 0.5569, 0.3827, 0.9805, 0.9904, 0.8896, 0.9623, 0.7161, 0.8627, 0.9983, 0.9990, 0.9944, 0.9965, 0.9750, 0.9821]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.0720e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.0778e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6280e-02],\n",
      "         [6.9354e+01, 1.4742e+02, 5.5690e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8271e-01],\n",
      "         [6.7846e+01, 1.6015e+02, 9.8054e-01],\n",
      "         [8.8414e+01, 1.5909e+02, 9.9040e-01],\n",
      "         [6.4944e+01, 1.7780e+02, 8.8965e-01],\n",
      "         [9.8996e+01, 1.7706e+02, 9.6226e-01],\n",
      "         [6.4214e+01, 1.8243e+02, 7.1611e-01],\n",
      "         [1.1170e+02, 1.8909e+02, 8.6270e-01],\n",
      "         [7.5816e+01, 1.9435e+02, 9.9830e-01],\n",
      "         [8.8940e+01, 1.9369e+02, 9.9896e-01],\n",
      "         [7.0979e+01, 2.1710e+02, 9.9437e-01],\n",
      "         [8.5855e+01, 2.1893e+02, 9.9645e-01],\n",
      "         [7.9493e+01, 2.4620e+02, 9.7500e-01],\n",
      "         [8.6557e+01, 2.4805e+02, 9.8206e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3539, 147.4187],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.8463, 160.1498],\n",
      "         [ 88.4137, 159.0910],\n",
      "         [ 64.9442, 177.8033],\n",
      "         [ 98.9957, 177.0643],\n",
      "         [ 64.2137, 182.4348],\n",
      "         [111.6962, 189.0948],\n",
      "         [ 75.8161, 194.3550],\n",
      "         [ 88.9399, 193.6852],\n",
      "         [ 70.9795, 217.0965],\n",
      "         [ 85.8551, 218.9251],\n",
      "         [ 79.4925, 246.2042],\n",
      "         [ 86.5565, 248.0504]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1926, 0.4095],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1885, 0.4449],\n",
      "         [0.2456, 0.4419],\n",
      "         [0.1804, 0.4939],\n",
      "         [0.2750, 0.4918],\n",
      "         [0.1784, 0.5068],\n",
      "         [0.3103, 0.5253],\n",
      "         [0.2106, 0.5399],\n",
      "         [0.2471, 0.5380],\n",
      "         [0.1972, 0.6030],\n",
      "         [0.2385, 0.6081],\n",
      "         [0.2208, 0.6839],\n",
      "         [0.2404, 0.6890]]])\n",
      "\n",
      "0: 640x640 1 person, 114.7ms\n",
      "Speed: 1.3ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3179779052734375, 'inference': 114.6550178527832, 'postprocess': 0.4360675811767578}]\n",
      "Bounding Box: tensor([[ 90.0000, 196.5000,  60.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3149, 0.1763, 0.1023, 0.6724, 0.3968, 0.9851, 0.9852, 0.9160, 0.9283, 0.7445, 0.7898, 0.9964, 0.9969, 0.9918, 0.9926, 0.9746, 0.9764]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.1492e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7626e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0228e-01],\n",
      "         [6.9325e+01, 1.4713e+02, 6.7241e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9682e-01],\n",
      "         [6.8663e+01, 1.6084e+02, 9.8506e-01],\n",
      "         [8.8006e+01, 1.5907e+02, 9.8517e-01],\n",
      "         [6.5332e+01, 1.7826e+02, 9.1597e-01],\n",
      "         [9.7542e+01, 1.7696e+02, 9.2828e-01],\n",
      "         [6.2740e+01, 1.7898e+02, 7.4449e-01],\n",
      "         [1.1020e+02, 1.8912e+02, 7.8978e-01],\n",
      "         [7.6651e+01, 1.9537e+02, 9.9642e-01],\n",
      "         [8.9163e+01, 1.9468e+02, 9.9686e-01],\n",
      "         [7.1163e+01, 2.1761e+02, 9.9181e-01],\n",
      "         [8.6682e+01, 2.1986e+02, 9.9262e-01],\n",
      "         [7.9173e+01, 2.4566e+02, 9.7463e-01],\n",
      "         [8.6932e+01, 2.4840e+02, 9.7636e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3248, 147.1293],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.6634, 160.8413],\n",
      "         [ 88.0061, 159.0736],\n",
      "         [ 65.3323, 178.2578],\n",
      "         [ 97.5421, 176.9593],\n",
      "         [ 62.7400, 178.9846],\n",
      "         [110.2031, 189.1194],\n",
      "         [ 76.6506, 195.3747],\n",
      "         [ 89.1629, 194.6837],\n",
      "         [ 71.1633, 217.6127],\n",
      "         [ 86.6825, 219.8568],\n",
      "         [ 79.1733, 245.6625],\n",
      "         [ 86.9316, 248.4005]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1926, 0.4087],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1907, 0.4468],\n",
      "         [0.2445, 0.4419],\n",
      "         [0.1815, 0.4952],\n",
      "         [0.2710, 0.4916],\n",
      "         [0.1743, 0.4972],\n",
      "         [0.3061, 0.5253],\n",
      "         [0.2129, 0.5427],\n",
      "         [0.2477, 0.5408],\n",
      "         [0.1977, 0.6045],\n",
      "         [0.2408, 0.6107],\n",
      "         [0.2199, 0.6824],\n",
      "         [0.2415, 0.6900]]])\n",
      "\n",
      "0: 640x640 1 person, 112.0ms\n",
      "Speed: 1.4ms preprocess, 112.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [167, 146, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [166, 145, 132]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [173, 152, 139]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3532638549804688, 'inference': 112.04099655151367, 'postprocess': 0.6327629089355469}]\n",
      "Bounding Box: tensor([[ 87.0000, 196.5000,  64.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2614, 0.1437, 0.0624, 0.6665, 0.3114, 0.9904, 0.9887, 0.9576, 0.9526, 0.8671, 0.8602, 0.9990, 0.9990, 0.9963, 0.9962, 0.9806, 0.9806]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.6139e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4366e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.2417e-02],\n",
      "         [6.9866e+01, 1.4700e+02, 6.6654e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1144e-01],\n",
      "         [6.9314e+01, 1.6008e+02, 9.9044e-01],\n",
      "         [8.8632e+01, 1.5907e+02, 9.8873e-01],\n",
      "         [6.4710e+01, 1.7710e+02, 9.5761e-01],\n",
      "         [9.9176e+01, 1.7699e+02, 9.5264e-01],\n",
      "         [5.5965e+01, 1.8038e+02, 8.6705e-01],\n",
      "         [1.1150e+02, 1.8816e+02, 8.6018e-01],\n",
      "         [7.7165e+01, 1.9529e+02, 9.9900e-01],\n",
      "         [8.9089e+01, 1.9458e+02, 9.9903e-01],\n",
      "         [7.2895e+01, 2.1773e+02, 9.9629e-01],\n",
      "         [8.6464e+01, 2.1976e+02, 9.9624e-01],\n",
      "         [8.1342e+01, 2.4560e+02, 9.8059e-01],\n",
      "         [8.7108e+01, 2.4730e+02, 9.8062e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.8657, 147.0003],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3143, 160.0763],\n",
      "         [ 88.6324, 159.0731],\n",
      "         [ 64.7103, 177.0984],\n",
      "         [ 99.1759, 176.9944],\n",
      "         [ 55.9649, 180.3840],\n",
      "         [111.4957, 188.1636],\n",
      "         [ 77.1650, 195.2925],\n",
      "         [ 89.0890, 194.5816],\n",
      "         [ 72.8947, 217.7325],\n",
      "         [ 86.4636, 219.7557],\n",
      "         [ 81.3423, 245.5997],\n",
      "         [ 87.1076, 247.3014]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1941, 0.4083],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1925, 0.4447],\n",
      "         [0.2462, 0.4419],\n",
      "         [0.1798, 0.4919],\n",
      "         [0.2755, 0.4917],\n",
      "         [0.1555, 0.5011],\n",
      "         [0.3097, 0.5227],\n",
      "         [0.2143, 0.5425],\n",
      "         [0.2475, 0.5405],\n",
      "         [0.2025, 0.6048],\n",
      "         [0.2402, 0.6104],\n",
      "         [0.2260, 0.6822],\n",
      "         [0.2420, 0.6869]]])\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.3ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [167, 146, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [166, 145, 132]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [173, 152, 139]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3151168823242188, 'inference': 115.52309989929199, 'postprocess': 0.4363059997558594}]\n",
      "Bounding Box: tensor([[ 89.0000, 196.5000,  60.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2128, 0.1323, 0.0549, 0.6807, 0.3056, 0.9860, 0.9831, 0.9238, 0.9162, 0.7595, 0.7493, 0.9984, 0.9985, 0.9953, 0.9954, 0.9806, 0.9809]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.1282e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3235e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.4938e-02],\n",
      "         [6.9577e+01, 1.4707e+02, 6.8073e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0560e-01],\n",
      "         [6.8909e+01, 1.5986e+02, 9.8602e-01],\n",
      "         [8.8465e+01, 1.5877e+02, 9.8309e-01],\n",
      "         [6.5138e+01, 1.7702e+02, 9.2379e-01],\n",
      "         [9.8876e+01, 1.7667e+02, 9.1624e-01],\n",
      "         [6.1669e+01, 1.8034e+02, 7.5949e-01],\n",
      "         [1.1126e+02, 1.8821e+02, 7.4934e-01],\n",
      "         [7.7326e+01, 1.9371e+02, 9.9844e-01],\n",
      "         [8.9454e+01, 1.9310e+02, 9.9850e-01],\n",
      "         [7.4307e+01, 2.1722e+02, 9.9534e-01],\n",
      "         [8.6589e+01, 2.1935e+02, 9.9540e-01],\n",
      "         [8.2196e+01, 2.4481e+02, 9.8059e-01],\n",
      "         [8.7142e+01, 2.4714e+02, 9.8088e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.5771, 147.0731],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.9086, 159.8561],\n",
      "         [ 88.4648, 158.7681],\n",
      "         [ 65.1380, 177.0201],\n",
      "         [ 98.8761, 176.6723],\n",
      "         [ 61.6686, 180.3429],\n",
      "         [111.2578, 188.2146],\n",
      "         [ 77.3255, 193.7102],\n",
      "         [ 89.4536, 193.1037],\n",
      "         [ 74.3068, 217.2238],\n",
      "         [ 86.5886, 219.3518],\n",
      "         [ 82.1960, 244.8065],\n",
      "         [ 87.1424, 247.1446]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1933, 0.4085],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1914, 0.4440],\n",
      "         [0.2457, 0.4410],\n",
      "         [0.1809, 0.4917],\n",
      "         [0.2747, 0.4908],\n",
      "         [0.1713, 0.5010],\n",
      "         [0.3090, 0.5228],\n",
      "         [0.2148, 0.5381],\n",
      "         [0.2485, 0.5364],\n",
      "         [0.2064, 0.6034],\n",
      "         [0.2405, 0.6093],\n",
      "         [0.2283, 0.6800],\n",
      "         [0.2421, 0.6865]]])\n",
      "\n",
      "0: 640x640 1 person, 127.1ms\n",
      "Speed: 1.3ms preprocess, 127.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 148, 136],\n",
      "        [168, 147, 134],\n",
      "        [165, 144, 131]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [171, 150, 137],\n",
      "        [166, 145, 132]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 151, 138],\n",
      "        [173, 152, 139],\n",
      "        [175, 154, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3060569763183594, 'inference': 127.07710266113281, 'postprocess': 0.408172607421875}]\n",
      "Bounding Box: tensor([[ 88.5000, 195.5000,  59.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2801, 0.1682, 0.0667, 0.7517, 0.3519, 0.9927, 0.9895, 0.9594, 0.9414, 0.8403, 0.8128, 0.9987, 0.9987, 0.9965, 0.9962, 0.9853, 0.9844]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8012e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6818e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6674e-02],\n",
      "         [6.9535e+01, 1.4642e+02, 7.5174e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5189e-01],\n",
      "         [6.9005e+01, 1.5992e+02, 9.9268e-01],\n",
      "         [8.8788e+01, 1.5859e+02, 9.8950e-01],\n",
      "         [6.5381e+01, 1.7633e+02, 9.5938e-01],\n",
      "         [9.8454e+01, 1.7663e+02, 9.4142e-01],\n",
      "         [5.9548e+01, 1.7961e+02, 8.4030e-01],\n",
      "         [1.1043e+02, 1.8808e+02, 8.1278e-01],\n",
      "         [7.7714e+01, 1.9472e+02, 9.9874e-01],\n",
      "         [9.0272e+01, 1.9394e+02, 9.9867e-01],\n",
      "         [7.3444e+01, 2.1774e+02, 9.9649e-01],\n",
      "         [8.7366e+01, 2.1932e+02, 9.9615e-01],\n",
      "         [8.1746e+01, 2.4556e+02, 9.8530e-01],\n",
      "         [8.6995e+01, 2.4766e+02, 9.8437e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.5350, 146.4153],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0054, 159.9178],\n",
      "         [ 88.7879, 158.5938],\n",
      "         [ 65.3810, 176.3335],\n",
      "         [ 98.4541, 176.6336],\n",
      "         [ 59.5480, 179.6130],\n",
      "         [110.4291, 188.0804],\n",
      "         [ 77.7143, 194.7180],\n",
      "         [ 90.2720, 193.9371],\n",
      "         [ 73.4438, 217.7437],\n",
      "         [ 87.3658, 219.3229],\n",
      "         [ 81.7458, 245.5648],\n",
      "         [ 86.9951, 247.6604]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1932, 0.4067],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1917, 0.4442],\n",
      "         [0.2466, 0.4405],\n",
      "         [0.1816, 0.4898],\n",
      "         [0.2735, 0.4906],\n",
      "         [0.1654, 0.4989],\n",
      "         [0.3067, 0.5224],\n",
      "         [0.2159, 0.5409],\n",
      "         [0.2508, 0.5387],\n",
      "         [0.2040, 0.6048],\n",
      "         [0.2427, 0.6092],\n",
      "         [0.2271, 0.6821],\n",
      "         [0.2417, 0.6879]]])\n",
      "\n",
      "0: 640x640 1 person, 114.7ms\n",
      "Speed: 1.3ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        [157, 176, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        [155, 175, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 147, 134],\n",
      "        [167, 146, 133],\n",
      "        [164, 143, 130]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 151, 138],\n",
      "        [172, 151, 138],\n",
      "        [167, 146, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 152, 139],\n",
      "        [174, 153, 140],\n",
      "        [176, 155, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2600421905517578, 'inference': 114.66503143310547, 'postprocess': 0.4208087921142578}]\n",
      "Bounding Box: tensor([[ 87.5000, 195.5000,  59.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2810, 0.1878, 0.0671, 0.7494, 0.3103, 0.9905, 0.9855, 0.9487, 0.9213, 0.8255, 0.7697, 0.9986, 0.9984, 0.9951, 0.9944, 0.9764, 0.9747]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8103e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8778e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.7131e-02],\n",
      "         [6.9944e+01, 1.4636e+02, 7.4944e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1030e-01],\n",
      "         [6.9277e+01, 1.5936e+02, 9.9046e-01],\n",
      "         [8.9367e+01, 1.5828e+02, 9.8551e-01],\n",
      "         [6.5096e+01, 1.7647e+02, 9.4868e-01],\n",
      "         [9.9114e+01, 1.7650e+02, 9.2133e-01],\n",
      "         [5.8648e+01, 1.7918e+02, 8.2546e-01],\n",
      "         [1.1074e+02, 1.8919e+02, 7.6975e-01],\n",
      "         [7.7418e+01, 1.9409e+02, 9.9856e-01],\n",
      "         [8.9814e+01, 1.9330e+02, 9.9843e-01],\n",
      "         [7.3845e+01, 2.1737e+02, 9.9507e-01],\n",
      "         [8.6377e+01, 2.1917e+02, 9.9442e-01],\n",
      "         [8.2901e+01, 2.4554e+02, 9.7644e-01],\n",
      "         [8.6692e+01, 2.4645e+02, 9.7468e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.9437, 146.3583],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.2775, 159.3613],\n",
      "         [ 89.3670, 158.2808],\n",
      "         [ 65.0955, 176.4713],\n",
      "         [ 99.1142, 176.5020],\n",
      "         [ 58.6483, 179.1825],\n",
      "         [110.7398, 189.1916],\n",
      "         [ 77.4182, 194.0919],\n",
      "         [ 89.8142, 193.3032],\n",
      "         [ 73.8452, 217.3661],\n",
      "         [ 86.3770, 219.1712],\n",
      "         [ 82.9009, 245.5442],\n",
      "         [ 86.6918, 246.4521]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1943, 0.4066],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1924, 0.4427],\n",
      "         [0.2482, 0.4397],\n",
      "         [0.1808, 0.4902],\n",
      "         [0.2753, 0.4903],\n",
      "         [0.1629, 0.4977],\n",
      "         [0.3076, 0.5255],\n",
      "         [0.2151, 0.5391],\n",
      "         [0.2495, 0.5370],\n",
      "         [0.2051, 0.6038],\n",
      "         [0.2399, 0.6088],\n",
      "         [0.2303, 0.6821],\n",
      "         [0.2408, 0.6846]]])\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.2ms preprocess, 114.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 148, 136],\n",
      "        [167, 146, 133],\n",
      "        [165, 144, 131]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 151, 138],\n",
      "        [169, 148, 136],\n",
      "        [168, 147, 134]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 155, 143],\n",
      "        [176, 155, 143],\n",
      "        [175, 154, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2180805206298828, 'inference': 114.50695991516113, 'postprocess': 0.40602684020996094}]\n",
      "Bounding Box: tensor([[ 88.5000, 196.0000,  55.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2471, 0.1279, 0.0680, 0.7509, 0.4913, 0.9915, 0.9951, 0.9381, 0.9679, 0.7708, 0.8494, 0.9988, 0.9991, 0.9960, 0.9969, 0.9836, 0.9866]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4706e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2793e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.7996e-02],\n",
      "         [7.0041e+01, 1.4660e+02, 7.5092e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9133e-01],\n",
      "         [6.8293e+01, 1.6028e+02, 9.9145e-01],\n",
      "         [8.8506e+01, 1.5900e+02, 9.9515e-01],\n",
      "         [6.5365e+01, 1.7723e+02, 9.3814e-01],\n",
      "         [9.8099e+01, 1.7720e+02, 9.6793e-01],\n",
      "         [6.5259e+01, 1.7901e+02, 7.7082e-01],\n",
      "         [1.0985e+02, 1.8883e+02, 8.4939e-01],\n",
      "         [7.6893e+01, 1.9512e+02, 9.9877e-01],\n",
      "         [8.9924e+01, 1.9475e+02, 9.9912e-01],\n",
      "         [7.4403e+01, 2.1863e+02, 9.9597e-01],\n",
      "         [8.7435e+01, 2.2039e+02, 9.9693e-01],\n",
      "         [8.3083e+01, 2.4616e+02, 9.8365e-01],\n",
      "         [8.6190e+01, 2.4712e+02, 9.8660e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.0410, 146.6012],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.2932, 160.2779],\n",
      "         [ 88.5059, 158.9980],\n",
      "         [ 65.3647, 177.2277],\n",
      "         [ 98.0994, 177.1996],\n",
      "         [ 65.2592, 179.0050],\n",
      "         [109.8465, 188.8302],\n",
      "         [ 76.8926, 195.1151],\n",
      "         [ 89.9240, 194.7550],\n",
      "         [ 74.4034, 218.6291],\n",
      "         [ 87.4349, 220.3875],\n",
      "         [ 83.0830, 246.1642],\n",
      "         [ 86.1902, 247.1207]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1946, 0.4072],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1897, 0.4452],\n",
      "         [0.2458, 0.4417],\n",
      "         [0.1816, 0.4923],\n",
      "         [0.2725, 0.4922],\n",
      "         [0.1813, 0.4972],\n",
      "         [0.3051, 0.5245],\n",
      "         [0.2136, 0.5420],\n",
      "         [0.2498, 0.5410],\n",
      "         [0.2067, 0.6073],\n",
      "         [0.2429, 0.6122],\n",
      "         [0.2308, 0.6838],\n",
      "         [0.2394, 0.6864]]])\n",
      "\n",
      "0: 640x640 1 person, 112.2ms\n",
      "Speed: 1.2ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 148, 136],\n",
      "        [167, 146, 133],\n",
      "        [165, 144, 131]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 151, 138],\n",
      "        [169, 148, 136],\n",
      "        [168, 147, 134]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 155, 143],\n",
      "        [176, 155, 143],\n",
      "        [175, 154, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2459754943847656, 'inference': 112.22076416015625, 'postprocess': 0.4050731658935547}]\n",
      "Bounding Box: tensor([[ 86.5000, 196.0000,  53.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2813, 0.1595, 0.0846, 0.6914, 0.3851, 0.9853, 0.9893, 0.9084, 0.9454, 0.7351, 0.8122, 0.9980, 0.9985, 0.9928, 0.9945, 0.9681, 0.9739]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8129e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5950e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.4578e-02],\n",
      "         [7.0305e+01, 1.4626e+02, 6.9137e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8509e-01],\n",
      "         [6.9353e+01, 1.5923e+02, 9.8533e-01],\n",
      "         [8.9488e+01, 1.5865e+02, 9.8933e-01],\n",
      "         [6.5489e+01, 1.7620e+02, 9.0840e-01],\n",
      "         [9.9696e+01, 1.7675e+02, 9.4537e-01],\n",
      "         [6.0788e+01, 1.8070e+02, 7.3511e-01],\n",
      "         [1.0807e+02, 1.8927e+02, 8.1225e-01],\n",
      "         [7.7955e+01, 1.9404e+02, 9.9795e-01],\n",
      "         [9.0404e+01, 1.9340e+02, 9.9849e-01],\n",
      "         [7.4312e+01, 2.1738e+02, 9.9279e-01],\n",
      "         [8.5670e+01, 2.1918e+02, 9.9447e-01],\n",
      "         [8.3497e+01, 2.4512e+02, 9.6808e-01],\n",
      "         [8.7265e+01, 2.4608e+02, 9.7387e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.3052, 146.2592],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3526, 159.2275],\n",
      "         [ 89.4881, 158.6537],\n",
      "         [ 65.4894, 176.2030],\n",
      "         [ 99.6958, 176.7522],\n",
      "         [ 60.7882, 180.6977],\n",
      "         [108.0715, 189.2713],\n",
      "         [ 77.9552, 194.0424],\n",
      "         [ 90.4043, 193.3994],\n",
      "         [ 74.3121, 217.3752],\n",
      "         [ 85.6698, 219.1821],\n",
      "         [ 83.4974, 245.1245],\n",
      "         [ 87.2648, 246.0827]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1953, 0.4063],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1926, 0.4423],\n",
      "         [0.2486, 0.4407],\n",
      "         [0.1819, 0.4895],\n",
      "         [0.2769, 0.4910],\n",
      "         [0.1689, 0.5019],\n",
      "         [0.3002, 0.5258],\n",
      "         [0.2165, 0.5390],\n",
      "         [0.2511, 0.5372],\n",
      "         [0.2064, 0.6038],\n",
      "         [0.2380, 0.6088],\n",
      "         [0.2319, 0.6809],\n",
      "         [0.2424, 0.6836]]])\n",
      "\n",
      "0: 640x640 1 person, 112.0ms\n",
      "Speed: 1.3ms preprocess, 112.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134],\n",
      "        [167, 146, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 153, 140],\n",
      "        [173, 152, 139],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 154, 141],\n",
      "        [178, 157, 144],\n",
      "        [178, 157, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2660026550292969, 'inference': 111.97018623352051, 'postprocess': 0.42891502380371094}]\n",
      "Bounding Box: tensor([[ 85., 196.,  52., 122.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2971, 0.1778, 0.0833, 0.7130, 0.3636, 0.9876, 0.9885, 0.9261, 0.9398, 0.7743, 0.8059, 0.9982, 0.9985, 0.9937, 0.9945, 0.9709, 0.9741]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.9708e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7777e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.3261e-02],\n",
      "         [7.0192e+01, 1.4589e+02, 7.1302e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6359e-01],\n",
      "         [6.9557e+01, 1.5898e+02, 9.8758e-01],\n",
      "         [8.9002e+01, 1.5834e+02, 9.8855e-01],\n",
      "         [6.5479e+01, 1.7599e+02, 9.2608e-01],\n",
      "         [9.8666e+01, 1.7630e+02, 9.3983e-01],\n",
      "         [5.8944e+01, 1.8050e+02, 7.7428e-01],\n",
      "         [1.0609e+02, 1.8854e+02, 8.0586e-01],\n",
      "         [7.7498e+01, 1.9404e+02, 9.9819e-01],\n",
      "         [8.9516e+01, 1.9333e+02, 9.9849e-01],\n",
      "         [7.3771e+01, 2.1728e+02, 9.9370e-01],\n",
      "         [8.5551e+01, 2.1907e+02, 9.9453e-01],\n",
      "         [8.2930e+01, 2.4578e+02, 9.7093e-01],\n",
      "         [8.7199e+01, 2.4668e+02, 9.7408e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.1921, 145.8865],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.5571, 158.9798],\n",
      "         [ 89.0020, 158.3434],\n",
      "         [ 65.4787, 175.9865],\n",
      "         [ 98.6665, 176.3028],\n",
      "         [ 58.9440, 180.4982],\n",
      "         [106.0855, 188.5418],\n",
      "         [ 77.4977, 194.0436],\n",
      "         [ 89.5160, 193.3313],\n",
      "         [ 73.7713, 217.2808],\n",
      "         [ 85.5510, 219.0698],\n",
      "         [ 82.9297, 245.7767],\n",
      "         [ 87.1988, 246.6843]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1950, 0.4052],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1932, 0.4416],\n",
      "         [0.2472, 0.4398],\n",
      "         [0.1819, 0.4889],\n",
      "         [0.2741, 0.4897],\n",
      "         [0.1637, 0.5014],\n",
      "         [0.2947, 0.5237],\n",
      "         [0.2153, 0.5390],\n",
      "         [0.2487, 0.5370],\n",
      "         [0.2049, 0.6036],\n",
      "         [0.2376, 0.6085],\n",
      "         [0.2304, 0.6827],\n",
      "         [0.2422, 0.6852]]])\n",
      "\n",
      "0: 640x640 1 person, 115.1ms\n",
      "Speed: 1.5ms preprocess, 115.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 150, 137],\n",
      "        [168, 147, 134],\n",
      "        [167, 146, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 153, 140],\n",
      "        [173, 152, 139],\n",
      "        [172, 151, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 154, 141],\n",
      "        [178, 157, 144],\n",
      "        [178, 157, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.461029052734375, 'inference': 115.06295204162598, 'postprocess': 0.43010711669921875}]\n",
      "Bounding Box: tensor([[ 84.5000, 196.0000,  47.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2345, 0.1415, 0.0717, 0.7287, 0.4394, 0.9833, 0.9920, 0.8742, 0.9437, 0.6364, 0.7717, 0.9983, 0.9989, 0.9950, 0.9966, 0.9815, 0.9861]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3447e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4154e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.1674e-02],\n",
      "         [7.0344e+01, 1.4582e+02, 7.2866e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3935e-01],\n",
      "         [6.8596e+01, 1.5890e+02, 9.8334e-01],\n",
      "         [8.8696e+01, 1.5852e+02, 9.9201e-01],\n",
      "         [6.5683e+01, 1.7545e+02, 8.7420e-01],\n",
      "         [9.7419e+01, 1.7637e+02, 9.4371e-01],\n",
      "         [6.2261e+01, 1.7925e+02, 6.3639e-01],\n",
      "         [1.0375e+02, 1.8778e+02, 7.7171e-01],\n",
      "         [7.6560e+01, 1.9286e+02, 9.9833e-01],\n",
      "         [8.9566e+01, 1.9276e+02, 9.9893e-01],\n",
      "         [7.3536e+01, 2.1702e+02, 9.9499e-01],\n",
      "         [8.6234e+01, 2.1973e+02, 9.9659e-01],\n",
      "         [8.2798e+01, 2.4570e+02, 9.8152e-01],\n",
      "         [8.6633e+01, 2.4747e+02, 9.8614e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.3439, 145.8245],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.5957, 158.9013],\n",
      "         [ 88.6956, 158.5219],\n",
      "         [ 65.6828, 175.4475],\n",
      "         [ 97.4190, 176.3714],\n",
      "         [ 62.2614, 179.2517],\n",
      "         [103.7526, 187.7819],\n",
      "         [ 76.5597, 192.8591],\n",
      "         [ 89.5657, 192.7641],\n",
      "         [ 73.5364, 217.0233],\n",
      "         [ 86.2335, 219.7281],\n",
      "         [ 82.7979, 245.6955],\n",
      "         [ 86.6330, 247.4740]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1954, 0.4051],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1905, 0.4414],\n",
      "         [0.2464, 0.4403],\n",
      "         [0.1825, 0.4874],\n",
      "         [0.2706, 0.4899],\n",
      "         [0.1729, 0.4979],\n",
      "         [0.2882, 0.5216],\n",
      "         [0.2127, 0.5357],\n",
      "         [0.2488, 0.5355],\n",
      "         [0.2043, 0.6028],\n",
      "         [0.2395, 0.6104],\n",
      "         [0.2300, 0.6825],\n",
      "         [0.2406, 0.6874]]])\n",
      "\n",
      "0: 640x640 1 person, 113.4ms\n",
      "Speed: 1.2ms preprocess, 113.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [173, 153, 134],\n",
      "        [172, 152, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140],\n",
      "        [180, 160, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [175, 155, 137],\n",
      "        [174, 154, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2481212615966797, 'inference': 113.43216896057129, 'postprocess': 0.4067420959472656}]\n",
      "Bounding Box: tensor([[ 83.5000, 196.5000,  43.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1966, 0.1393, 0.0622, 0.7481, 0.4140, 0.9770, 0.9879, 0.8017, 0.8979, 0.4746, 0.6179, 0.9975, 0.9984, 0.9941, 0.9959, 0.9825, 0.9868]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.9663e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3925e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.2177e-02],\n",
      "         [7.0110e+01, 1.4611e+02, 7.4814e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1403e-01],\n",
      "         [6.8707e+01, 1.5878e+02, 9.7695e-01],\n",
      "         [8.8547e+01, 1.5831e+02, 9.8791e-01],\n",
      "         [6.6490e+01, 1.7599e+02, 8.0174e-01],\n",
      "         [9.6698e+01, 1.7687e+02, 8.9792e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7462e-01],\n",
      "         [9.9965e+01, 1.8844e+02, 6.1792e-01],\n",
      "         [7.7103e+01, 1.9258e+02, 9.9752e-01],\n",
      "         [9.0064e+01, 1.9237e+02, 9.9840e-01],\n",
      "         [7.3494e+01, 2.1744e+02, 9.9409e-01],\n",
      "         [8.5529e+01, 2.1982e+02, 9.9594e-01],\n",
      "         [8.2113e+01, 2.4681e+02, 9.8251e-01],\n",
      "         [8.6642e+01, 2.4806e+02, 9.8681e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.1104, 146.1115],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.7068, 158.7775],\n",
      "         [ 88.5466, 158.3143],\n",
      "         [ 66.4900, 175.9939],\n",
      "         [ 96.6979, 176.8715],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 99.9646, 188.4421],\n",
      "         [ 77.1031, 192.5764],\n",
      "         [ 90.0637, 192.3745],\n",
      "         [ 73.4945, 217.4370],\n",
      "         [ 85.5295, 219.8182],\n",
      "         [ 82.1130, 246.8147],\n",
      "         [ 86.6422, 248.0620]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1948, 0.4059],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1909, 0.4410],\n",
      "         [0.2460, 0.4398],\n",
      "         [0.1847, 0.4889],\n",
      "         [0.2686, 0.4913],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2777, 0.5235],\n",
      "         [0.2142, 0.5349],\n",
      "         [0.2502, 0.5344],\n",
      "         [0.2042, 0.6040],\n",
      "         [0.2376, 0.6106],\n",
      "         [0.2281, 0.6856],\n",
      "         [0.2407, 0.6891]]])\n",
      "\n",
      "0: 640x640 1 person, 115.4ms\n",
      "Speed: 1.3ms preprocess, 115.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [173, 153, 134],\n",
      "        [172, 152, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140],\n",
      "        [180, 160, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [175, 155, 137],\n",
      "        [174, 154, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3136863708496094, 'inference': 115.42129516601562, 'postprocess': 0.4200935363769531}]\n",
      "Bounding Box: tensor([[ 79.5000, 197.0000,  41.0000, 124.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2492, 0.2745, 0.0490, 0.8586, 0.2516, 0.9892, 0.9668, 0.9340, 0.7132, 0.7352, 0.4094, 0.9980, 0.9968, 0.9959, 0.9927, 0.9864, 0.9800]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4922e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7453e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9043e-02],\n",
      "         [7.0513e+01, 1.4577e+02, 8.5863e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5160e-01],\n",
      "         [6.9899e+01, 1.5816e+02, 9.8922e-01],\n",
      "         [8.8780e+01, 1.5772e+02, 9.6676e-01],\n",
      "         [6.6935e+01, 1.7551e+02, 9.3401e-01],\n",
      "         [9.5543e+01, 1.7814e+02, 7.1321e-01],\n",
      "         [5.9272e+01, 1.7496e+02, 7.3517e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0940e-01],\n",
      "         [7.8026e+01, 1.9247e+02, 9.9800e-01],\n",
      "         [9.0426e+01, 1.9213e+02, 9.9677e-01],\n",
      "         [7.2227e+01, 2.1667e+02, 9.9586e-01],\n",
      "         [8.4958e+01, 2.1955e+02, 9.9269e-01],\n",
      "         [8.1481e+01, 2.4635e+02, 9.8639e-01],\n",
      "         [8.6597e+01, 2.4809e+02, 9.7997e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.5126, 145.7732],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.8993, 158.1648],\n",
      "         [ 88.7803, 157.7164],\n",
      "         [ 66.9351, 175.5067],\n",
      "         [ 95.5430, 178.1407],\n",
      "         [ 59.2717, 174.9633],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.0261, 192.4708],\n",
      "         [ 90.4255, 192.1281],\n",
      "         [ 72.2271, 216.6658],\n",
      "         [ 84.9583, 219.5487],\n",
      "         [ 81.4812, 246.3459],\n",
      "         [ 86.5969, 248.0941]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1959, 0.4049],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1942, 0.4393],\n",
      "         [0.2466, 0.4381],\n",
      "         [0.1859, 0.4875],\n",
      "         [0.2654, 0.4948],\n",
      "         [0.1646, 0.4860],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2167, 0.5346],\n",
      "         [0.2512, 0.5337],\n",
      "         [0.2006, 0.6018],\n",
      "         [0.2360, 0.6099],\n",
      "         [0.2263, 0.6843],\n",
      "         [0.2405, 0.6892]]])\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.3ms preprocess, 114.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        [160, 175, 202],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        [159, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [173, 153, 134],\n",
      "        [172, 152, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140],\n",
      "        [180, 160, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [175, 155, 137],\n",
      "        [174, 154, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2810230255126953, 'inference': 114.1057014465332, 'postprocess': 0.4010200500488281}]\n",
      "Bounding Box: tensor([[ 78.0000, 196.5000,  38.0000, 125.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1836, 0.2714, 0.0298, 0.8753, 0.1625, 0.9864, 0.9164, 0.9243, 0.4424, 0.6813, 0.2026, 0.9969, 0.9930, 0.9949, 0.9874, 0.9856, 0.9730]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.8360e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7136e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9773e-02],\n",
      "         [7.0341e+01, 1.4536e+02, 8.7531e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6249e-01],\n",
      "         [7.0295e+01, 1.5814e+02, 9.8640e-01],\n",
      "         [8.8198e+01, 1.5743e+02, 9.1636e-01],\n",
      "         [6.7784e+01, 1.7567e+02, 9.2432e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4239e-01],\n",
      "         [5.9960e+01, 1.7604e+02, 6.8127e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0259e-01],\n",
      "         [7.7905e+01, 1.9212e+02, 9.9692e-01],\n",
      "         [9.0090e+01, 1.9152e+02, 9.9297e-01],\n",
      "         [7.2291e+01, 2.1712e+02, 9.9493e-01],\n",
      "         [8.4993e+01, 2.1928e+02, 9.8743e-01],\n",
      "         [8.0574e+01, 2.4711e+02, 9.8556e-01],\n",
      "         [8.6966e+01, 2.4815e+02, 9.7302e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.3413, 145.3647],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.2948, 158.1416],\n",
      "         [ 88.1978, 157.4268],\n",
      "         [ 67.7840, 175.6697],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 59.9596, 176.0361],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.9050, 192.1239],\n",
      "         [ 90.0905, 191.5185],\n",
      "         [ 72.2914, 217.1183],\n",
      "         [ 84.9926, 219.2751],\n",
      "         [ 80.5742, 247.1065],\n",
      "         [ 86.9659, 248.1515]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1954, 0.4038],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1953, 0.4393],\n",
      "         [0.2450, 0.4373],\n",
      "         [0.1883, 0.4880],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1666, 0.4890],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2164, 0.5337],\n",
      "         [0.2503, 0.5320],\n",
      "         [0.2008, 0.6031],\n",
      "         [0.2361, 0.6091],\n",
      "         [0.2238, 0.6864],\n",
      "         [0.2416, 0.6893]]])\n",
      "\n",
      "0: 640x640 1 person, 114.3ms\n",
      "Speed: 1.3ms preprocess, 114.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [173, 153, 134],\n",
      "        [172, 152, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140],\n",
      "        [180, 160, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [175, 155, 137],\n",
      "        [174, 154, 136]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2881755828857422, 'inference': 114.27092552185059, 'postprocess': 0.3998279571533203}]\n",
      "Bounding Box: tensor([[ 77., 197.,  38., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2600, 0.3712, 0.0446, 0.8508, 0.1283, 0.9797, 0.8675, 0.9175, 0.3839, 0.7366, 0.2303, 0.9936, 0.9844, 0.9883, 0.9691, 0.9602, 0.9242]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5998e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7125e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4614e-02],\n",
      "         [7.0718e+01, 1.4479e+02, 8.5079e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2828e-01],\n",
      "         [7.1109e+01, 1.5770e+02, 9.7971e-01],\n",
      "         [8.6973e+01, 1.5731e+02, 8.6747e-01],\n",
      "         [6.7895e+01, 1.7579e+02, 9.1750e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8390e-01],\n",
      "         [5.9304e+01, 1.7503e+02, 7.3658e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3030e-01],\n",
      "         [7.8345e+01, 1.9274e+02, 9.9358e-01],\n",
      "         [8.9821e+01, 1.9201e+02, 9.8435e-01],\n",
      "         [7.0918e+01, 2.1760e+02, 9.8830e-01],\n",
      "         [8.4975e+01, 2.1942e+02, 9.6907e-01],\n",
      "         [7.9083e+01, 2.4994e+02, 9.6016e-01],\n",
      "         [8.7568e+01, 2.4883e+02, 9.2416e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.7180, 144.7905],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.1092, 157.7049],\n",
      "         [ 86.9725, 157.3141],\n",
      "         [ 67.8949, 175.7874],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 59.3038, 175.0303],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.3453, 192.7350],\n",
      "         [ 89.8206, 192.0090],\n",
      "         [ 70.9179, 217.6004],\n",
      "         [ 84.9749, 219.4232],\n",
      "         [ 79.0831, 249.9437],\n",
      "         [ 87.5685, 248.8340]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1964, 0.4022],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1975, 0.4381],\n",
      "         [0.2416, 0.4370],\n",
      "         [0.1886, 0.4883],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1647, 0.4862],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2176, 0.5354],\n",
      "         [0.2495, 0.5334],\n",
      "         [0.1970, 0.6044],\n",
      "         [0.2360, 0.6095],\n",
      "         [0.2197, 0.6943],\n",
      "         [0.2432, 0.6912]]])\n",
      "\n",
      "0: 640x640 1 person, 117.7ms\n",
      "Speed: 1.3ms preprocess, 117.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [176, 157, 138],\n",
      "        [178, 158, 139]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [175, 155, 137],\n",
      "        [176, 157, 138]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2722015380859375, 'inference': 117.7217960357666, 'postprocess': 0.41604042053222656}]\n",
      "Bounding Box: tensor([[ 76.5000, 198.0000,  39.0000, 126.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3054, 0.4337, 0.0380, 0.8898, 0.1040, 0.9920, 0.8980, 0.9740, 0.4518, 0.8922, 0.3032, 0.9979, 0.9927, 0.9964, 0.9862, 0.9856, 0.9640]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.0543e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3369e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8027e-02],\n",
      "         [7.0511e+01, 1.4479e+02, 8.8982e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0403e-01],\n",
      "         [7.1081e+01, 1.5717e+02, 9.9195e-01],\n",
      "         [8.6149e+01, 1.5667e+02, 8.9803e-01],\n",
      "         [6.9281e+01, 1.7471e+02, 9.7398e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5175e-01],\n",
      "         [6.0948e+01, 1.7455e+02, 8.9223e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0321e-01],\n",
      "         [7.9227e+01, 1.9096e+02, 9.9789e-01],\n",
      "         [9.0160e+01, 1.9015e+02, 9.9269e-01],\n",
      "         [7.1963e+01, 2.1777e+02, 9.9644e-01],\n",
      "         [8.4865e+01, 2.1874e+02, 9.8616e-01],\n",
      "         [7.9507e+01, 2.4960e+02, 9.8565e-01],\n",
      "         [8.7186e+01, 2.4802e+02, 9.6398e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.5114, 144.7939],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.0810, 157.1709],\n",
      "         [ 86.1494, 156.6739],\n",
      "         [ 69.2813, 174.7127],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 60.9481, 174.5497],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.2265, 190.9564],\n",
      "         [ 90.1603, 190.1499],\n",
      "         [ 71.9634, 217.7695],\n",
      "         [ 84.8649, 218.7436],\n",
      "         [ 79.5074, 249.5986],\n",
      "         [ 87.1857, 248.0229]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1959, 0.4022],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1974, 0.4366],\n",
      "         [0.2393, 0.4352],\n",
      "         [0.1924, 0.4853],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1693, 0.4849],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2201, 0.5304],\n",
      "         [0.2504, 0.5282],\n",
      "         [0.1999, 0.6049],\n",
      "         [0.2357, 0.6076],\n",
      "         [0.2209, 0.6933],\n",
      "         [0.2422, 0.6890]]])\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 1.2ms preprocess, 113.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138],\n",
      "        [175, 155, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [176, 157, 138],\n",
      "        [173, 153, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2202262878417969, 'inference': 113.27004432678223, 'postprocess': 0.720977783203125}]\n",
      "Bounding Box: tensor([[ 75.5000, 198.5000,  41.0000, 127.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4248, 0.5604, 0.0592, 0.9004, 0.1018, 0.9923, 0.9025, 0.9753, 0.4691, 0.9097, 0.3452, 0.9978, 0.9924, 0.9964, 0.9863, 0.9861, 0.9655]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.2475e-01],\n",
      "         [6.7383e+01, 1.4274e+02, 5.6044e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.9183e-02],\n",
      "         [7.0887e+01, 1.4369e+02, 9.0043e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0176e-01],\n",
      "         [7.1519e+01, 1.5735e+02, 9.9235e-01],\n",
      "         [8.5784e+01, 1.5671e+02, 9.0246e-01],\n",
      "         [7.0534e+01, 1.7534e+02, 9.7530e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6908e-01],\n",
      "         [6.1277e+01, 1.7329e+02, 9.0974e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4522e-01],\n",
      "         [7.7812e+01, 1.9337e+02, 9.9781e-01],\n",
      "         [8.8232e+01, 1.9243e+02, 9.9245e-01],\n",
      "         [7.0345e+01, 2.1867e+02, 9.9642e-01],\n",
      "         [8.3802e+01, 2.1966e+02, 9.8631e-01],\n",
      "         [7.8859e+01, 2.4986e+02, 9.8608e-01],\n",
      "         [8.6400e+01, 2.4886e+02, 9.6553e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 67.3833, 142.7384],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8872, 143.6916],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.5185, 157.3491],\n",
      "         [ 85.7842, 156.7091],\n",
      "         [ 70.5337, 175.3386],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 61.2769, 173.2947],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.8119, 193.3699],\n",
      "         [ 88.2315, 192.4312],\n",
      "         [ 70.3447, 218.6729],\n",
      "         [ 83.8018, 219.6625],\n",
      "         [ 78.8591, 249.8622],\n",
      "         [ 86.3999, 248.8616]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1872, 0.3965],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1969, 0.3991],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1987, 0.4371],\n",
      "         [0.2383, 0.4353],\n",
      "         [0.1959, 0.4871],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1702, 0.4814],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2161, 0.5371],\n",
      "         [0.2451, 0.5345],\n",
      "         [0.1954, 0.6074],\n",
      "         [0.2328, 0.6102],\n",
      "         [0.2191, 0.6941],\n",
      "         [0.2400, 0.6913]]])\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 1.2ms preprocess, 117.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138],\n",
      "        [175, 155, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [178, 158, 139],\n",
      "        [179, 159, 140]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [176, 157, 138],\n",
      "        [173, 153, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2369155883789062, 'inference': 117.8750991821289, 'postprocess': 0.453948974609375}]\n",
      "Bounding Box: tensor([[ 77., 198.,  36., 130.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6099, 0.7081, 0.1069, 0.9182, 0.1142, 0.9946, 0.9248, 0.9836, 0.5557, 0.9407, 0.4459, 0.9975, 0.9909, 0.9967, 0.9867, 0.9879, 0.9693]])\n",
      "data: tensor([[[6.5390e+01, 1.4484e+02, 6.0992e-01],\n",
      "         [6.6361e+01, 1.4223e+02, 7.0811e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0687e-01],\n",
      "         [7.0736e+01, 1.4287e+02, 9.1816e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1422e-01],\n",
      "         [7.1717e+01, 1.5715e+02, 9.9459e-01],\n",
      "         [8.5763e+01, 1.5661e+02, 9.2478e-01],\n",
      "         [7.3071e+01, 1.7529e+02, 9.8359e-01],\n",
      "         [8.9699e+01, 1.7700e+02, 5.5574e-01],\n",
      "         [6.3604e+01, 1.7424e+02, 9.4070e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4595e-01],\n",
      "         [7.7380e+01, 1.9338e+02, 9.9747e-01],\n",
      "         [8.7682e+01, 1.9248e+02, 9.9095e-01],\n",
      "         [6.9361e+01, 2.1904e+02, 9.9666e-01],\n",
      "         [8.3910e+01, 2.2007e+02, 9.8671e-01],\n",
      "         [7.7823e+01, 2.5058e+02, 9.8790e-01],\n",
      "         [8.6001e+01, 2.4917e+02, 9.6934e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 65.3903, 144.8392],\n",
      "         [ 66.3606, 142.2273],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.7365, 142.8691],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.7166, 157.1461],\n",
      "         [ 85.7628, 156.6067],\n",
      "         [ 73.0714, 175.2892],\n",
      "         [ 89.6992, 177.0017],\n",
      "         [ 63.6042, 174.2352],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.3797, 193.3827],\n",
      "         [ 87.6818, 192.4784],\n",
      "         [ 69.3609, 219.0422],\n",
      "         [ 83.9104, 220.0727],\n",
      "         [ 77.8234, 250.5844],\n",
      "         [ 86.0006, 249.1712]]])\n",
      "xyn: tensor([[[0.1816, 0.4023],\n",
      "         [0.1843, 0.3951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1965, 0.3969],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1992, 0.4365],\n",
      "         [0.2382, 0.4350],\n",
      "         [0.2030, 0.4869],\n",
      "         [0.2492, 0.4917],\n",
      "         [0.1767, 0.4840],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2149, 0.5372],\n",
      "         [0.2436, 0.5347],\n",
      "         [0.1927, 0.6085],\n",
      "         [0.2331, 0.6113],\n",
      "         [0.2162, 0.6961],\n",
      "         [0.2389, 0.6921]]])\n",
      "\n",
      "0: 640x640 1 person, 111.0ms\n",
      "Speed: 1.2ms preprocess, 111.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138],\n",
      "        [176, 157, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 155, 137],\n",
      "        [176, 157, 138],\n",
      "        [175, 155, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 154, 136],\n",
      "        [176, 157, 138],\n",
      "        [173, 153, 134]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.249074935913086, 'inference': 110.9609603881836, 'postprocess': 0.4050731658935547}]\n",
      "Bounding Box: tensor([[ 76., 198.,  38., 130.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6199, 0.7238, 0.0980, 0.9146, 0.0832, 0.9944, 0.9118, 0.9859, 0.5394, 0.9531, 0.4591, 0.9975, 0.9902, 0.9967, 0.9857, 0.9876, 0.9669]])\n",
      "data: tensor([[[6.4796e+01, 1.4492e+02, 6.1987e-01],\n",
      "         [6.5830e+01, 1.4231e+02, 7.2383e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.8046e-02],\n",
      "         [7.0813e+01, 1.4254e+02, 9.1464e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.3186e-02],\n",
      "         [7.2510e+01, 1.5655e+02, 9.9444e-01],\n",
      "         [8.4698e+01, 1.5658e+02, 9.1182e-01],\n",
      "         [7.4126e+01, 1.7541e+02, 9.8590e-01],\n",
      "         [8.6928e+01, 1.7671e+02, 5.3939e-01],\n",
      "         [6.4008e+01, 1.7733e+02, 9.5308e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5912e-01],\n",
      "         [7.7580e+01, 1.9322e+02, 9.9749e-01],\n",
      "         [8.6902e+01, 1.9239e+02, 9.9021e-01],\n",
      "         [6.8346e+01, 2.1902e+02, 9.9675e-01],\n",
      "         [8.2210e+01, 2.1965e+02, 9.8571e-01],\n",
      "         [7.6265e+01, 2.5129e+02, 9.8764e-01],\n",
      "         [8.5553e+01, 2.4838e+02, 9.6693e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 64.7959, 144.9249],\n",
      "         [ 65.8296, 142.3108],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8135, 142.5369],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.5096, 156.5535],\n",
      "         [ 84.6981, 156.5791],\n",
      "         [ 74.1261, 175.4079],\n",
      "         [ 86.9276, 176.7105],\n",
      "         [ 64.0075, 177.3270],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.5798, 193.2208],\n",
      "         [ 86.9020, 192.3934],\n",
      "         [ 68.3461, 219.0201],\n",
      "         [ 82.2100, 219.6538],\n",
      "         [ 76.2646, 251.2928],\n",
      "         [ 85.5528, 248.3765]]])\n",
      "xyn: tensor([[[0.1800, 0.4026],\n",
      "         [0.1829, 0.3953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1967, 0.3959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2014, 0.4349],\n",
      "         [0.2353, 0.4349],\n",
      "         [0.2059, 0.4872],\n",
      "         [0.2415, 0.4909],\n",
      "         [0.1778, 0.4926],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2155, 0.5367],\n",
      "         [0.2414, 0.5344],\n",
      "         [0.1899, 0.6084],\n",
      "         [0.2284, 0.6101],\n",
      "         [0.2118, 0.6980],\n",
      "         [0.2376, 0.6899]]])\n",
      "\n",
      "0: 640x640 1 person, 112.8ms\n",
      "Speed: 1.2ms preprocess, 112.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 160, 140],\n",
      "        [176, 160, 140],\n",
      "        [175, 159, 139]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 136],\n",
      "        [171, 154, 134],\n",
      "        [169, 153, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [167, 151, 131],\n",
      "        [166, 150, 130]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2309551239013672, 'inference': 112.81108856201172, 'postprocess': 0.40411949157714844}]\n",
      "Bounding Box: tensor([[ 73.5000, 197.5000,  43.0000, 131.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7628, 0.8391, 0.1878, 0.9186, 0.0992, 0.9942, 0.9165, 0.9838, 0.5409, 0.9566, 0.4976, 0.9976, 0.9912, 0.9966, 0.9860, 0.9859, 0.9641]])\n",
      "data: tensor([[[6.3948e+01, 1.4479e+02, 7.6275e-01],\n",
      "         [6.5188e+01, 1.4223e+02, 8.3915e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8785e-01],\n",
      "         [7.0811e+01, 1.4245e+02, 9.1864e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.9196e-02],\n",
      "         [7.3122e+01, 1.5659e+02, 9.9418e-01],\n",
      "         [8.2248e+01, 1.5720e+02, 9.1646e-01],\n",
      "         [7.4723e+01, 1.7484e+02, 9.8379e-01],\n",
      "         [8.4614e+01, 1.7704e+02, 5.4089e-01],\n",
      "         [6.0969e+01, 1.7527e+02, 9.5663e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9759e-01],\n",
      "         [7.9573e+01, 1.9219e+02, 9.9759e-01],\n",
      "         [8.7021e+01, 1.9145e+02, 9.9124e-01],\n",
      "         [6.7768e+01, 2.1726e+02, 9.9662e-01],\n",
      "         [8.1328e+01, 2.1776e+02, 9.8597e-01],\n",
      "         [7.4526e+01, 2.5150e+02, 9.8592e-01],\n",
      "         [8.5207e+01, 2.4706e+02, 9.6410e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 63.9480, 144.7883],\n",
      "         [ 65.1876, 142.2317],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8106, 142.4529],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.1216, 156.5867],\n",
      "         [ 82.2478, 157.2016],\n",
      "         [ 74.7225, 174.8355],\n",
      "         [ 84.6136, 177.0355],\n",
      "         [ 60.9687, 175.2656],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.5729, 192.1875],\n",
      "         [ 87.0209, 191.4484],\n",
      "         [ 67.7676, 217.2642],\n",
      "         [ 81.3277, 217.7621],\n",
      "         [ 74.5258, 251.4951],\n",
      "         [ 85.2071, 247.0604]]])\n",
      "xyn: tensor([[[0.1776, 0.4022],\n",
      "         [0.1811, 0.3951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1967, 0.3957],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2031, 0.4350],\n",
      "         [0.2285, 0.4367],\n",
      "         [0.2076, 0.4857],\n",
      "         [0.2350, 0.4918],\n",
      "         [0.1694, 0.4868],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2210, 0.5339],\n",
      "         [0.2417, 0.5318],\n",
      "         [0.1882, 0.6035],\n",
      "         [0.2259, 0.6049],\n",
      "         [0.2070, 0.6986],\n",
      "         [0.2367, 0.6863]]])\n",
      "\n",
      "0: 640x640 1 person, 119.6ms\n",
      "Speed: 1.2ms preprocess, 119.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 159, 139],\n",
      "        [174, 158, 138],\n",
      "        [173, 157, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [168, 152, 132],\n",
      "        [168, 152, 132]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [167, 151, 131],\n",
      "        [169, 153, 133]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2309551239013672, 'inference': 119.58670616149902, 'postprocess': 0.4038810729980469}]\n",
      "Bounding Box: tensor([[ 72.0000, 198.5000,  46.0000, 131.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7780, 0.8497, 0.2018, 0.9195, 0.1037, 0.9945, 0.9174, 0.9842, 0.5370, 0.9576, 0.5015, 0.9975, 0.9908, 0.9965, 0.9852, 0.9849, 0.9612]])\n",
      "data: tensor([[[6.3731e+01, 1.4460e+02, 7.7804e-01],\n",
      "         [6.5065e+01, 1.4213e+02, 8.4972e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0181e-01],\n",
      "         [7.0687e+01, 1.4238e+02, 9.1945e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0367e-01],\n",
      "         [7.3117e+01, 1.5705e+02, 9.9447e-01],\n",
      "         [8.1505e+01, 1.5699e+02, 9.1740e-01],\n",
      "         [7.5065e+01, 1.7598e+02, 9.8417e-01],\n",
      "         [8.3960e+01, 1.7600e+02, 5.3703e-01],\n",
      "         [6.0223e+01, 1.7671e+02, 9.5758e-01],\n",
      "         [6.3646e+01, 1.7772e+02, 5.0150e-01],\n",
      "         [7.9757e+01, 1.9233e+02, 9.9751e-01],\n",
      "         [8.6893e+01, 1.9122e+02, 9.9080e-01],\n",
      "         [6.8223e+01, 2.1836e+02, 9.9649e-01],\n",
      "         [8.2130e+01, 2.1796e+02, 9.8516e-01],\n",
      "         [7.2790e+01, 2.5269e+02, 9.8485e-01],\n",
      "         [8.5417e+01, 2.4679e+02, 9.6120e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 63.7310, 144.6043],\n",
      "         [ 65.0646, 142.1316],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.6868, 142.3766],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.1168, 157.0537],\n",
      "         [ 81.5050, 156.9875],\n",
      "         [ 75.0654, 175.9802],\n",
      "         [ 83.9604, 175.9991],\n",
      "         [ 60.2230, 176.7131],\n",
      "         [ 63.6457, 177.7204],\n",
      "         [ 79.7566, 192.3301],\n",
      "         [ 86.8934, 191.2210],\n",
      "         [ 68.2233, 218.3618],\n",
      "         [ 82.1300, 217.9619],\n",
      "         [ 72.7904, 252.6936],\n",
      "         [ 85.4167, 246.7923]]])\n",
      "xyn: tensor([[[0.1770, 0.4017],\n",
      "         [0.1807, 0.3948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1964, 0.3955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2031, 0.4363],\n",
      "         [0.2264, 0.4361],\n",
      "         [0.2085, 0.4888],\n",
      "         [0.2332, 0.4889],\n",
      "         [0.1673, 0.4909],\n",
      "         [0.1768, 0.4937],\n",
      "         [0.2215, 0.5343],\n",
      "         [0.2414, 0.5312],\n",
      "         [0.1895, 0.6066],\n",
      "         [0.2281, 0.6054],\n",
      "         [0.2022, 0.7019],\n",
      "         [0.2373, 0.6855]]])\n",
      "\n",
      "0: 640x640 1 person, 119.2ms\n",
      "Speed: 1.3ms preprocess, 119.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 159, 139],\n",
      "        [174, 158, 138],\n",
      "        [173, 157, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 136],\n",
      "        [171, 154, 134],\n",
      "        [171, 154, 134]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 132],\n",
      "        [166, 150, 130],\n",
      "        [168, 152, 132]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2841224670410156, 'inference': 119.19593811035156, 'postprocess': 0.40221214294433594}]\n",
      "Bounding Box: tensor([[ 70.5000, 198.5000,  47.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7736, 0.8464, 0.1994, 0.9156, 0.0999, 0.9943, 0.9096, 0.9841, 0.5214, 0.9574, 0.4874, 0.9973, 0.9898, 0.9961, 0.9831, 0.9829, 0.9561]])\n",
      "data: tensor([[[6.2862e+01, 1.4381e+02, 7.7360e-01],\n",
      "         [6.4272e+01, 1.4133e+02, 8.4640e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9935e-01],\n",
      "         [7.0298e+01, 1.4187e+02, 9.1562e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.9931e-02],\n",
      "         [7.3102e+01, 1.5681e+02, 9.9428e-01],\n",
      "         [8.0538e+01, 1.5711e+02, 9.0960e-01],\n",
      "         [7.5411e+01, 1.7539e+02, 9.8405e-01],\n",
      "         [8.6033e+01, 1.7715e+02, 5.2139e-01],\n",
      "         [5.9210e+01, 1.7648e+02, 9.5737e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8740e-01],\n",
      "         [7.8755e+01, 1.9225e+02, 9.9733e-01],\n",
      "         [8.4931e+01, 1.9130e+02, 9.8984e-01],\n",
      "         [6.8583e+01, 2.1890e+02, 9.9606e-01],\n",
      "         [8.1702e+01, 2.1857e+02, 9.8312e-01],\n",
      "         [7.1697e+01, 2.5333e+02, 9.8292e-01],\n",
      "         [8.4177e+01, 2.4718e+02, 9.5607e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 62.8622, 143.8113],\n",
      "         [ 64.2718, 141.3349],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.2978, 141.8654],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.1022, 156.8089],\n",
      "         [ 80.5384, 157.1097],\n",
      "         [ 75.4112, 175.3866],\n",
      "         [ 86.0332, 177.1462],\n",
      "         [ 59.2100, 176.4756],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.7550, 192.2534],\n",
      "         [ 84.9309, 191.3020],\n",
      "         [ 68.5834, 218.9040],\n",
      "         [ 81.7019, 218.5729],\n",
      "         [ 71.6966, 253.3315],\n",
      "         [ 84.1769, 247.1834]]])\n",
      "xyn: tensor([[[0.1746, 0.3995],\n",
      "         [0.1785, 0.3926],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1953, 0.3941],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2031, 0.4356],\n",
      "         [0.2237, 0.4364],\n",
      "         [0.2095, 0.4872],\n",
      "         [0.2390, 0.4921],\n",
      "         [0.1645, 0.4902],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2188, 0.5340],\n",
      "         [0.2359, 0.5314],\n",
      "         [0.1905, 0.6081],\n",
      "         [0.2269, 0.6071],\n",
      "         [0.1992, 0.7037],\n",
      "         [0.2338, 0.6866]]])\n",
      "\n",
      "0: 640x640 1 person, 115.3ms\n",
      "Speed: 1.3ms preprocess, 115.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 138],\n",
      "        [173, 157, 137],\n",
      "        [173, 157, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 134],\n",
      "        [171, 154, 134],\n",
      "        [172, 155, 136]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [167, 151, 131],\n",
      "        [167, 151, 131],\n",
      "        [169, 153, 133]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2857913970947266, 'inference': 115.31901359558105, 'postprocess': 0.7069110870361328}]\n",
      "Bounding Box: tensor([[ 69.5000, 199.0000,  47.0000, 134.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7552, 0.8343, 0.1841, 0.9101, 0.0942, 0.9943, 0.9046, 0.9847, 0.5061, 0.9571, 0.4706, 0.9975, 0.9900, 0.9962, 0.9829, 0.9835, 0.9562]])\n",
      "data: tensor([[[6.2226e+01, 1.4362e+02, 7.5517e-01],\n",
      "         [6.3723e+01, 1.4134e+02, 8.3430e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8412e-01],\n",
      "         [6.9389e+01, 1.4203e+02, 9.1005e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.4236e-02],\n",
      "         [7.3125e+01, 1.5684e+02, 9.9434e-01],\n",
      "         [7.6715e+01, 1.5697e+02, 9.0459e-01],\n",
      "         [7.5179e+01, 1.7541e+02, 9.8467e-01],\n",
      "         [7.8278e+01, 1.7591e+02, 5.0607e-01],\n",
      "         [5.8214e+01, 1.7702e+02, 9.5707e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7065e-01],\n",
      "         [7.9103e+01, 1.9251e+02, 9.9748e-01],\n",
      "         [8.3033e+01, 1.9137e+02, 9.9003e-01],\n",
      "         [6.8860e+01, 2.1949e+02, 9.9621e-01],\n",
      "         [8.1076e+01, 2.1795e+02, 9.8293e-01],\n",
      "         [7.0970e+01, 2.5347e+02, 9.8351e-01],\n",
      "         [8.4156e+01, 2.4594e+02, 9.5623e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 62.2262, 143.6246],\n",
      "         [ 63.7228, 141.3423],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3887, 142.0291],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.1246, 156.8356],\n",
      "         [ 76.7150, 156.9692],\n",
      "         [ 75.1787, 175.4084],\n",
      "         [ 78.2781, 175.9096],\n",
      "         [ 58.2135, 177.0159],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.1029, 192.5071],\n",
      "         [ 83.0333, 191.3708],\n",
      "         [ 68.8604, 219.4854],\n",
      "         [ 81.0765, 217.9499],\n",
      "         [ 70.9698, 253.4713],\n",
      "         [ 84.1558, 245.9352]]])\n",
      "xyn: tensor([[[0.1729, 0.3990],\n",
      "         [0.1770, 0.3926],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1927, 0.3945],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2031, 0.4357],\n",
      "         [0.2131, 0.4360],\n",
      "         [0.2088, 0.4872],\n",
      "         [0.2174, 0.4886],\n",
      "         [0.1617, 0.4917],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2197, 0.5347],\n",
      "         [0.2306, 0.5316],\n",
      "         [0.1913, 0.6097],\n",
      "         [0.2252, 0.6054],\n",
      "         [0.1971, 0.7041],\n",
      "         [0.2338, 0.6832]]])\n",
      "\n",
      "0: 640x640 1 person, 115.9ms\n",
      "Speed: 1.4ms preprocess, 115.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 159, 139],\n",
      "        [174, 158, 138],\n",
      "        [173, 157, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 136],\n",
      "        [171, 154, 134],\n",
      "        [172, 155, 136]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 132],\n",
      "        [167, 151, 131],\n",
      "        [168, 152, 132]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3530254364013672, 'inference': 115.88096618652344, 'postprocess': 0.7877349853515625}]\n",
      "Bounding Box: tensor([[ 68., 199.,  48., 134.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7396, 0.8204, 0.1658, 0.9129, 0.0878, 0.9948, 0.9183, 0.9854, 0.5433, 0.9576, 0.4893, 0.9978, 0.9916, 0.9963, 0.9839, 0.9830, 0.9562]])\n",
      "data: tensor([[[6.0931e+01, 1.4370e+02, 7.3959e-01],\n",
      "         [6.2381e+01, 1.4131e+02, 8.2041e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6577e-01],\n",
      "         [6.8320e+01, 1.4201e+02, 9.1288e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.7824e-02],\n",
      "         [7.3579e+01, 1.5726e+02, 9.9481e-01],\n",
      "         [7.4062e+01, 1.5743e+02, 9.1833e-01],\n",
      "         [7.5980e+01, 1.7596e+02, 9.8541e-01],\n",
      "         [7.2334e+01, 1.7585e+02, 5.4328e-01],\n",
      "         [5.7485e+01, 1.7742e+02, 9.5758e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8929e-01],\n",
      "         [7.8744e+01, 1.9340e+02, 9.9779e-01],\n",
      "         [8.0888e+01, 1.9224e+02, 9.9161e-01],\n",
      "         [6.8565e+01, 2.1989e+02, 9.9629e-01],\n",
      "         [8.0081e+01, 2.1834e+02, 9.8388e-01],\n",
      "         [7.1269e+01, 2.5368e+02, 9.8295e-01],\n",
      "         [8.4446e+01, 2.4549e+02, 9.5622e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 60.9313, 143.6964],\n",
      "         [ 62.3809, 141.3136],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.3205, 142.0121],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.5793, 157.2631],\n",
      "         [ 74.0619, 157.4273],\n",
      "         [ 75.9799, 175.9592],\n",
      "         [ 72.3345, 175.8455],\n",
      "         [ 57.4847, 177.4167],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.7439, 193.4012],\n",
      "         [ 80.8876, 192.2356],\n",
      "         [ 68.5652, 219.8874],\n",
      "         [ 80.0810, 218.3384],\n",
      "         [ 71.2689, 253.6792],\n",
      "         [ 84.4458, 245.4918]]])\n",
      "xyn: tensor([[[0.1693, 0.3992],\n",
      "         [0.1733, 0.3925],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1898, 0.3945],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2044, 0.4368],\n",
      "         [0.2057, 0.4373],\n",
      "         [0.2111, 0.4888],\n",
      "         [0.2009, 0.4885],\n",
      "         [0.1597, 0.4928],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2187, 0.5372],\n",
      "         [0.2247, 0.5340],\n",
      "         [0.1905, 0.6108],\n",
      "         [0.2224, 0.6065],\n",
      "         [0.1980, 0.7047],\n",
      "         [0.2346, 0.6819]]])\n",
      "\n",
      "0: 640x640 1 person, 125.4ms\n",
      "Speed: 1.3ms preprocess, 125.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 137],\n",
      "        [172, 155, 136],\n",
      "        [173, 157, 137]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [169, 153, 133],\n",
      "        [171, 154, 134]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 132],\n",
      "        [168, 152, 132],\n",
      "        [169, 153, 133]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3020038604736328, 'inference': 125.43511390686035, 'postprocess': 0.40984153747558594}]\n",
      "Bounding Box: tensor([[ 68.5000, 200.0000,  49.0000, 134.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7375, 0.8211, 0.1623, 0.9133, 0.0848, 0.9948, 0.9159, 0.9857, 0.5339, 0.9582, 0.4809, 0.9979, 0.9918, 0.9966, 0.9847, 0.9841, 0.9587]])\n",
      "data: tensor([[[6.0217e+01, 1.4341e+02, 7.3750e-01],\n",
      "         [6.1760e+01, 1.4113e+02, 8.2112e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6233e-01],\n",
      "         [6.7680e+01, 1.4200e+02, 9.1331e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.4814e-02],\n",
      "         [7.3003e+01, 1.5662e+02, 9.9479e-01],\n",
      "         [7.3393e+01, 1.5726e+02, 9.1586e-01],\n",
      "         [7.6085e+01, 1.7519e+02, 9.8567e-01],\n",
      "         [7.3363e+01, 1.7651e+02, 5.3385e-01],\n",
      "         [5.8308e+01, 1.7709e+02, 9.5821e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8090e-01],\n",
      "         [7.9277e+01, 1.9245e+02, 9.9789e-01],\n",
      "         [8.1278e+01, 1.9145e+02, 9.9184e-01],\n",
      "         [6.8196e+01, 2.1997e+02, 9.9657e-01],\n",
      "         [7.9222e+01, 2.1842e+02, 9.8467e-01],\n",
      "         [7.1519e+01, 2.5455e+02, 9.8415e-01],\n",
      "         [8.5059e+01, 2.4553e+02, 9.5866e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 60.2169, 143.4070],\n",
      "         [ 61.7603, 141.1346],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.6802, 142.0010],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.0033, 156.6171],\n",
      "         [ 73.3934, 157.2579],\n",
      "         [ 76.0849, 175.1925],\n",
      "         [ 73.3629, 176.5068],\n",
      "         [ 58.3080, 177.0861],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.2774, 192.4473],\n",
      "         [ 81.2782, 191.4544],\n",
      "         [ 68.1964, 219.9706],\n",
      "         [ 79.2220, 218.4154],\n",
      "         [ 71.5193, 254.5531],\n",
      "         [ 85.0592, 245.5325]]])\n",
      "xyn: tensor([[[0.1673, 0.3984],\n",
      "         [0.1716, 0.3920],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1880, 0.3944],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2028, 0.4350],\n",
      "         [0.2039, 0.4368],\n",
      "         [0.2113, 0.4866],\n",
      "         [0.2038, 0.4903],\n",
      "         [0.1620, 0.4919],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2202, 0.5346],\n",
      "         [0.2258, 0.5318],\n",
      "         [0.1894, 0.6110],\n",
      "         [0.2201, 0.6067],\n",
      "         [0.1987, 0.7071],\n",
      "         [0.2363, 0.6820]]])\n",
      "\n",
      "0: 640x640 1 person, 115.2ms\n",
      "Speed: 1.3ms preprocess, 115.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 137],\n",
      "        [173, 157, 137],\n",
      "        [174, 158, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [171, 154, 134],\n",
      "        [171, 154, 134]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 132],\n",
      "        [169, 153, 133],\n",
      "        [169, 153, 133]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2581348419189453, 'inference': 115.24701118469238, 'postprocess': 0.4100799560546875}]\n",
      "Bounding Box: tensor([[ 68.5000, 199.5000,  47.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7455, 0.8293, 0.1683, 0.9167, 0.0858, 0.9948, 0.9141, 0.9854, 0.5218, 0.9570, 0.4694, 0.9977, 0.9912, 0.9962, 0.9828, 0.9825, 0.9544]])\n",
      "data: tensor([[[5.9493e+01, 1.4363e+02, 7.4550e-01],\n",
      "         [6.1064e+01, 1.4121e+02, 8.2928e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6834e-01],\n",
      "         [6.7124e+01, 1.4216e+02, 9.1671e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.5778e-02],\n",
      "         [7.2798e+01, 1.5779e+02, 9.9484e-01],\n",
      "         [7.2353e+01, 1.5815e+02, 9.1406e-01],\n",
      "         [7.5632e+01, 1.7648e+02, 9.8536e-01],\n",
      "         [7.2354e+01, 1.7658e+02, 5.2175e-01],\n",
      "         [5.6172e+01, 1.7940e+02, 9.5700e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6943e-01],\n",
      "         [7.8368e+01, 1.9369e+02, 9.9774e-01],\n",
      "         [7.9815e+01, 1.9254e+02, 9.9118e-01],\n",
      "         [6.8110e+01, 2.2108e+02, 9.9617e-01],\n",
      "         [7.7986e+01, 2.1933e+02, 9.8283e-01],\n",
      "         [7.1221e+01, 2.5475e+02, 9.8247e-01],\n",
      "         [8.4959e+01, 2.4531e+02, 9.5439e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 59.4929, 143.6257],\n",
      "         [ 61.0639, 141.2091],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.1236, 142.1604],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.7977, 157.7948],\n",
      "         [ 72.3526, 158.1513],\n",
      "         [ 75.6322, 176.4814],\n",
      "         [ 72.3544, 176.5802],\n",
      "         [ 56.1717, 179.3992],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.3680, 193.6883],\n",
      "         [ 79.8151, 192.5393],\n",
      "         [ 68.1098, 221.0780],\n",
      "         [ 77.9862, 219.3319],\n",
      "         [ 71.2209, 254.7544],\n",
      "         [ 84.9586, 245.3138]]])\n",
      "xyn: tensor([[[0.1653, 0.3990],\n",
      "         [0.1696, 0.3922],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1865, 0.3949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2022, 0.4383],\n",
      "         [0.2010, 0.4393],\n",
      "         [0.2101, 0.4902],\n",
      "         [0.2010, 0.4905],\n",
      "         [0.1560, 0.4983],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2177, 0.5380],\n",
      "         [0.2217, 0.5348],\n",
      "         [0.1892, 0.6141],\n",
      "         [0.2166, 0.6093],\n",
      "         [0.1978, 0.7077],\n",
      "         [0.2360, 0.6814]]])\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        [154, 174, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 137],\n",
      "        [173, 157, 137],\n",
      "        [174, 158, 138]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 133],\n",
      "        [171, 154, 134],\n",
      "        [169, 153, 133]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 132],\n",
      "        [169, 153, 133],\n",
      "        [169, 153, 133]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2369155883789062, 'inference': 113.80195617675781, 'postprocess': 0.6620883941650391}]\n",
      "Bounding Box: tensor([[ 68.5000, 200.0000,  47.0000, 134.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7921, 0.8550, 0.1921, 0.9271, 0.0828, 0.9959, 0.9384, 0.9878, 0.6008, 0.9628, 0.5342, 0.9983, 0.9936, 0.9975, 0.9895, 0.9891, 0.9727]])\n",
      "data: tensor([[[5.8195e+01, 1.4379e+02, 7.9214e-01],\n",
      "         [5.9875e+01, 1.4122e+02, 8.5500e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9212e-01],\n",
      "         [6.6431e+01, 1.4213e+02, 9.2713e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.2782e-02],\n",
      "         [7.1811e+01, 1.5794e+02, 9.9590e-01],\n",
      "         [7.0944e+01, 1.5851e+02, 9.3842e-01],\n",
      "         [7.5620e+01, 1.7747e+02, 9.8777e-01],\n",
      "         [7.0933e+01, 1.7787e+02, 6.0082e-01],\n",
      "         [5.7488e+01, 1.7977e+02, 9.6275e-01],\n",
      "         [5.4351e+01, 1.8132e+02, 5.3425e-01],\n",
      "         [7.6452e+01, 1.9422e+02, 9.9828e-01],\n",
      "         [7.7423e+01, 1.9331e+02, 9.9365e-01],\n",
      "         [6.7552e+01, 2.2154e+02, 9.9752e-01],\n",
      "         [7.6263e+01, 2.2055e+02, 9.8946e-01],\n",
      "         [7.2573e+01, 2.5599e+02, 9.8912e-01],\n",
      "         [8.3626e+01, 2.4600e+02, 9.7269e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 58.1954, 143.7896],\n",
      "         [ 59.8746, 141.2186],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.4306, 142.1297],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.8110, 157.9377],\n",
      "         [ 70.9436, 158.5050],\n",
      "         [ 75.6205, 177.4669],\n",
      "         [ 70.9331, 177.8709],\n",
      "         [ 57.4878, 179.7674],\n",
      "         [ 54.3509, 181.3165],\n",
      "         [ 76.4524, 194.2248],\n",
      "         [ 77.4233, 193.3070],\n",
      "         [ 67.5524, 221.5408],\n",
      "         [ 76.2634, 220.5454],\n",
      "         [ 72.5729, 255.9861],\n",
      "         [ 83.6261, 246.0015]]])\n",
      "xyn: tensor([[[0.1617, 0.3994],\n",
      "         [0.1663, 0.3923],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1845, 0.3948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1995, 0.4387],\n",
      "         [0.1971, 0.4403],\n",
      "         [0.2101, 0.4930],\n",
      "         [0.1970, 0.4941],\n",
      "         [0.1597, 0.4994],\n",
      "         [0.1510, 0.5037],\n",
      "         [0.2124, 0.5395],\n",
      "         [0.2151, 0.5370],\n",
      "         [0.1876, 0.6154],\n",
      "         [0.2118, 0.6126],\n",
      "         [0.2016, 0.7111],\n",
      "         [0.2323, 0.6833]]])\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 1.3ms preprocess, 113.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 136],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 134],\n",
      "        [169, 153, 136],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2788772583007812, 'inference': 113.30604553222656, 'postprocess': 0.40602684020996094}]\n",
      "Bounding Box: tensor([[ 69., 200.,  46., 134.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8497, 0.8840, 0.2812, 0.9242, 0.1087, 0.9962, 0.9547, 0.9877, 0.6871, 0.9651, 0.6215, 0.9984, 0.9947, 0.9975, 0.9909, 0.9889, 0.9745]])\n",
      "data: tensor([[[5.7021e+01, 1.4423e+02, 8.4970e-01],\n",
      "         [5.8889e+01, 1.4175e+02, 8.8400e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8122e-01],\n",
      "         [6.5512e+01, 1.4281e+02, 9.2416e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0874e-01],\n",
      "         [7.1960e+01, 1.5796e+02, 9.9619e-01],\n",
      "         [6.7376e+01, 1.5858e+02, 9.5474e-01],\n",
      "         [7.6948e+01, 1.7860e+02, 9.8773e-01],\n",
      "         [6.8851e+01, 1.7866e+02, 6.8713e-01],\n",
      "         [5.9945e+01, 1.8089e+02, 9.6507e-01],\n",
      "         [5.5067e+01, 1.8271e+02, 6.2146e-01],\n",
      "         [7.6450e+01, 1.9327e+02, 9.9836e-01],\n",
      "         [7.4769e+01, 1.9243e+02, 9.9468e-01],\n",
      "         [6.7518e+01, 2.2123e+02, 9.9755e-01],\n",
      "         [7.4584e+01, 2.2016e+02, 9.9086e-01],\n",
      "         [7.3228e+01, 2.5572e+02, 9.8888e-01],\n",
      "         [8.3764e+01, 2.4569e+02, 9.7454e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 57.0209, 144.2342],\n",
      "         [ 58.8888, 141.7482],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.5115, 142.8088],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.9602, 157.9597],\n",
      "         [ 67.3758, 158.5752],\n",
      "         [ 76.9485, 178.6006],\n",
      "         [ 68.8510, 178.6606],\n",
      "         [ 59.9445, 180.8863],\n",
      "         [ 55.0675, 182.7083],\n",
      "         [ 76.4497, 193.2725],\n",
      "         [ 74.7691, 192.4301],\n",
      "         [ 67.5178, 221.2281],\n",
      "         [ 74.5838, 220.1565],\n",
      "         [ 73.2284, 255.7234],\n",
      "         [ 83.7636, 245.6884]]])\n",
      "xyn: tensor([[[0.1584, 0.4007],\n",
      "         [0.1636, 0.3937],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1820, 0.3967],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1999, 0.4388],\n",
      "         [0.1872, 0.4405],\n",
      "         [0.2137, 0.4961],\n",
      "         [0.1913, 0.4963],\n",
      "         [0.1665, 0.5025],\n",
      "         [0.1530, 0.5075],\n",
      "         [0.2124, 0.5369],\n",
      "         [0.2077, 0.5345],\n",
      "         [0.1875, 0.6145],\n",
      "         [0.2072, 0.6115],\n",
      "         [0.2034, 0.7103],\n",
      "         [0.2327, 0.6825]]])\n",
      "\n",
      "0: 640x640 1 person, 114.8ms\n",
      "Speed: 1.3ms preprocess, 114.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 134],\n",
      "        [171, 154, 137],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [172, 155, 138],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.271963119506836, 'inference': 114.83311653137207, 'postprocess': 0.4286766052246094}]\n",
      "Bounding Box: tensor([[ 69., 200.,  46., 134.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8546, 0.8916, 0.2858, 0.9310, 0.1058, 0.9961, 0.9544, 0.9868, 0.6748, 0.9621, 0.6019, 0.9982, 0.9943, 0.9972, 0.9898, 0.9876, 0.9721]])\n",
      "data: tensor([[[5.6244e+01, 1.4461e+02, 8.5459e-01],\n",
      "         [5.8165e+01, 1.4215e+02, 8.9155e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8577e-01],\n",
      "         [6.5009e+01, 1.4330e+02, 9.3097e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0578e-01],\n",
      "         [7.1479e+01, 1.5838e+02, 9.9615e-01],\n",
      "         [6.7084e+01, 1.5908e+02, 9.5438e-01],\n",
      "         [7.6906e+01, 1.7853e+02, 9.8682e-01],\n",
      "         [6.9185e+01, 1.7882e+02, 6.7483e-01],\n",
      "         [6.1095e+01, 1.8202e+02, 9.6214e-01],\n",
      "         [5.5207e+01, 1.8342e+02, 6.0189e-01],\n",
      "         [7.5685e+01, 1.9435e+02, 9.9822e-01],\n",
      "         [7.4107e+01, 1.9350e+02, 9.9427e-01],\n",
      "         [6.6724e+01, 2.2214e+02, 9.9723e-01],\n",
      "         [7.3772e+01, 2.2092e+02, 9.8981e-01],\n",
      "         [7.2478e+01, 2.5593e+02, 9.8761e-01],\n",
      "         [8.3391e+01, 2.4570e+02, 9.7211e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 56.2435, 144.6134],\n",
      "         [ 58.1647, 142.1547],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.0092, 143.2984],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.4795, 158.3752],\n",
      "         [ 67.0840, 159.0810],\n",
      "         [ 76.9058, 178.5339],\n",
      "         [ 69.1846, 178.8184],\n",
      "         [ 61.0950, 182.0203],\n",
      "         [ 55.2072, 183.4209],\n",
      "         [ 75.6845, 194.3450],\n",
      "         [ 74.1074, 193.4994],\n",
      "         [ 66.7241, 222.1422],\n",
      "         [ 73.7724, 220.9218],\n",
      "         [ 72.4775, 255.9321],\n",
      "         [ 83.3912, 245.7034]]])\n",
      "xyn: tensor([[[0.1562, 0.4017],\n",
      "         [0.1616, 0.3949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1806, 0.3981],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1986, 0.4399],\n",
      "         [0.1863, 0.4419],\n",
      "         [0.2136, 0.4959],\n",
      "         [0.1922, 0.4967],\n",
      "         [0.1697, 0.5056],\n",
      "         [0.1534, 0.5095],\n",
      "         [0.2102, 0.5398],\n",
      "         [0.2059, 0.5375],\n",
      "         [0.1853, 0.6171],\n",
      "         [0.2049, 0.6137],\n",
      "         [0.2013, 0.7109],\n",
      "         [0.2316, 0.6825]]])\n",
      "\n",
      "0: 640x640 1 person, 113.1ms\n",
      "Speed: 1.3ms preprocess, 113.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 134],\n",
      "        [171, 154, 137],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [172, 155, 138],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3229846954345703, 'inference': 113.05403709411621, 'postprocess': 0.38909912109375}]\n",
      "Bounding Box: tensor([[ 68.5000, 199.5000,  47.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8786, 0.8951, 0.3506, 0.9152, 0.1234, 0.9958, 0.9625, 0.9865, 0.7515, 0.9657, 0.6960, 0.9982, 0.9950, 0.9970, 0.9905, 0.9862, 0.9717]])\n",
      "data: tensor([[[5.5593e+01, 1.4507e+02, 8.7863e-01],\n",
      "         [5.7631e+01, 1.4253e+02, 8.9509e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5060e-01],\n",
      "         [6.4353e+01, 1.4341e+02, 9.1519e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2343e-01],\n",
      "         [7.0918e+01, 1.5829e+02, 9.9582e-01],\n",
      "         [6.5411e+01, 1.5906e+02, 9.6251e-01],\n",
      "         [7.7150e+01, 1.7836e+02, 9.8646e-01],\n",
      "         [6.7961e+01, 1.7852e+02, 7.5154e-01],\n",
      "         [6.1765e+01, 1.8279e+02, 9.6574e-01],\n",
      "         [5.4572e+01, 1.8447e+02, 6.9599e-01],\n",
      "         [7.5615e+01, 1.9438e+02, 9.9821e-01],\n",
      "         [7.3417e+01, 1.9351e+02, 9.9499e-01],\n",
      "         [6.5322e+01, 2.2265e+02, 9.9705e-01],\n",
      "         [7.2035e+01, 2.2113e+02, 9.9046e-01],\n",
      "         [7.2150e+01, 2.5620e+02, 9.8620e-01],\n",
      "         [8.3649e+01, 2.4538e+02, 9.7171e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 55.5925, 145.0742],\n",
      "         [ 57.6313, 142.5336],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.3525, 143.4103],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.9181, 158.2916],\n",
      "         [ 65.4107, 159.0638],\n",
      "         [ 77.1503, 178.3627],\n",
      "         [ 67.9610, 178.5196],\n",
      "         [ 61.7650, 182.7853],\n",
      "         [ 54.5720, 184.4662],\n",
      "         [ 75.6148, 194.3766],\n",
      "         [ 73.4168, 193.5087],\n",
      "         [ 65.3216, 222.6530],\n",
      "         [ 72.0350, 221.1331],\n",
      "         [ 72.1495, 256.2034],\n",
      "         [ 83.6490, 245.3804]]])\n",
      "xyn: tensor([[[0.1544, 0.4030],\n",
      "         [0.1601, 0.3959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1788, 0.3984],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1970, 0.4397],\n",
      "         [0.1817, 0.4418],\n",
      "         [0.2143, 0.4955],\n",
      "         [0.1888, 0.4959],\n",
      "         [0.1716, 0.5077],\n",
      "         [0.1516, 0.5124],\n",
      "         [0.2100, 0.5399],\n",
      "         [0.2039, 0.5375],\n",
      "         [0.1814, 0.6185],\n",
      "         [0.2001, 0.6143],\n",
      "         [0.2004, 0.7117],\n",
      "         [0.2324, 0.6816]]])\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.2ms preprocess, 114.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [168, 152, 134],\n",
      "        [171, 154, 137],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [172, 155, 138],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2209415435791016, 'inference': 114.22991752624512, 'postprocess': 0.41294097900390625}]\n",
      "Bounding Box: tensor([[ 68.5000, 199.5000,  47.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9080, 0.9253, 0.4241, 0.9243, 0.1240, 0.9956, 0.9576, 0.9852, 0.7192, 0.9646, 0.6748, 0.9978, 0.9937, 0.9967, 0.9890, 0.9857, 0.9702]])\n",
      "data: tensor([[[5.4977e+01, 1.4573e+02, 9.0798e-01],\n",
      "         [5.7108e+01, 1.4309e+02, 9.2533e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2411e-01],\n",
      "         [6.4162e+01, 1.4384e+02, 9.2429e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2395e-01],\n",
      "         [7.0748e+01, 1.5842e+02, 9.9557e-01],\n",
      "         [6.4922e+01, 1.5992e+02, 9.5755e-01],\n",
      "         [7.6304e+01, 1.7850e+02, 9.8524e-01],\n",
      "         [6.7705e+01, 1.7954e+02, 7.1917e-01],\n",
      "         [6.1100e+01, 1.8395e+02, 9.6456e-01],\n",
      "         [5.4480e+01, 1.8559e+02, 6.7479e-01],\n",
      "         [7.6022e+01, 1.9455e+02, 9.9779e-01],\n",
      "         [7.3516e+01, 1.9391e+02, 9.9367e-01],\n",
      "         [6.4262e+01, 2.2237e+02, 9.9670e-01],\n",
      "         [7.1537e+01, 2.2075e+02, 9.8902e-01],\n",
      "         [7.2297e+01, 2.5620e+02, 9.8569e-01],\n",
      "         [8.3695e+01, 2.4516e+02, 9.7019e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.9773, 145.7285],\n",
      "         [ 57.1081, 143.0941],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.1618, 143.8394],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.7478, 158.4175],\n",
      "         [ 64.9219, 159.9197],\n",
      "         [ 76.3039, 178.5036],\n",
      "         [ 67.7054, 179.5418],\n",
      "         [ 61.0997, 183.9537],\n",
      "         [ 54.4799, 185.5925],\n",
      "         [ 76.0224, 194.5526],\n",
      "         [ 73.5155, 193.9090],\n",
      "         [ 64.2623, 222.3747],\n",
      "         [ 71.5367, 220.7464],\n",
      "         [ 72.2972, 256.1959],\n",
      "         [ 83.6953, 245.1620]]])\n",
      "xyn: tensor([[[0.1527, 0.4048],\n",
      "         [0.1586, 0.3975],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1782, 0.3996],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1965, 0.4400],\n",
      "         [0.1803, 0.4442],\n",
      "         [0.2120, 0.4958],\n",
      "         [0.1881, 0.4987],\n",
      "         [0.1697, 0.5110],\n",
      "         [0.1513, 0.5155],\n",
      "         [0.2112, 0.5404],\n",
      "         [0.2042, 0.5386],\n",
      "         [0.1785, 0.6177],\n",
      "         [0.1987, 0.6132],\n",
      "         [0.2008, 0.7117],\n",
      "         [0.2325, 0.6810]]])\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.3ms preprocess, 116.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [174, 158, 140],\n",
      "        [174, 158, 140]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 136],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2967586517333984, 'inference': 116.09005928039551, 'postprocess': 0.6117820739746094}]\n",
      "Bounding Box: tensor([[ 69.5000, 200.0000,  47.0000, 132.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8964, 0.9152, 0.3870, 0.9203, 0.1188, 0.9958, 0.9584, 0.9867, 0.7326, 0.9674, 0.6878, 0.9981, 0.9945, 0.9971, 0.9901, 0.9867, 0.9719]])\n",
      "data: tensor([[[5.4704e+01, 1.4665e+02, 8.9644e-01],\n",
      "         [5.6419e+01, 1.4384e+02, 9.1519e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8698e-01],\n",
      "         [6.3283e+01, 1.4372e+02, 9.2033e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1884e-01],\n",
      "         [7.1163e+01, 1.5816e+02, 9.9580e-01],\n",
      "         [6.5302e+01, 1.5975e+02, 9.5838e-01],\n",
      "         [7.7817e+01, 1.7828e+02, 9.8673e-01],\n",
      "         [6.7882e+01, 1.7874e+02, 7.3261e-01],\n",
      "         [6.2768e+01, 1.8342e+02, 9.6742e-01],\n",
      "         [5.4978e+01, 1.8474e+02, 6.8780e-01],\n",
      "         [7.6779e+01, 1.9426e+02, 9.9811e-01],\n",
      "         [7.4352e+01, 1.9374e+02, 9.9449e-01],\n",
      "         [6.5105e+01, 2.2254e+02, 9.9705e-01],\n",
      "         [7.1674e+01, 2.2148e+02, 9.9005e-01],\n",
      "         [7.2558e+01, 2.5561e+02, 9.8673e-01],\n",
      "         [8.4038e+01, 2.4588e+02, 9.7190e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.7041, 146.6506],\n",
      "         [ 56.4194, 143.8376],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.2830, 143.7233],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.1627, 158.1610],\n",
      "         [ 65.3020, 159.7537],\n",
      "         [ 77.8171, 178.2802],\n",
      "         [ 67.8823, 178.7429],\n",
      "         [ 62.7677, 183.4249],\n",
      "         [ 54.9776, 184.7439],\n",
      "         [ 76.7789, 194.2612],\n",
      "         [ 74.3516, 193.7374],\n",
      "         [ 65.1054, 222.5418],\n",
      "         [ 71.6744, 221.4761],\n",
      "         [ 72.5577, 255.6061],\n",
      "         [ 84.0379, 245.8806]]])\n",
      "xyn: tensor([[[0.1520, 0.4074],\n",
      "         [0.1567, 0.3995],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1758, 0.3992],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1977, 0.4393],\n",
      "         [0.1814, 0.4438],\n",
      "         [0.2162, 0.4952],\n",
      "         [0.1886, 0.4965],\n",
      "         [0.1744, 0.5095],\n",
      "         [0.1527, 0.5132],\n",
      "         [0.2133, 0.5396],\n",
      "         [0.2065, 0.5382],\n",
      "         [0.1808, 0.6182],\n",
      "         [0.1991, 0.6152],\n",
      "         [0.2015, 0.7100],\n",
      "         [0.2334, 0.6830]]])\n",
      "\n",
      "0: 640x640 1 person, 112.9ms\n",
      "Speed: 1.2ms preprocess, 112.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [174, 158, 140],\n",
      "        [174, 158, 140]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [169, 153, 136],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2440681457519531, 'inference': 112.87212371826172, 'postprocess': 0.41294097900390625}]\n",
      "Bounding Box: tensor([[ 69., 200.,  48., 132.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8608, 0.8950, 0.3005, 0.9172, 0.0957, 0.9953, 0.9418, 0.9866, 0.6601, 0.9657, 0.6161, 0.9979, 0.9932, 0.9968, 0.9880, 0.9862, 0.9682]])\n",
      "data: tensor([[[5.4668e+01, 1.4685e+02, 8.6076e-01],\n",
      "         [5.6281e+01, 1.4407e+02, 8.9505e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0047e-01],\n",
      "         [6.2904e+01, 1.4385e+02, 9.1719e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.5684e-02],\n",
      "         [7.0716e+01, 1.5808e+02, 9.9529e-01],\n",
      "         [6.5615e+01, 1.5965e+02, 9.4176e-01],\n",
      "         [7.7610e+01, 1.7708e+02, 9.8664e-01],\n",
      "         [6.8245e+01, 1.7828e+02, 6.6014e-01],\n",
      "         [6.1447e+01, 1.8341e+02, 9.6572e-01],\n",
      "         [5.4585e+01, 1.8554e+02, 6.1614e-01],\n",
      "         [7.6064e+01, 1.9428e+02, 9.9794e-01],\n",
      "         [7.4162e+01, 1.9369e+02, 9.9323e-01],\n",
      "         [6.4809e+01, 2.2272e+02, 9.9684e-01],\n",
      "         [7.0905e+01, 2.2144e+02, 9.8797e-01],\n",
      "         [7.2632e+01, 2.5542e+02, 9.8621e-01],\n",
      "         [8.4045e+01, 2.4562e+02, 9.6824e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.6685, 146.8504],\n",
      "         [ 56.2809, 144.0654],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 62.9042, 143.8501],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.7165, 158.0752],\n",
      "         [ 65.6149, 159.6550],\n",
      "         [ 77.6099, 177.0757],\n",
      "         [ 68.2446, 178.2833],\n",
      "         [ 61.4472, 183.4083],\n",
      "         [ 54.5846, 185.5417],\n",
      "         [ 76.0638, 194.2826],\n",
      "         [ 74.1623, 193.6950],\n",
      "         [ 64.8095, 222.7198],\n",
      "         [ 70.9045, 221.4386],\n",
      "         [ 72.6315, 255.4196],\n",
      "         [ 84.0454, 245.6156]]])\n",
      "xyn: tensor([[[0.1519, 0.4079],\n",
      "         [0.1563, 0.4002],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1747, 0.3996],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1964, 0.4391],\n",
      "         [0.1823, 0.4435],\n",
      "         [0.2156, 0.4919],\n",
      "         [0.1896, 0.4952],\n",
      "         [0.1707, 0.5095],\n",
      "         [0.1516, 0.5154],\n",
      "         [0.2113, 0.5397],\n",
      "         [0.2060, 0.5380],\n",
      "         [0.1800, 0.6187],\n",
      "         [0.1970, 0.6151],\n",
      "         [0.2018, 0.7095],\n",
      "         [0.2335, 0.6823]]])\n",
      "\n",
      "0: 640x640 1 person, 124.8ms\n",
      "Speed: 1.2ms preprocess, 124.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [173, 157, 139],\n",
      "        [169, 153, 136]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [175, 159, 141],\n",
      "        [180, 164, 146]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2347698211669922, 'inference': 124.75800514221191, 'postprocess': 0.492095947265625}]\n",
      "Bounding Box: tensor([[ 68.5000, 200.0000,  49.0000, 132.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8331, 0.8754, 0.2791, 0.9086, 0.1031, 0.9943, 0.9309, 0.9838, 0.6294, 0.9586, 0.5784, 0.9974, 0.9914, 0.9948, 0.9807, 0.9757, 0.9454]])\n",
      "data: tensor([[[5.4569e+01, 1.4679e+02, 8.3307e-01],\n",
      "         [5.6167e+01, 1.4399e+02, 8.7537e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7913e-01],\n",
      "         [6.2708e+01, 1.4376e+02, 9.0861e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0310e-01],\n",
      "         [7.0689e+01, 1.5790e+02, 9.9433e-01],\n",
      "         [6.5467e+01, 1.5929e+02, 9.3090e-01],\n",
      "         [7.5871e+01, 1.7742e+02, 9.8375e-01],\n",
      "         [6.6926e+01, 1.7817e+02, 6.2941e-01],\n",
      "         [6.1236e+01, 1.8433e+02, 9.5856e-01],\n",
      "         [5.3289e+01, 1.8535e+02, 5.7843e-01],\n",
      "         [7.5316e+01, 1.9472e+02, 9.9736e-01],\n",
      "         [7.3475e+01, 1.9404e+02, 9.9137e-01],\n",
      "         [6.4716e+01, 2.2281e+02, 9.9476e-01],\n",
      "         [7.2197e+01, 2.2096e+02, 9.8069e-01],\n",
      "         [7.1779e+01, 2.5347e+02, 9.7568e-01],\n",
      "         [8.5214e+01, 2.4541e+02, 9.4542e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.5692, 146.7871],\n",
      "         [ 56.1668, 143.9887],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 62.7077, 143.7635],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.6885, 157.8962],\n",
      "         [ 65.4675, 159.2883],\n",
      "         [ 75.8710, 177.4151],\n",
      "         [ 66.9264, 178.1732],\n",
      "         [ 61.2362, 184.3270],\n",
      "         [ 53.2887, 185.3504],\n",
      "         [ 75.3162, 194.7184],\n",
      "         [ 73.4751, 194.0430],\n",
      "         [ 64.7159, 222.8054],\n",
      "         [ 72.1967, 220.9625],\n",
      "         [ 71.7794, 253.4673],\n",
      "         [ 85.2137, 245.4068]]])\n",
      "xyn: tensor([[[0.1516, 0.4077],\n",
      "         [0.1560, 0.4000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1742, 0.3993],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1964, 0.4386],\n",
      "         [0.1819, 0.4425],\n",
      "         [0.2108, 0.4928],\n",
      "         [0.1859, 0.4949],\n",
      "         [0.1701, 0.5120],\n",
      "         [0.1480, 0.5149],\n",
      "         [0.2092, 0.5409],\n",
      "         [0.2041, 0.5390],\n",
      "         [0.1798, 0.6189],\n",
      "         [0.2005, 0.6138],\n",
      "         [0.1994, 0.7041],\n",
      "         [0.2367, 0.6817]]])\n",
      "\n",
      "0: 640x640 1 person, 123.6ms\n",
      "Speed: 1.3ms preprocess, 123.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        [158, 173, 200],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        [155, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [173, 157, 139],\n",
      "        [169, 153, 136]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [171, 154, 137],\n",
      "        [175, 159, 141],\n",
      "        [180, 164, 146]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3430118560791016, 'inference': 123.5802173614502, 'postprocess': 0.4177093505859375}]\n",
      "Bounding Box: tensor([[ 68., 200.,  50., 132.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8579, 0.8899, 0.2930, 0.9241, 0.1016, 0.9959, 0.9536, 0.9870, 0.6960, 0.9645, 0.6309, 0.9981, 0.9942, 0.9969, 0.9887, 0.9857, 0.9685]])\n",
      "data: tensor([[[5.4436e+01, 1.4656e+02, 8.5792e-01],\n",
      "         [5.6064e+01, 1.4376e+02, 8.8994e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9304e-01],\n",
      "         [6.2800e+01, 1.4370e+02, 9.2411e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0162e-01],\n",
      "         [7.0875e+01, 1.5777e+02, 9.9585e-01],\n",
      "         [6.4673e+01, 1.5971e+02, 9.5355e-01],\n",
      "         [7.6612e+01, 1.7718e+02, 9.8697e-01],\n",
      "         [6.6004e+01, 1.7871e+02, 6.9602e-01],\n",
      "         [6.1470e+01, 1.8444e+02, 9.6448e-01],\n",
      "         [5.2794e+01, 1.8592e+02, 6.3087e-01],\n",
      "         [7.5345e+01, 1.9427e+02, 9.9814e-01],\n",
      "         [7.2703e+01, 1.9386e+02, 9.9418e-01],\n",
      "         [6.4744e+01, 2.2235e+02, 9.9686e-01],\n",
      "         [7.0506e+01, 2.2117e+02, 9.8868e-01],\n",
      "         [7.2223e+01, 2.5492e+02, 9.8573e-01],\n",
      "         [8.4284e+01, 2.4503e+02, 9.6846e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.4364, 146.5568],\n",
      "         [ 56.0639, 143.7619],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 62.8005, 143.7041],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8755, 157.7718],\n",
      "         [ 64.6726, 159.7145],\n",
      "         [ 76.6123, 177.1844],\n",
      "         [ 66.0040, 178.7054],\n",
      "         [ 61.4700, 184.4411],\n",
      "         [ 52.7945, 185.9178],\n",
      "         [ 75.3455, 194.2735],\n",
      "         [ 72.7031, 193.8635],\n",
      "         [ 64.7438, 222.3498],\n",
      "         [ 70.5061, 221.1675],\n",
      "         [ 72.2227, 254.9152],\n",
      "         [ 84.2837, 245.0337]]])\n",
      "xyn: tensor([[[0.1512, 0.4071],\n",
      "         [0.1557, 0.3993],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1744, 0.3992],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1969, 0.4383],\n",
      "         [0.1796, 0.4437],\n",
      "         [0.2128, 0.4922],\n",
      "         [0.1833, 0.4964],\n",
      "         [0.1707, 0.5123],\n",
      "         [0.1467, 0.5164],\n",
      "         [0.2093, 0.5396],\n",
      "         [0.2020, 0.5385],\n",
      "         [0.1798, 0.6176],\n",
      "         [0.1959, 0.6144],\n",
      "         [0.2006, 0.7081],\n",
      "         [0.2341, 0.6806]]])\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.2ms preprocess, 114.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [148, 165, 192]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2390613555908203, 'inference': 114.52198028564453, 'postprocess': 0.6568431854248047}]\n",
      "Bounding Box: tensor([[ 67.5000, 199.5000,  51.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8856, 0.9113, 0.3538, 0.9351, 0.1358, 0.9960, 0.9641, 0.9862, 0.7142, 0.9646, 0.6472, 0.9981, 0.9944, 0.9974, 0.9909, 0.9895, 0.9771]])\n",
      "data: tensor([[[5.4720e+01, 1.4589e+02, 8.8562e-01],\n",
      "         [5.6433e+01, 1.4317e+02, 9.1127e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5385e-01],\n",
      "         [6.3031e+01, 1.4315e+02, 9.3513e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3582e-01],\n",
      "         [7.0347e+01, 1.5683e+02, 9.9597e-01],\n",
      "         [6.4541e+01, 1.5876e+02, 9.6405e-01],\n",
      "         [7.5590e+01, 1.7687e+02, 9.8623e-01],\n",
      "         [6.7176e+01, 1.7864e+02, 7.1417e-01],\n",
      "         [5.8833e+01, 1.8287e+02, 9.6460e-01],\n",
      "         [5.2156e+01, 1.8458e+02, 6.4718e-01],\n",
      "         [7.3865e+01, 1.9332e+02, 9.9813e-01],\n",
      "         [7.1658e+01, 1.9273e+02, 9.9444e-01],\n",
      "         [6.5062e+01, 2.2191e+02, 9.9736e-01],\n",
      "         [6.9815e+01, 2.2117e+02, 9.9091e-01],\n",
      "         [7.1065e+01, 2.5556e+02, 9.8953e-01],\n",
      "         [8.4255e+01, 2.4479e+02, 9.7715e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.7196, 145.8916],\n",
      "         [ 56.4333, 143.1731],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.0308, 143.1510],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.3470, 156.8315],\n",
      "         [ 64.5408, 158.7633],\n",
      "         [ 75.5903, 176.8660],\n",
      "         [ 67.1761, 178.6422],\n",
      "         [ 58.8331, 182.8747],\n",
      "         [ 52.1556, 184.5841],\n",
      "         [ 73.8655, 193.3199],\n",
      "         [ 71.6578, 192.7253],\n",
      "         [ 65.0615, 221.9058],\n",
      "         [ 69.8151, 221.1726],\n",
      "         [ 71.0655, 255.5638],\n",
      "         [ 84.2549, 244.7851]]])\n",
      "xyn: tensor([[[0.1520, 0.4053],\n",
      "         [0.1568, 0.3977],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1751, 0.3976],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1954, 0.4356],\n",
      "         [0.1793, 0.4410],\n",
      "         [0.2100, 0.4913],\n",
      "         [0.1866, 0.4962],\n",
      "         [0.1634, 0.5080],\n",
      "         [0.1449, 0.5127],\n",
      "         [0.2052, 0.5370],\n",
      "         [0.1990, 0.5353],\n",
      "         [0.1807, 0.6164],\n",
      "         [0.1939, 0.6144],\n",
      "         [0.1974, 0.7099],\n",
      "         [0.2340, 0.6800]]])\n",
      "\n",
      "0: 640x640 1 person, 115.0ms\n",
      "Speed: 1.4ms preprocess, 115.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3828277587890625, 'inference': 114.95828628540039, 'postprocess': 0.4107952117919922}]\n",
      "Bounding Box: tensor([[ 67.0000, 199.5000,  52.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8372, 0.8797, 0.2530, 0.9367, 0.1168, 0.9960, 0.9612, 0.9867, 0.6852, 0.9611, 0.5938, 0.9982, 0.9945, 0.9974, 0.9905, 0.9895, 0.9763]])\n",
      "data: tensor([[[5.4738e+01, 1.4624e+02, 8.3724e-01],\n",
      "         [5.6217e+01, 1.4344e+02, 8.7965e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5295e-01],\n",
      "         [6.2643e+01, 1.4306e+02, 9.3670e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1683e-01],\n",
      "         [6.9985e+01, 1.5656e+02, 9.9604e-01],\n",
      "         [6.5599e+01, 1.5843e+02, 9.6121e-01],\n",
      "         [7.3292e+01, 1.7717e+02, 9.8670e-01],\n",
      "         [6.6502e+01, 1.7919e+02, 6.8520e-01],\n",
      "         [5.6895e+01, 1.8225e+02, 9.6108e-01],\n",
      "         [5.0490e+01, 1.8478e+02, 5.9384e-01],\n",
      "         [7.4069e+01, 1.9351e+02, 9.9823e-01],\n",
      "         [7.2602e+01, 1.9290e+02, 9.9446e-01],\n",
      "         [6.5259e+01, 2.2197e+02, 9.9742e-01],\n",
      "         [6.9209e+01, 2.2112e+02, 9.9054e-01],\n",
      "         [7.0939e+01, 2.5568e+02, 9.8954e-01],\n",
      "         [8.3149e+01, 2.4476e+02, 9.7626e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.7376, 146.2363],\n",
      "         [ 56.2170, 143.4385],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 62.6426, 143.0630],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.9847, 156.5614],\n",
      "         [ 65.5988, 158.4309],\n",
      "         [ 73.2922, 177.1740],\n",
      "         [ 66.5025, 179.1910],\n",
      "         [ 56.8946, 182.2497],\n",
      "         [ 50.4896, 184.7795],\n",
      "         [ 74.0695, 193.5055],\n",
      "         [ 72.6023, 192.9007],\n",
      "         [ 65.2588, 221.9726],\n",
      "         [ 69.2092, 221.1164],\n",
      "         [ 70.9386, 255.6770],\n",
      "         [ 83.1487, 244.7617]]])\n",
      "xyn: tensor([[[0.1520, 0.4062],\n",
      "         [0.1562, 0.3984],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1740, 0.3974],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1944, 0.4349],\n",
      "         [0.1822, 0.4401],\n",
      "         [0.2036, 0.4922],\n",
      "         [0.1847, 0.4978],\n",
      "         [0.1580, 0.5062],\n",
      "         [0.1402, 0.5133],\n",
      "         [0.2057, 0.5375],\n",
      "         [0.2017, 0.5358],\n",
      "         [0.1813, 0.6166],\n",
      "         [0.1922, 0.6142],\n",
      "         [0.1971, 0.7102],\n",
      "         [0.2310, 0.6799]]])\n",
      "\n",
      "0: 640x640 1 person, 110.8ms\n",
      "Speed: 1.3ms preprocess, 110.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.276254653930664, 'inference': 110.82601547241211, 'postprocess': 0.4181861877441406}]\n",
      "Bounding Box: tensor([[ 66.5000, 199.5000,  53.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8223, 0.8698, 0.2300, 0.9346, 0.1106, 0.9960, 0.9581, 0.9868, 0.6678, 0.9617, 0.5817, 0.9983, 0.9945, 0.9976, 0.9910, 0.9904, 0.9778]])\n",
      "data: tensor([[[5.4795e+01, 1.4518e+02, 8.2234e-01],\n",
      "         [5.6409e+01, 1.4259e+02, 8.6977e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3000e-01],\n",
      "         [6.2798e+01, 1.4268e+02, 9.3459e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1061e-01],\n",
      "         [6.9314e+01, 1.5667e+02, 9.9595e-01],\n",
      "         [6.6543e+01, 1.5736e+02, 9.5810e-01],\n",
      "         [7.3351e+01, 1.7724e+02, 9.8684e-01],\n",
      "         [6.7817e+01, 1.7712e+02, 6.6781e-01],\n",
      "         [5.6054e+01, 1.8168e+02, 9.6168e-01],\n",
      "         [5.1259e+01, 1.8292e+02, 5.8168e-01],\n",
      "         [7.3993e+01, 1.9320e+02, 9.9828e-01],\n",
      "         [7.3828e+01, 1.9212e+02, 9.9445e-01],\n",
      "         [6.4768e+01, 2.2170e+02, 9.9762e-01],\n",
      "         [7.1086e+01, 2.2044e+02, 9.9100e-01],\n",
      "         [7.0502e+01, 2.5624e+02, 9.9040e-01],\n",
      "         [8.4326e+01, 2.4411e+02, 9.7775e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 54.7955, 145.1832],\n",
      "         [ 56.4092, 142.5860],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 62.7978, 142.6848],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3138, 156.6711],\n",
      "         [ 66.5426, 157.3550],\n",
      "         [ 73.3513, 177.2386],\n",
      "         [ 67.8167, 177.1239],\n",
      "         [ 56.0539, 181.6772],\n",
      "         [ 51.2592, 182.9197],\n",
      "         [ 73.9935, 193.1958],\n",
      "         [ 73.8284, 192.1196],\n",
      "         [ 64.7678, 221.6985],\n",
      "         [ 71.0859, 220.4368],\n",
      "         [ 70.5021, 256.2426],\n",
      "         [ 84.3259, 244.1053]]])\n",
      "xyn: tensor([[[0.1522, 0.4033],\n",
      "         [0.1567, 0.3961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1744, 0.3963],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1925, 0.4352],\n",
      "         [0.1848, 0.4371],\n",
      "         [0.2038, 0.4923],\n",
      "         [0.1884, 0.4920],\n",
      "         [0.1557, 0.5047],\n",
      "         [0.1424, 0.5081],\n",
      "         [0.2055, 0.5367],\n",
      "         [0.2051, 0.5337],\n",
      "         [0.1799, 0.6158],\n",
      "         [0.1975, 0.6123],\n",
      "         [0.1958, 0.7118],\n",
      "         [0.2342, 0.6781]]])\n",
      "\n",
      "0: 640x640 1 person, 117.0ms\n",
      "Speed: 1.3ms preprocess, 117.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193],\n",
      "        [150, 166, 193]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3010501861572266, 'inference': 116.95384979248047, 'postprocess': 0.5280971527099609}]\n",
      "Bounding Box: tensor([[ 66.5000, 199.5000,  53.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8141, 0.8686, 0.2211, 0.9429, 0.1097, 0.9970, 0.9563, 0.9893, 0.6400, 0.9601, 0.5297, 0.9984, 0.9940, 0.9976, 0.9898, 0.9901, 0.9754]])\n",
      "data: tensor([[[5.5704e+01, 1.4488e+02, 8.1406e-01],\n",
      "         [5.7412e+01, 1.4241e+02, 8.6856e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2110e-01],\n",
      "         [6.3395e+01, 1.4281e+02, 9.4289e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0969e-01],\n",
      "         [6.9544e+01, 1.5680e+02, 9.9696e-01],\n",
      "         [6.6938e+01, 1.5725e+02, 9.5634e-01],\n",
      "         [7.0423e+01, 1.7784e+02, 9.8929e-01],\n",
      "         [6.7739e+01, 1.7687e+02, 6.4002e-01],\n",
      "         [5.3428e+01, 1.8138e+02, 9.6008e-01],\n",
      "         [5.1397e+01, 1.8275e+02, 5.2968e-01],\n",
      "         [7.5667e+01, 1.9230e+02, 9.9836e-01],\n",
      "         [7.4992e+01, 1.9140e+02, 9.9402e-01],\n",
      "         [6.5868e+01, 2.2078e+02, 9.9764e-01],\n",
      "         [7.1398e+01, 2.1968e+02, 9.8983e-01],\n",
      "         [7.1762e+01, 2.5623e+02, 9.9014e-01],\n",
      "         [8.5623e+01, 2.4373e+02, 9.7544e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 55.7036, 144.8753],\n",
      "         [ 57.4121, 142.4143],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.3954, 142.8133],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.5438, 156.8037],\n",
      "         [ 66.9382, 157.2465],\n",
      "         [ 70.4233, 177.8384],\n",
      "         [ 67.7387, 176.8729],\n",
      "         [ 53.4283, 181.3799],\n",
      "         [ 51.3974, 182.7455],\n",
      "         [ 75.6665, 192.3000],\n",
      "         [ 74.9918, 191.4005],\n",
      "         [ 65.8682, 220.7835],\n",
      "         [ 71.3976, 219.6805],\n",
      "         [ 71.7620, 256.2306],\n",
      "         [ 85.6228, 243.7276]]])\n",
      "xyn: tensor([[[0.1547, 0.4024],\n",
      "         [0.1595, 0.3956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1761, 0.3967],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1932, 0.4356],\n",
      "         [0.1859, 0.4368],\n",
      "         [0.1956, 0.4940],\n",
      "         [0.1882, 0.4913],\n",
      "         [0.1484, 0.5038],\n",
      "         [0.1428, 0.5076],\n",
      "         [0.2102, 0.5342],\n",
      "         [0.2083, 0.5317],\n",
      "         [0.1830, 0.6133],\n",
      "         [0.1983, 0.6102],\n",
      "         [0.1993, 0.7118],\n",
      "         [0.2378, 0.6770]]])\n",
      "\n",
      "0: 640x640 1 person, 122.8ms\n",
      "Speed: 1.3ms preprocess, 122.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3420581817626953, 'inference': 122.81608581542969, 'postprocess': 0.4298686981201172}]\n",
      "Bounding Box: tensor([[ 67.0000, 199.5000,  52.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7324, 0.8135, 0.1462, 0.9426, 0.0975, 0.9970, 0.9508, 0.9894, 0.6025, 0.9554, 0.4687, 0.9983, 0.9935, 0.9974, 0.9881, 0.9887, 0.9708]])\n",
      "data: tensor([[[5.6419e+01, 1.4540e+02, 7.3238e-01],\n",
      "         [5.7771e+01, 1.4282e+02, 8.1349e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4615e-01],\n",
      "         [6.3424e+01, 1.4290e+02, 9.4256e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.7524e-02],\n",
      "         [6.9724e+01, 1.5679e+02, 9.9696e-01],\n",
      "         [6.8859e+01, 1.5687e+02, 9.5081e-01],\n",
      "         [6.9701e+01, 1.7754e+02, 9.8937e-01],\n",
      "         [6.9193e+01, 1.7575e+02, 6.0245e-01],\n",
      "         [5.3412e+01, 1.7968e+02, 9.5540e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6865e-01],\n",
      "         [7.4894e+01, 1.9281e+02, 9.9833e-01],\n",
      "         [7.5348e+01, 1.9173e+02, 9.9351e-01],\n",
      "         [6.5919e+01, 2.2108e+02, 9.9738e-01],\n",
      "         [7.2535e+01, 2.1953e+02, 9.8812e-01],\n",
      "         [7.1294e+01, 2.5624e+02, 9.8868e-01],\n",
      "         [8.6159e+01, 2.4379e+02, 9.7079e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 56.4195, 145.3954],\n",
      "         [ 57.7709, 142.8246],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.4244, 142.8957],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.7242, 156.7937],\n",
      "         [ 68.8590, 156.8696],\n",
      "         [ 69.7005, 177.5369],\n",
      "         [ 69.1926, 175.7519],\n",
      "         [ 53.4124, 179.6773],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.8942, 192.8142],\n",
      "         [ 75.3480, 191.7269],\n",
      "         [ 65.9191, 221.0785],\n",
      "         [ 72.5350, 219.5347],\n",
      "         [ 71.2936, 256.2372],\n",
      "         [ 86.1594, 243.7942]]])\n",
      "xyn: tensor([[[0.1567, 0.4039],\n",
      "         [0.1605, 0.3967],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1762, 0.3969],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1937, 0.4355],\n",
      "         [0.1913, 0.4357],\n",
      "         [0.1936, 0.4932],\n",
      "         [0.1922, 0.4882],\n",
      "         [0.1484, 0.4991],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2080, 0.5356],\n",
      "         [0.2093, 0.5326],\n",
      "         [0.1831, 0.6141],\n",
      "         [0.2015, 0.6098],\n",
      "         [0.1980, 0.7118],\n",
      "         [0.2393, 0.6772]]])\n",
      "\n",
      "0: 640x640 1 person, 114.0ms\n",
      "Speed: 1.4ms preprocess, 114.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4438629150390625, 'inference': 114.0279769897461, 'postprocess': 0.4069805145263672}]\n",
      "Bounding Box: tensor([[ 67.5000, 199.5000,  51.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7068, 0.8000, 0.1280, 0.9317, 0.0764, 0.9959, 0.9308, 0.9876, 0.5566, 0.9560, 0.4584, 0.9980, 0.9923, 0.9968, 0.9856, 0.9852, 0.9618]])\n",
      "data: tensor([[[5.7032e+01, 1.4501e+02, 7.0683e-01],\n",
      "         [5.8385e+01, 1.4236e+02, 8.0004e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2801e-01],\n",
      "         [6.4426e+01, 1.4283e+02, 9.3166e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.6445e-02],\n",
      "         [7.0237e+01, 1.5759e+02, 9.9595e-01],\n",
      "         [7.0286e+01, 1.5731e+02, 9.3076e-01],\n",
      "         [7.1029e+01, 1.7797e+02, 9.8764e-01],\n",
      "         [6.9773e+01, 1.7534e+02, 5.5660e-01],\n",
      "         [5.4499e+01, 1.7925e+02, 9.5604e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5840e-01],\n",
      "         [7.4766e+01, 1.9380e+02, 9.9805e-01],\n",
      "         [7.5979e+01, 1.9256e+02, 9.9233e-01],\n",
      "         [6.5782e+01, 2.2073e+02, 9.9680e-01],\n",
      "         [7.4298e+01, 2.1927e+02, 9.8559e-01],\n",
      "         [7.2748e+01, 2.5520e+02, 9.8525e-01],\n",
      "         [8.5085e+01, 2.4481e+02, 9.6180e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 57.0315, 145.0124],\n",
      "         [ 58.3855, 142.3625],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.4256, 142.8263],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.2374, 157.5861],\n",
      "         [ 70.2860, 157.3074],\n",
      "         [ 71.0285, 177.9675],\n",
      "         [ 69.7735, 175.3416],\n",
      "         [ 54.4992, 179.2528],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.7660, 193.8022],\n",
      "         [ 75.9795, 192.5639],\n",
      "         [ 65.7817, 220.7300],\n",
      "         [ 74.2978, 219.2695],\n",
      "         [ 72.7482, 255.2000],\n",
      "         [ 85.0846, 244.8090]]])\n",
      "xyn: tensor([[[0.1584, 0.4028],\n",
      "         [0.1622, 0.3955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1790, 0.3967],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1951, 0.4377],\n",
      "         [0.1952, 0.4370],\n",
      "         [0.1973, 0.4944],\n",
      "         [0.1938, 0.4871],\n",
      "         [0.1514, 0.4979],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2077, 0.5383],\n",
      "         [0.2111, 0.5349],\n",
      "         [0.1827, 0.6131],\n",
      "         [0.2064, 0.6091],\n",
      "         [0.2021, 0.7089],\n",
      "         [0.2363, 0.6800]]])\n",
      "\n",
      "0: 640x640 1 person, 160.1ms\n",
      "Speed: 1.2ms preprocess, 160.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[155, 171, 197],\n",
      "        [155, 171, 197],\n",
      "        [155, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2369155883789062, 'inference': 160.11786460876465, 'postprocess': 0.4191398620605469}]\n",
      "Bounding Box: tensor([[ 68.5000, 199.5000,  49.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6528, 0.7671, 0.1121, 0.9256, 0.0842, 0.9954, 0.9141, 0.9858, 0.4955, 0.9506, 0.4070, 0.9978, 0.9911, 0.9959, 0.9806, 0.9801, 0.9470]])\n",
      "data: tensor([[[5.8043e+01, 1.4451e+02, 6.5277e-01],\n",
      "         [5.9349e+01, 1.4208e+02, 7.6711e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1207e-01],\n",
      "         [6.4657e+01, 1.4260e+02, 9.2563e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.4190e-02],\n",
      "         [6.9668e+01, 1.5713e+02, 9.9536e-01],\n",
      "         [7.2221e+01, 1.5654e+02, 9.1414e-01],\n",
      "         [7.0225e+01, 1.7702e+02, 9.8577e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9550e-01],\n",
      "         [5.4405e+01, 1.7817e+02, 9.5060e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0699e-01],\n",
      "         [7.5781e+01, 1.9362e+02, 9.9784e-01],\n",
      "         [7.8861e+01, 1.9202e+02, 9.9111e-01],\n",
      "         [6.5820e+01, 2.2111e+02, 9.9589e-01],\n",
      "         [7.6219e+01, 2.1832e+02, 9.8058e-01],\n",
      "         [7.1895e+01, 2.5551e+02, 9.8011e-01],\n",
      "         [8.6911e+01, 2.4415e+02, 9.4701e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 58.0435, 144.5085],\n",
      "         [ 59.3487, 142.0754],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.6567, 142.6001],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.6677, 157.1283],\n",
      "         [ 72.2206, 156.5375],\n",
      "         [ 70.2253, 177.0155],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 54.4049, 178.1700],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.7810, 193.6180],\n",
      "         [ 78.8609, 192.0205],\n",
      "         [ 65.8201, 221.1076],\n",
      "         [ 76.2193, 218.3156],\n",
      "         [ 71.8949, 255.5096],\n",
      "         [ 86.9114, 244.1482]]])\n",
      "xyn: tensor([[[0.1612, 0.4014],\n",
      "         [0.1649, 0.3947],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1796, 0.3961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1935, 0.4365],\n",
      "         [0.2006, 0.4348],\n",
      "         [0.1951, 0.4917],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1511, 0.4949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2105, 0.5378],\n",
      "         [0.2191, 0.5334],\n",
      "         [0.1828, 0.6142],\n",
      "         [0.2117, 0.6064],\n",
      "         [0.1997, 0.7097],\n",
      "         [0.2414, 0.6782]]])\n",
      "\n",
      "0: 640x640 1 person, 112.9ms\n",
      "Speed: 1.2ms preprocess, 112.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        [157, 172, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[155, 171, 197],\n",
      "        [155, 171, 197],\n",
      "        [155, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.245260238647461, 'inference': 112.93911933898926, 'postprocess': 0.40411949157714844}]\n",
      "Bounding Box: tensor([[ 69.5000, 199.5000,  47.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6244, 0.7461, 0.0991, 0.9227, 0.0793, 0.9950, 0.9091, 0.9853, 0.4867, 0.9501, 0.4038, 0.9979, 0.9912, 0.9962, 0.9822, 0.9821, 0.9524]])\n",
      "data: tensor([[[5.8970e+01, 1.4520e+02, 6.2444e-01],\n",
      "         [6.0094e+01, 1.4266e+02, 7.4611e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.9068e-02],\n",
      "         [6.5289e+01, 1.4261e+02, 9.2273e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.9252e-02],\n",
      "         [6.9367e+01, 1.5682e+02, 9.9499e-01],\n",
      "         [7.5557e+01, 1.5637e+02, 9.0909e-01],\n",
      "         [7.0280e+01, 1.7676e+02, 9.8532e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8675e-01],\n",
      "         [5.5252e+01, 1.7797e+02, 9.5005e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0385e-01],\n",
      "         [7.5445e+01, 1.9303e+02, 9.9785e-01],\n",
      "         [8.0980e+01, 1.9137e+02, 9.9119e-01],\n",
      "         [6.5792e+01, 2.2050e+02, 9.9623e-01],\n",
      "         [7.7159e+01, 2.1751e+02, 9.8221e-01],\n",
      "         [7.1925e+01, 2.5510e+02, 9.8214e-01],\n",
      "         [8.6809e+01, 2.4400e+02, 9.5238e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 58.9704, 145.1998],\n",
      "         [ 60.0936, 142.6558],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.2894, 142.6125],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3674, 156.8235],\n",
      "         [ 75.5572, 156.3714],\n",
      "         [ 70.2797, 176.7630],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 55.2521, 177.9658],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.4452, 193.0254],\n",
      "         [ 80.9797, 191.3745],\n",
      "         [ 65.7923, 220.5026],\n",
      "         [ 77.1593, 217.5105],\n",
      "         [ 71.9254, 255.1047],\n",
      "         [ 86.8085, 244.0039]]])\n",
      "xyn: tensor([[[0.1638, 0.4033],\n",
      "         [0.1669, 0.3963],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1814, 0.3961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1927, 0.4356],\n",
      "         [0.2099, 0.4344],\n",
      "         [0.1952, 0.4910],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1535, 0.4943],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2096, 0.5362],\n",
      "         [0.2249, 0.5316],\n",
      "         [0.1828, 0.6125],\n",
      "         [0.2143, 0.6042],\n",
      "         [0.1998, 0.7086],\n",
      "         [0.2411, 0.6778]]])\n",
      "\n",
      "0: 640x640 1 person, 115.9ms\n",
      "Speed: 1.2ms preprocess, 115.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [178, 161, 144]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [178, 161, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2269020080566406, 'inference': 115.94605445861816, 'postprocess': 0.40078163146972656}]\n",
      "Bounding Box: tensor([[ 71.5000, 199.0000,  45.0000, 132.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4973, 0.6140, 0.0633, 0.9033, 0.0934, 0.9942, 0.9214, 0.9838, 0.5157, 0.9451, 0.4152, 0.9983, 0.9934, 0.9969, 0.9864, 0.9853, 0.9615]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.9730e-01],\n",
      "         [6.1295e+01, 1.4233e+02, 6.1404e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.3289e-02],\n",
      "         [6.5398e+01, 1.4284e+02, 9.0334e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.3448e-02],\n",
      "         [6.7912e+01, 1.5750e+02, 9.9416e-01],\n",
      "         [7.7262e+01, 1.5623e+02, 9.2144e-01],\n",
      "         [6.6519e+01, 1.7616e+02, 9.8382e-01],\n",
      "         [7.7105e+01, 1.7389e+02, 5.1572e-01],\n",
      "         [5.5478e+01, 1.7748e+02, 9.4507e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1521e-01],\n",
      "         [7.4597e+01, 1.9250e+02, 9.9827e-01],\n",
      "         [8.2615e+01, 1.9070e+02, 9.9342e-01],\n",
      "         [6.5721e+01, 2.2030e+02, 9.9694e-01],\n",
      "         [7.7602e+01, 2.1735e+02, 9.8644e-01],\n",
      "         [7.0060e+01, 2.5327e+02, 9.8534e-01],\n",
      "         [8.7281e+01, 2.4318e+02, 9.6147e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 61.2946, 142.3255],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.3976, 142.8410],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.9116, 157.5014],\n",
      "         [ 77.2621, 156.2285],\n",
      "         [ 66.5195, 176.1558],\n",
      "         [ 77.1050, 173.8923],\n",
      "         [ 55.4782, 177.4752],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.5966, 192.5038],\n",
      "         [ 82.6152, 190.7028],\n",
      "         [ 65.7208, 220.2996],\n",
      "         [ 77.6017, 217.3541],\n",
      "         [ 70.0597, 253.2725],\n",
      "         [ 87.2808, 243.1759]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1703, 0.3953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1817, 0.3968],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1886, 0.4375],\n",
      "         [0.2146, 0.4340],\n",
      "         [0.1848, 0.4893],\n",
      "         [0.2142, 0.4830],\n",
      "         [0.1541, 0.4930],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2072, 0.5347],\n",
      "         [0.2295, 0.5297],\n",
      "         [0.1826, 0.6119],\n",
      "         [0.2156, 0.6038],\n",
      "         [0.1946, 0.7035],\n",
      "         [0.2424, 0.6755]]])\n",
      "\n",
      "0: 640x640 1 person, 122.6ms\n",
      "Speed: 1.3ms preprocess, 122.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3003349304199219, 'inference': 122.64418601989746, 'postprocess': 0.43892860412597656}]\n",
      "Bounding Box: tensor([[ 73.5000, 199.0000,  41.0000, 132.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4685, 0.5947, 0.0534, 0.9294, 0.0904, 0.9964, 0.9364, 0.9884, 0.5393, 0.9434, 0.3760, 0.9983, 0.9930, 0.9973, 0.9870, 0.9885, 0.9681]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.6854e-01],\n",
      "         [6.1925e+01, 1.4215e+02, 5.9466e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.3440e-02],\n",
      "         [6.5811e+01, 1.4290e+02, 9.2944e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.0445e-02],\n",
      "         [6.8149e+01, 1.5701e+02, 9.9643e-01],\n",
      "         [7.9217e+01, 1.5595e+02, 9.3637e-01],\n",
      "         [6.9605e+01, 1.7771e+02, 9.8836e-01],\n",
      "         [8.0390e+01, 1.7563e+02, 5.3931e-01],\n",
      "         [5.9552e+01, 1.7856e+02, 9.4344e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7602e-01],\n",
      "         [7.5344e+01, 1.9281e+02, 9.9833e-01],\n",
      "         [8.4174e+01, 1.9135e+02, 9.9303e-01],\n",
      "         [6.5388e+01, 2.1917e+02, 9.9733e-01],\n",
      "         [7.8607e+01, 2.1777e+02, 9.8698e-01],\n",
      "         [7.1531e+01, 2.5363e+02, 9.8849e-01],\n",
      "         [8.6151e+01, 2.4311e+02, 9.6810e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 61.9253, 142.1493],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.8110, 142.9030],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.1485, 157.0145],\n",
      "         [ 79.2168, 155.9515],\n",
      "         [ 69.6050, 177.7134],\n",
      "         [ 80.3898, 175.6261],\n",
      "         [ 59.5519, 178.5571],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.3437, 192.8064],\n",
      "         [ 84.1744, 191.3461],\n",
      "         [ 65.3884, 219.1655],\n",
      "         [ 78.6071, 217.7651],\n",
      "         [ 71.5311, 253.6259],\n",
      "         [ 86.1507, 243.1058]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1720, 0.3949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1828, 0.3970],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1893, 0.4362],\n",
      "         [0.2200, 0.4332],\n",
      "         [0.1933, 0.4936],\n",
      "         [0.2233, 0.4879],\n",
      "         [0.1654, 0.4960],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2093, 0.5356],\n",
      "         [0.2338, 0.5315],\n",
      "         [0.1816, 0.6088],\n",
      "         [0.2184, 0.6049],\n",
      "         [0.1987, 0.7045],\n",
      "         [0.2393, 0.6753]]])\n",
      "\n",
      "0: 640x640 1 person, 113.4ms\n",
      "Speed: 1.2ms preprocess, 113.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2469291687011719, 'inference': 113.44599723815918, 'postprocess': 0.431060791015625}]\n",
      "Bounding Box: tensor([[ 74., 198.,  40., 132.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3716, 0.4888, 0.0409, 0.9324, 0.1224, 0.9970, 0.9506, 0.9877, 0.5666, 0.9285, 0.3521, 0.9988, 0.9953, 0.9980, 0.9910, 0.9920, 0.9784]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.7161e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8880e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0866e-02],\n",
      "         [6.5840e+01, 1.4291e+02, 9.3239e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2242e-01],\n",
      "         [6.7723e+01, 1.5753e+02, 9.9701e-01],\n",
      "         [8.1528e+01, 1.5505e+02, 9.5063e-01],\n",
      "         [6.7281e+01, 1.7759e+02, 9.8775e-01],\n",
      "         [8.6675e+01, 1.7346e+02, 5.6655e-01],\n",
      "         [5.8898e+01, 1.7672e+02, 9.2852e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5206e-01],\n",
      "         [7.5939e+01, 1.9241e+02, 9.9881e-01],\n",
      "         [8.6164e+01, 1.9066e+02, 9.9534e-01],\n",
      "         [6.5464e+01, 2.1791e+02, 9.9802e-01],\n",
      "         [7.9629e+01, 2.1722e+02, 9.9100e-01],\n",
      "         [7.0562e+01, 2.5227e+02, 9.9200e-01],\n",
      "         [8.6873e+01, 2.4261e+02, 9.7843e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.8403, 142.9141],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.7232, 157.5268],\n",
      "         [ 81.5277, 155.0540],\n",
      "         [ 67.2814, 177.5850],\n",
      "         [ 86.6755, 173.4605],\n",
      "         [ 58.8983, 176.7195],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.9393, 192.4119],\n",
      "         [ 86.1638, 190.6619],\n",
      "         [ 65.4637, 217.9067],\n",
      "         [ 79.6290, 217.2157],\n",
      "         [ 70.5625, 252.2748],\n",
      "         [ 86.8733, 242.6130]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1829, 0.3970],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1881, 0.4376],\n",
      "         [0.2265, 0.4307],\n",
      "         [0.1869, 0.4933],\n",
      "         [0.2408, 0.4818],\n",
      "         [0.1636, 0.4909],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2109, 0.5345],\n",
      "         [0.2393, 0.5296],\n",
      "         [0.1818, 0.6053],\n",
      "         [0.2212, 0.6034],\n",
      "         [0.1960, 0.7008],\n",
      "         [0.2413, 0.6739]]])\n",
      "\n",
      "0: 640x640 1 person, 118.9ms\n",
      "Speed: 1.3ms preprocess, 118.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3000965118408203, 'inference': 118.94607543945312, 'postprocess': 0.41413307189941406}]\n",
      "Bounding Box: tensor([[ 74.0000, 198.5000,  40.0000, 131.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3820, 0.5036, 0.0412, 0.9288, 0.1043, 0.9964, 0.9450, 0.9861, 0.5504, 0.9238, 0.3453, 0.9985, 0.9943, 0.9975, 0.9891, 0.9901, 0.9738]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.8196e-01],\n",
      "         [6.3366e+01, 1.4209e+02, 5.0364e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1249e-02],\n",
      "         [6.6738e+01, 1.4288e+02, 9.2878e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0429e-01],\n",
      "         [6.8446e+01, 1.5637e+02, 9.9640e-01],\n",
      "         [8.1666e+01, 1.5507e+02, 9.4500e-01],\n",
      "         [6.7813e+01, 1.7621e+02, 9.8612e-01],\n",
      "         [8.3007e+01, 1.7346e+02, 5.5042e-01],\n",
      "         [5.9855e+01, 1.7574e+02, 9.2381e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4532e-01],\n",
      "         [7.6249e+01, 1.9198e+02, 9.9849e-01],\n",
      "         [8.6445e+01, 1.9072e+02, 9.9426e-01],\n",
      "         [6.5244e+01, 2.1731e+02, 9.9755e-01],\n",
      "         [8.0225e+01, 2.1755e+02, 9.8913e-01],\n",
      "         [6.9521e+01, 2.5114e+02, 9.9008e-01],\n",
      "         [8.6808e+01, 2.4296e+02, 9.7379e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [ 63.3658, 142.0882],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.7377, 142.8825],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.4459, 156.3694],\n",
      "         [ 81.6657, 155.0742],\n",
      "         [ 67.8134, 176.2129],\n",
      "         [ 83.0066, 173.4642],\n",
      "         [ 59.8551, 175.7376],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.2491, 191.9758],\n",
      "         [ 86.4450, 190.7185],\n",
      "         [ 65.2443, 217.3102],\n",
      "         [ 80.2253, 217.5534],\n",
      "         [ 69.5213, 251.1447],\n",
      "         [ 86.8080, 242.9648]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.1760, 0.3947],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1854, 0.3969],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1901, 0.4344],\n",
      "         [0.2268, 0.4308],\n",
      "         [0.1884, 0.4895],\n",
      "         [0.2306, 0.4818],\n",
      "         [0.1663, 0.4882],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2118, 0.5333],\n",
      "         [0.2401, 0.5298],\n",
      "         [0.1812, 0.6036],\n",
      "         [0.2228, 0.6043],\n",
      "         [0.1931, 0.6976],\n",
      "         [0.2411, 0.6749]]])\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 1.2ms preprocess, 116.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        [153, 173, 198],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2221336364746094, 'inference': 116.75524711608887, 'postprocess': 0.41174888610839844}]\n",
      "Bounding Box: tensor([[ 75.5000, 196.5000,  39.0000, 129.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2824, 0.3919, 0.0351, 0.9036, 0.1100, 0.9920, 0.9277, 0.9618, 0.4816, 0.8169, 0.2622, 0.9971, 0.9918, 0.9963, 0.9882, 0.9888, 0.9761]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8239e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9191e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5133e-02],\n",
      "         [6.7554e+01, 1.4342e+02, 9.0364e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0996e-01],\n",
      "         [6.8156e+01, 1.5633e+02, 9.9200e-01],\n",
      "         [8.4107e+01, 1.5457e+02, 9.2773e-01],\n",
      "         [6.6252e+01, 1.7619e+02, 9.6176e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8163e-01],\n",
      "         [6.0944e+01, 1.7544e+02, 8.1693e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6219e-01],\n",
      "         [7.6280e+01, 1.8981e+02, 9.9707e-01],\n",
      "         [8.8356e+01, 1.8869e+02, 9.9180e-01],\n",
      "         [6.5586e+01, 2.1505e+02, 9.9628e-01],\n",
      "         [8.2264e+01, 2.1671e+02, 9.8821e-01],\n",
      "         [6.8936e+01, 2.4766e+02, 9.8882e-01],\n",
      "         [8.7131e+01, 2.4337e+02, 9.7607e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5539, 143.4247],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.1558, 156.3302],\n",
      "         [ 84.1075, 154.5712],\n",
      "         [ 66.2521, 176.1912],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 60.9438, 175.4418],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.2799, 189.8068],\n",
      "         [ 88.3563, 188.6899],\n",
      "         [ 65.5861, 215.0455],\n",
      "         [ 82.2643, 216.7062],\n",
      "         [ 68.9365, 247.6567],\n",
      "         [ 87.1309, 243.3743]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1876, 0.3984],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1893, 0.4343],\n",
      "         [0.2336, 0.4294],\n",
      "         [0.1840, 0.4894],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1693, 0.4873],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2119, 0.5272],\n",
      "         [0.2454, 0.5241],\n",
      "         [0.1822, 0.5973],\n",
      "         [0.2285, 0.6020],\n",
      "         [0.1915, 0.6879],\n",
      "         [0.2420, 0.6760]]])\n",
      "\n",
      "0: 640x640 1 person, 113.9ms\n",
      "Speed: 1.3ms preprocess, 113.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2822151184082031, 'inference': 113.88587951660156, 'postprocess': 0.40984153747558594}]\n",
      "Bounding Box: tensor([[ 75.5000, 195.0000,  39.0000, 126.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2895, 0.3921, 0.0399, 0.9163, 0.1465, 0.9930, 0.9473, 0.9567, 0.5248, 0.7805, 0.2640, 0.9974, 0.9936, 0.9966, 0.9905, 0.9905, 0.9813]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8945e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9205e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9934e-02],\n",
      "         [6.7845e+01, 1.4362e+02, 9.1630e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4650e-01],\n",
      "         [6.8048e+01, 1.5642e+02, 9.9299e-01],\n",
      "         [8.5556e+01, 1.5491e+02, 9.4735e-01],\n",
      "         [6.6101e+01, 1.7586e+02, 9.5670e-01],\n",
      "         [8.9982e+01, 1.7440e+02, 5.2477e-01],\n",
      "         [6.1051e+01, 1.7447e+02, 7.8053e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6399e-01],\n",
      "         [7.5823e+01, 1.9086e+02, 9.9742e-01],\n",
      "         [8.8637e+01, 1.8990e+02, 9.9358e-01],\n",
      "         [6.5093e+01, 2.1462e+02, 9.9657e-01],\n",
      "         [8.3328e+01, 2.1679e+02, 9.9046e-01],\n",
      "         [6.8781e+01, 2.4632e+02, 9.9054e-01],\n",
      "         [8.6963e+01, 2.4279e+02, 9.8132e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.8454, 143.6225],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.0482, 156.4193],\n",
      "         [ 85.5559, 154.9089],\n",
      "         [ 66.1009, 175.8629],\n",
      "         [ 89.9818, 174.4021],\n",
      "         [ 61.0510, 174.4679],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.8227, 190.8637],\n",
      "         [ 88.6370, 189.9031],\n",
      "         [ 65.0925, 214.6178],\n",
      "         [ 83.3284, 216.7868],\n",
      "         [ 68.7812, 246.3176],\n",
      "         [ 86.9629, 242.7926]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1885, 0.3990],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1890, 0.4345],\n",
      "         [0.2377, 0.4303],\n",
      "         [0.1836, 0.4885],\n",
      "         [0.2499, 0.4845],\n",
      "         [0.1696, 0.4846],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2106, 0.5302],\n",
      "         [0.2462, 0.5275],\n",
      "         [0.1808, 0.5962],\n",
      "         [0.2315, 0.6022],\n",
      "         [0.1911, 0.6842],\n",
      "         [0.2416, 0.6744]]])\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2421607971191406, 'inference': 113.77406120300293, 'postprocess': 0.39196014404296875}]\n",
      "Bounding Box: tensor([[ 77.5000, 192.5000,  37.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2075, 0.2864, 0.0367, 0.8961, 0.1826, 0.9857, 0.9448, 0.8777, 0.4964, 0.5355, 0.2014, 0.9957, 0.9927, 0.9950, 0.9909, 0.9902, 0.9853]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.0755e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8639e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6725e-02],\n",
      "         [6.7861e+01, 1.4360e+02, 8.9607e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8261e-01],\n",
      "         [6.7422e+01, 1.5663e+02, 9.8567e-01],\n",
      "         [8.6445e+01, 1.5505e+02, 9.4484e-01],\n",
      "         [6.5944e+01, 1.7584e+02, 8.7767e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9643e-01],\n",
      "         [6.5385e+01, 1.7902e+02, 5.3549e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0141e-01],\n",
      "         [7.4892e+01, 1.9018e+02, 9.9567e-01],\n",
      "         [8.8665e+01, 1.8951e+02, 9.9265e-01],\n",
      "         [6.5640e+01, 2.1271e+02, 9.9504e-01],\n",
      "         [8.3521e+01, 2.1597e+02, 9.9091e-01],\n",
      "         [6.9834e+01, 2.4275e+02, 9.9022e-01],\n",
      "         [8.7090e+01, 2.4282e+02, 9.8534e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.8610, 143.5998],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.4218, 156.6341],\n",
      "         [ 86.4446, 155.0521],\n",
      "         [ 65.9438, 175.8365],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.3851, 179.0159],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.8920, 190.1789],\n",
      "         [ 88.6648, 189.5069],\n",
      "         [ 65.6404, 212.7119],\n",
      "         [ 83.5209, 215.9658],\n",
      "         [ 69.8344, 242.7510],\n",
      "         [ 87.0903, 242.8211]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1885, 0.3989],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1873, 0.4351],\n",
      "         [0.2401, 0.4307],\n",
      "         [0.1832, 0.4884],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1816, 0.4973],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2080, 0.5283],\n",
      "         [0.2463, 0.5264],\n",
      "         [0.1823, 0.5909],\n",
      "         [0.2320, 0.5999],\n",
      "         [0.1940, 0.6743],\n",
      "         [0.2419, 0.6745]]])\n",
      "\n",
      "0: 640x640 1 person, 118.3ms\n",
      "Speed: 1.2ms preprocess, 118.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2202262878417969, 'inference': 118.25203895568848, 'postprocess': 0.41675567626953125}]\n",
      "Bounding Box: tensor([[ 78.5000, 192.0000,  39.0000, 120.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2328, 0.2649, 0.0514, 0.8852, 0.2845, 0.9860, 0.9721, 0.8542, 0.6739, 0.4909, 0.3029, 0.9961, 0.9953, 0.9947, 0.9930, 0.9891, 0.9871]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3279e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6485e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.1440e-02],\n",
      "         [6.7762e+01, 1.4401e+02, 8.8520e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8455e-01],\n",
      "         [6.7019e+01, 1.5613e+02, 9.8603e-01],\n",
      "         [8.7230e+01, 1.5528e+02, 9.7206e-01],\n",
      "         [6.7658e+01, 1.7527e+02, 8.5424e-01],\n",
      "         [9.5025e+01, 1.7310e+02, 6.7392e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9091e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0290e-01],\n",
      "         [7.5206e+01, 1.9039e+02, 9.9612e-01],\n",
      "         [8.9631e+01, 1.8993e+02, 9.9531e-01],\n",
      "         [6.6291e+01, 2.1259e+02, 9.9468e-01],\n",
      "         [8.3499e+01, 2.1539e+02, 9.9303e-01],\n",
      "         [7.0849e+01, 2.4252e+02, 9.8909e-01],\n",
      "         [8.6573e+01, 2.4176e+02, 9.8715e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.7624, 144.0088],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.0193, 156.1343],\n",
      "         [ 87.2303, 155.2782],\n",
      "         [ 67.6579, 175.2667],\n",
      "         [ 95.0247, 173.1044],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.2058, 190.3929],\n",
      "         [ 89.6307, 189.9292],\n",
      "         [ 66.2906, 212.5867],\n",
      "         [ 83.4986, 215.3929],\n",
      "         [ 70.8491, 242.5233],\n",
      "         [ 86.5731, 241.7586]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1882, 0.4000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1862, 0.4337],\n",
      "         [0.2423, 0.4313],\n",
      "         [0.1879, 0.4869],\n",
      "         [0.2640, 0.4808],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2089, 0.5289],\n",
      "         [0.2490, 0.5276],\n",
      "         [0.1841, 0.5905],\n",
      "         [0.2319, 0.5983],\n",
      "         [0.1968, 0.6737],\n",
      "         [0.2405, 0.6716]]])\n",
      "\n",
      "0: 640x640 1 person, 129.2ms\n",
      "Speed: 1.2ms preprocess, 129.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.239776611328125, 'inference': 129.23216819763184, 'postprocess': 0.6628036499023438}]\n",
      "Bounding Box: tensor([[ 80.5000, 191.5000,  39.0000, 117.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3104, 0.2438, 0.0579, 0.8257, 0.3104, 0.9946, 0.9854, 0.9749, 0.9016, 0.8981, 0.7464, 0.9987, 0.9980, 0.9963, 0.9937, 0.9842, 0.9774]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.1038e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4382e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.7891e-02],\n",
      "         [6.8389e+01, 1.4414e+02, 8.2569e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1043e-01],\n",
      "         [6.8129e+01, 1.5652e+02, 9.9463e-01],\n",
      "         [8.7388e+01, 1.5535e+02, 9.8541e-01],\n",
      "         [6.9191e+01, 1.7608e+02, 9.7486e-01],\n",
      "         [9.7002e+01, 1.7250e+02, 9.0161e-01],\n",
      "         [7.0118e+01, 1.9338e+02, 8.9812e-01],\n",
      "         [8.7982e+01, 1.8295e+02, 7.4643e-01],\n",
      "         [7.6508e+01, 1.9000e+02, 9.9869e-01],\n",
      "         [9.0125e+01, 1.8950e+02, 9.9796e-01],\n",
      "         [6.9684e+01, 2.1250e+02, 9.9627e-01],\n",
      "         [8.4798e+01, 2.1497e+02, 9.9371e-01],\n",
      "         [7.3779e+01, 2.3998e+02, 9.8418e-01],\n",
      "         [8.6670e+01, 2.4171e+02, 9.7742e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.3894, 144.1411],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.1289, 156.5210],\n",
      "         [ 87.3881, 155.3481],\n",
      "         [ 69.1908, 176.0787],\n",
      "         [ 97.0016, 172.5007],\n",
      "         [ 70.1179, 193.3839],\n",
      "         [ 87.9825, 182.9530],\n",
      "         [ 76.5078, 189.9980],\n",
      "         [ 90.1251, 189.5018],\n",
      "         [ 69.6843, 212.5027],\n",
      "         [ 84.7979, 214.9736],\n",
      "         [ 73.7788, 239.9767],\n",
      "         [ 86.6702, 241.7093]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1900, 0.4004],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1892, 0.4348],\n",
      "         [0.2427, 0.4315],\n",
      "         [0.1922, 0.4891],\n",
      "         [0.2694, 0.4792],\n",
      "         [0.1948, 0.5372],\n",
      "         [0.2444, 0.5082],\n",
      "         [0.2125, 0.5278],\n",
      "         [0.2503, 0.5264],\n",
      "         [0.1936, 0.5903],\n",
      "         [0.2355, 0.5971],\n",
      "         [0.2049, 0.6666],\n",
      "         [0.2408, 0.6714]]])\n",
      "\n",
      "0: 640x640 1 person, 111.8ms\n",
      "Speed: 1.2ms preprocess, 111.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2469291687011719, 'inference': 111.7868423461914, 'postprocess': 0.4029273986816406}]\n",
      "Bounding Box: tensor([[ 81.5000, 191.5000,  41.0000, 119.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2883, 0.2110, 0.0917, 0.7964, 0.4140, 0.9780, 0.9888, 0.7869, 0.9012, 0.4730, 0.6448, 0.9955, 0.9973, 0.9915, 0.9946, 0.9802, 0.9860]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8828e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1102e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.1694e-02],\n",
      "         [6.9097e+01, 1.4375e+02, 7.9636e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1402e-01],\n",
      "         [6.7553e+01, 1.5614e+02, 9.7796e-01],\n",
      "         [8.8881e+01, 1.5550e+02, 9.8880e-01],\n",
      "         [6.7094e+01, 1.7321e+02, 7.8695e-01],\n",
      "         [9.8854e+01, 1.7201e+02, 9.0116e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7299e-01],\n",
      "         [9.2145e+01, 1.8213e+02, 6.4478e-01],\n",
      "         [7.4163e+01, 1.9014e+02, 9.9550e-01],\n",
      "         [8.9009e+01, 1.8981e+02, 9.9726e-01],\n",
      "         [6.8953e+01, 2.1288e+02, 9.9152e-01],\n",
      "         [8.4542e+01, 2.1574e+02, 9.9465e-01],\n",
      "         [7.3136e+01, 2.4011e+02, 9.8021e-01],\n",
      "         [8.6781e+01, 2.4227e+02, 9.8595e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0968, 143.7538],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5529, 156.1365],\n",
      "         [ 88.8814, 155.4972],\n",
      "         [ 67.0945, 173.2121],\n",
      "         [ 98.8537, 172.0132],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 92.1453, 182.1289],\n",
      "         [ 74.1632, 190.1415],\n",
      "         [ 89.0089, 189.8141],\n",
      "         [ 68.9526, 212.8834],\n",
      "         [ 84.5420, 215.7404],\n",
      "         [ 73.1356, 240.1131],\n",
      "         [ 86.7806, 242.2738]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1919, 0.3993],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1876, 0.4337],\n",
      "         [0.2469, 0.4319],\n",
      "         [0.1864, 0.4811],\n",
      "         [0.2746, 0.4778],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2560, 0.5059],\n",
      "         [0.2060, 0.5282],\n",
      "         [0.2472, 0.5273],\n",
      "         [0.1915, 0.5913],\n",
      "         [0.2348, 0.5993],\n",
      "         [0.2032, 0.6670],\n",
      "         [0.2411, 0.6730]]])\n",
      "\n",
      "0: 640x640 1 person, 113.7ms\n",
      "Speed: 1.3ms preprocess, 113.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2531280517578125, 'inference': 113.74783515930176, 'postprocess': 0.42176246643066406}]\n",
      "Bounding Box: tensor([[ 82.5000, 191.0000,  43.0000, 118.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2991, 0.2126, 0.0804, 0.8114, 0.4250, 0.9886, 0.9903, 0.9094, 0.9220, 0.6976, 0.7275, 0.9975, 0.9979, 0.9937, 0.9946, 0.9806, 0.9826]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.9905e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1263e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.0395e-02],\n",
      "         [6.9028e+01, 1.4354e+02, 8.1142e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2496e-01],\n",
      "         [6.7692e+01, 1.5615e+02, 9.8859e-01],\n",
      "         [8.9674e+01, 1.5481e+02, 9.9031e-01],\n",
      "         [6.7041e+01, 1.7389e+02, 9.0940e-01],\n",
      "         [1.0017e+02, 1.7134e+02, 9.2204e-01],\n",
      "         [6.8627e+01, 1.8464e+02, 6.9756e-01],\n",
      "         [9.2742e+01, 1.8236e+02, 7.2746e-01],\n",
      "         [7.5487e+01, 1.9016e+02, 9.9745e-01],\n",
      "         [9.0478e+01, 1.8966e+02, 9.9792e-01],\n",
      "         [6.9810e+01, 2.1197e+02, 9.9373e-01],\n",
      "         [8.4767e+01, 2.1521e+02, 9.9458e-01],\n",
      "         [7.5109e+01, 2.3859e+02, 9.8058e-01],\n",
      "         [8.6105e+01, 2.4209e+02, 9.8258e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0276, 143.5390],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.6917, 156.1478],\n",
      "         [ 89.6737, 154.8092],\n",
      "         [ 67.0406, 173.8937],\n",
      "         [100.1711, 171.3365],\n",
      "         [ 68.6270, 184.6379],\n",
      "         [ 92.7421, 182.3630],\n",
      "         [ 75.4868, 190.1608],\n",
      "         [ 90.4780, 189.6590],\n",
      "         [ 69.8104, 211.9698],\n",
      "         [ 84.7671, 215.2095],\n",
      "         [ 75.1088, 238.5928],\n",
      "         [ 86.1053, 242.0882]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1917, 0.3987],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1880, 0.4337],\n",
      "         [0.2491, 0.4300],\n",
      "         [0.1862, 0.4830],\n",
      "         [0.2783, 0.4759],\n",
      "         [0.1906, 0.5129],\n",
      "         [0.2576, 0.5066],\n",
      "         [0.2097, 0.5282],\n",
      "         [0.2513, 0.5268],\n",
      "         [0.1939, 0.5888],\n",
      "         [0.2355, 0.5978],\n",
      "         [0.2086, 0.6628],\n",
      "         [0.2392, 0.6725]]])\n",
      "\n",
      "0: 640x640 1 person, 112.2ms\n",
      "Speed: 1.3ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        [152, 172, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        [151, 171, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2738704681396484, 'inference': 112.1680736541748, 'postprocess': 0.4222393035888672}]\n",
      "Bounding Box: tensor([[ 82.5000, 191.0000,  43.0000, 118.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2637, 0.1797, 0.0772, 0.7866, 0.4265, 0.9832, 0.9901, 0.8577, 0.9231, 0.5895, 0.7129, 0.9965, 0.9977, 0.9918, 0.9943, 0.9774, 0.9825]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.6366e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7975e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.7152e-02],\n",
      "         [6.9002e+01, 1.4380e+02, 7.8658e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2648e-01],\n",
      "         [6.7520e+01, 1.5574e+02, 9.8320e-01],\n",
      "         [8.9960e+01, 1.5472e+02, 9.9008e-01],\n",
      "         [6.6821e+01, 1.7250e+02, 8.5766e-01],\n",
      "         [1.0010e+02, 1.7138e+02, 9.2312e-01],\n",
      "         [7.0022e+01, 1.8395e+02, 5.8947e-01],\n",
      "         [9.4651e+01, 1.8469e+02, 7.1287e-01],\n",
      "         [7.5348e+01, 1.8960e+02, 9.9653e-01],\n",
      "         [9.0496e+01, 1.8933e+02, 9.9768e-01],\n",
      "         [7.0547e+01, 2.1097e+02, 9.9177e-01],\n",
      "         [8.4999e+01, 2.1482e+02, 9.9428e-01],\n",
      "         [7.5922e+01, 2.3735e+02, 9.7739e-01],\n",
      "         [8.4955e+01, 2.4186e+02, 9.8253e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0022, 143.8018],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5198, 155.7365],\n",
      "         [ 89.9598, 154.7199],\n",
      "         [ 66.8209, 172.4993],\n",
      "         [100.0969, 171.3793],\n",
      "         [ 70.0217, 183.9529],\n",
      "         [ 94.6506, 184.6930],\n",
      "         [ 75.3482, 189.6027],\n",
      "         [ 90.4957, 189.3349],\n",
      "         [ 70.5471, 210.9666],\n",
      "         [ 84.9993, 214.8176],\n",
      "         [ 75.9218, 237.3460],\n",
      "         [ 84.9551, 241.8592]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1917, 0.3994],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1876, 0.4326],\n",
      "         [0.2499, 0.4298],\n",
      "         [0.1856, 0.4792],\n",
      "         [0.2780, 0.4761],\n",
      "         [0.1945, 0.5110],\n",
      "         [0.2629, 0.5130],\n",
      "         [0.2093, 0.5267],\n",
      "         [0.2514, 0.5259],\n",
      "         [0.1960, 0.5860],\n",
      "         [0.2361, 0.5967],\n",
      "         [0.2109, 0.6593],\n",
      "         [0.2360, 0.6718]]])\n",
      "\n",
      "0: 640x640 1 person, 125.0ms\n",
      "Speed: 1.2ms preprocess, 125.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [150, 166, 193],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [178, 161, 144]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [178, 161, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.1959075927734375, 'inference': 125.00119209289551, 'postprocess': 0.4191398620605469}]\n",
      "Bounding Box: tensor([[ 82.5000, 191.5000,  41.0000, 117.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2496, 0.1690, 0.0689, 0.7793, 0.4045, 0.9852, 0.9895, 0.8841, 0.9219, 0.6434, 0.7176, 0.9972, 0.9979, 0.9931, 0.9946, 0.9801, 0.9834]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4962e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6904e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.8914e-02],\n",
      "         [6.9313e+01, 1.4382e+02, 7.7934e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0451e-01],\n",
      "         [6.8095e+01, 1.5600e+02, 9.8524e-01],\n",
      "         [8.9241e+01, 1.5463e+02, 9.8949e-01],\n",
      "         [6.6767e+01, 1.7204e+02, 8.8411e-01],\n",
      "         [9.8820e+01, 1.7030e+02, 9.2193e-01],\n",
      "         [7.0916e+01, 1.8336e+02, 6.4336e-01],\n",
      "         [9.6703e+01, 1.8494e+02, 7.1764e-01],\n",
      "         [7.5730e+01, 1.8877e+02, 9.9717e-01],\n",
      "         [9.0138e+01, 1.8843e+02, 9.9788e-01],\n",
      "         [7.1880e+01, 2.1113e+02, 9.9311e-01],\n",
      "         [8.5748e+01, 2.1448e+02, 9.9464e-01],\n",
      "         [7.5753e+01, 2.3725e+02, 9.8014e-01],\n",
      "         [8.5984e+01, 2.4118e+02, 9.8341e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.3127, 143.8217],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.0952, 156.0007],\n",
      "         [ 89.2414, 154.6275],\n",
      "         [ 66.7673, 172.0410],\n",
      "         [ 98.8197, 170.2983],\n",
      "         [ 70.9157, 183.3647],\n",
      "         [ 96.7034, 184.9378],\n",
      "         [ 75.7304, 188.7675],\n",
      "         [ 90.1381, 188.4276],\n",
      "         [ 71.8797, 211.1329],\n",
      "         [ 85.7476, 214.4789],\n",
      "         [ 75.7526, 237.2543],\n",
      "         [ 85.9841, 241.1764]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1925, 0.3995],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1892, 0.4333],\n",
      "         [0.2479, 0.4295],\n",
      "         [0.1855, 0.4779],\n",
      "         [0.2745, 0.4731],\n",
      "         [0.1970, 0.5093],\n",
      "         [0.2686, 0.5137],\n",
      "         [0.2104, 0.5244],\n",
      "         [0.2504, 0.5234],\n",
      "         [0.1997, 0.5865],\n",
      "         [0.2382, 0.5958],\n",
      "         [0.2104, 0.6590],\n",
      "         [0.2388, 0.6699]]])\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 1.3ms preprocess, 113.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.28173828125, 'inference': 113.56902122497559, 'postprocess': 0.4050731658935547}]\n",
      "Bounding Box: tensor([[ 80., 191.,  38., 116.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2535, 0.2557, 0.0583, 0.8654, 0.3073, 0.9867, 0.9731, 0.8909, 0.7525, 0.6135, 0.4261, 0.9958, 0.9947, 0.9919, 0.9895, 0.9808, 0.9769]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5350e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5569e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.8268e-02],\n",
      "         [6.9185e+01, 1.4366e+02, 8.6542e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0726e-01],\n",
      "         [6.7968e+01, 1.5612e+02, 9.8675e-01],\n",
      "         [8.8903e+01, 1.5468e+02, 9.7309e-01],\n",
      "         [6.5996e+01, 1.7204e+02, 8.9088e-01],\n",
      "         [9.6561e+01, 1.7075e+02, 7.5253e-01],\n",
      "         [7.1303e+01, 1.8252e+02, 6.1355e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2615e-01],\n",
      "         [7.6163e+01, 1.8890e+02, 9.9576e-01],\n",
      "         [9.0522e+01, 1.8877e+02, 9.9472e-01],\n",
      "         [7.2198e+01, 2.0992e+02, 9.9186e-01],\n",
      "         [8.6125e+01, 2.1444e+02, 9.8954e-01],\n",
      "         [7.5796e+01, 2.3487e+02, 9.8077e-01],\n",
      "         [8.5974e+01, 2.4069e+02, 9.7687e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.1852, 143.6561],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.9684, 156.1185],\n",
      "         [ 88.9027, 154.6836],\n",
      "         [ 65.9956, 172.0361],\n",
      "         [ 96.5608, 170.7505],\n",
      "         [ 71.3035, 182.5213],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.1625, 188.8995],\n",
      "         [ 90.5216, 188.7735],\n",
      "         [ 72.1984, 209.9196],\n",
      "         [ 86.1249, 214.4417],\n",
      "         [ 75.7964, 234.8660],\n",
      "         [ 85.9736, 240.6858]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1922, 0.3990],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1888, 0.4337],\n",
      "         [0.2470, 0.4297],\n",
      "         [0.1833, 0.4779],\n",
      "         [0.2682, 0.4743],\n",
      "         [0.1981, 0.5070],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2116, 0.5247],\n",
      "         [0.2514, 0.5244],\n",
      "         [0.2006, 0.5831],\n",
      "         [0.2392, 0.5957],\n",
      "         [0.2105, 0.6524],\n",
      "         [0.2388, 0.6686]]])\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.3ms preprocess, 114.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3289451599121094, 'inference': 114.10713195800781, 'postprocess': 0.4277229309082031}]\n",
      "Bounding Box: tensor([[ 81., 191.,  40., 116.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2714, 0.2524, 0.0635, 0.8521, 0.3230, 0.9880, 0.9773, 0.9078, 0.8052, 0.6719, 0.5101, 0.9965, 0.9958, 0.9930, 0.9913, 0.9825, 0.9792]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.7137e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5245e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.3546e-02],\n",
      "         [6.9469e+01, 1.4383e+02, 8.5212e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2296e-01],\n",
      "         [6.7990e+01, 1.5650e+02, 9.8796e-01],\n",
      "         [8.9255e+01, 1.5470e+02, 9.7729e-01],\n",
      "         [6.6319e+01, 1.7334e+02, 9.0775e-01],\n",
      "         [9.7650e+01, 1.7093e+02, 8.0516e-01],\n",
      "         [7.2762e+01, 1.8406e+02, 6.7187e-01],\n",
      "         [9.7030e+01, 1.8593e+02, 5.1009e-01],\n",
      "         [7.6157e+01, 1.8912e+02, 9.9655e-01],\n",
      "         [9.0660e+01, 1.8893e+02, 9.9582e-01],\n",
      "         [7.2536e+01, 2.1012e+02, 9.9304e-01],\n",
      "         [8.6025e+01, 2.1456e+02, 9.9128e-01],\n",
      "         [7.6632e+01, 2.3449e+02, 9.8247e-01],\n",
      "         [8.5580e+01, 2.4087e+02, 9.7922e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.4694, 143.8266],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.9902, 156.4955],\n",
      "         [ 89.2553, 154.6974],\n",
      "         [ 66.3192, 173.3431],\n",
      "         [ 97.6502, 170.9250],\n",
      "         [ 72.7619, 184.0642],\n",
      "         [ 97.0300, 185.9281],\n",
      "         [ 76.1568, 189.1204],\n",
      "         [ 90.6599, 188.9294],\n",
      "         [ 72.5360, 210.1155],\n",
      "         [ 86.0250, 214.5556],\n",
      "         [ 76.6321, 234.4858],\n",
      "         [ 85.5799, 240.8677]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1930, 0.3995],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1889, 0.4347],\n",
      "         [0.2479, 0.4297],\n",
      "         [0.1842, 0.4815],\n",
      "         [0.2713, 0.4748],\n",
      "         [0.2021, 0.5113],\n",
      "         [0.2695, 0.5165],\n",
      "         [0.2115, 0.5253],\n",
      "         [0.2518, 0.5248],\n",
      "         [0.2015, 0.5837],\n",
      "         [0.2390, 0.5960],\n",
      "         [0.2129, 0.6513],\n",
      "         [0.2377, 0.6691]]])\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.3ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.332998275756836, 'inference': 113.79313468933105, 'postprocess': 0.4200935363769531}]\n",
      "Bounding Box: tensor([[ 79.5000, 191.5000,  39.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1970, 0.2146, 0.0452, 0.8469, 0.2914, 0.9845, 0.9592, 0.8827, 0.6658, 0.5733, 0.3307, 0.9955, 0.9937, 0.9925, 0.9890, 0.9845, 0.9792]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.9704e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1456e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5241e-02],\n",
      "         [6.9000e+01, 1.4474e+02, 8.4689e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9137e-01],\n",
      "         [6.7839e+01, 1.5641e+02, 9.8454e-01],\n",
      "         [8.8900e+01, 1.5455e+02, 9.5925e-01],\n",
      "         [6.6365e+01, 1.7302e+02, 8.8266e-01],\n",
      "         [9.6721e+01, 1.7136e+02, 6.6583e-01],\n",
      "         [7.2934e+01, 1.8225e+02, 5.7331e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3065e-01],\n",
      "         [7.7401e+01, 1.8901e+02, 9.9549e-01],\n",
      "         [9.1589e+01, 1.8897e+02, 9.9366e-01],\n",
      "         [7.4314e+01, 2.0915e+02, 9.9246e-01],\n",
      "         [8.6115e+01, 2.1441e+02, 9.8904e-01],\n",
      "         [7.7586e+01, 2.3280e+02, 9.8450e-01],\n",
      "         [8.4977e+01, 2.4132e+02, 9.7922e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0000, 144.7403],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.8389, 156.4079],\n",
      "         [ 88.8996, 154.5514],\n",
      "         [ 66.3650, 173.0223],\n",
      "         [ 96.7206, 171.3628],\n",
      "         [ 72.9338, 182.2529],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.4005, 189.0130],\n",
      "         [ 91.5891, 188.9685],\n",
      "         [ 74.3142, 209.1530],\n",
      "         [ 86.1147, 214.4059],\n",
      "         [ 77.5861, 232.8029],\n",
      "         [ 84.9769, 241.3243]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1917, 0.4021],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1884, 0.4345],\n",
      "         [0.2469, 0.4293],\n",
      "         [0.1843, 0.4806],\n",
      "         [0.2687, 0.4760],\n",
      "         [0.2026, 0.5063],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2150, 0.5250],\n",
      "         [0.2544, 0.5249],\n",
      "         [0.2064, 0.5810],\n",
      "         [0.2392, 0.5956],\n",
      "         [0.2155, 0.6467],\n",
      "         [0.2360, 0.6703]]])\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 1.2ms preprocess, 113.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2290477752685547, 'inference': 113.6178970336914, 'postprocess': 0.5671977996826172}]\n",
      "Bounding Box: tensor([[ 80.0000, 190.5000,  38.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2381, 0.2742, 0.0625, 0.8018, 0.2510, 0.9722, 0.9295, 0.8393, 0.5852, 0.5408, 0.3054, 0.9921, 0.9888, 0.9846, 0.9779, 0.9614, 0.9490]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3810e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7417e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.2472e-02],\n",
      "         [6.9657e+01, 1.4457e+02, 8.0175e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5101e-01],\n",
      "         [6.8238e+01, 1.5624e+02, 9.7222e-01],\n",
      "         [8.9516e+01, 1.5491e+02, 9.2947e-01],\n",
      "         [6.8005e+01, 1.7346e+02, 8.3931e-01],\n",
      "         [9.6512e+01, 1.7238e+02, 5.8517e-01],\n",
      "         [7.4118e+01, 1.7990e+02, 5.4078e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0539e-01],\n",
      "         [7.8253e+01, 1.8844e+02, 9.9206e-01],\n",
      "         [9.2734e+01, 1.8871e+02, 9.8882e-01],\n",
      "         [7.4561e+01, 2.0797e+02, 9.8457e-01],\n",
      "         [8.7451e+01, 2.1437e+02, 9.7788e-01],\n",
      "         [7.7829e+01, 2.3124e+02, 9.6137e-01],\n",
      "         [8.4611e+01, 2.4087e+02, 9.4897e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.6572, 144.5686],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.2383, 156.2368],\n",
      "         [ 89.5160, 154.9077],\n",
      "         [ 68.0054, 173.4619],\n",
      "         [ 96.5122, 172.3803],\n",
      "         [ 74.1179, 179.8974],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.2533, 188.4417],\n",
      "         [ 92.7337, 188.7063],\n",
      "         [ 74.5614, 207.9720],\n",
      "         [ 87.4508, 214.3651],\n",
      "         [ 77.8294, 231.2366],\n",
      "         [ 84.6111, 240.8740]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1935, 0.4016],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1896, 0.4340],\n",
      "         [0.2487, 0.4303],\n",
      "         [0.1889, 0.4818],\n",
      "         [0.2681, 0.4788],\n",
      "         [0.2059, 0.4997],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2174, 0.5234],\n",
      "         [0.2576, 0.5242],\n",
      "         [0.2071, 0.5777],\n",
      "         [0.2429, 0.5955],\n",
      "         [0.2162, 0.6423],\n",
      "         [0.2350, 0.6691]]])\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.6ms preprocess, 114.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.554727554321289, 'inference': 114.52507972717285, 'postprocess': 0.4799365997314453}]\n",
      "Bounding Box: tensor([[ 86.5000, 191.5000,  49.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3504, 0.2042, 0.1104, 0.7001, 0.4283, 0.9852, 0.9881, 0.9237, 0.9458, 0.7914, 0.8322, 0.9960, 0.9966, 0.9889, 0.9905, 0.9676, 0.9702]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.5044e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0420e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1037e-01],\n",
      "         [6.9715e+01, 1.4539e+02, 7.0007e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2834e-01],\n",
      "         [6.9100e+01, 1.5752e+02, 9.8519e-01],\n",
      "         [8.9171e+01, 1.5609e+02, 9.8813e-01],\n",
      "         [6.6932e+01, 1.7362e+02, 9.2368e-01],\n",
      "         [1.0021e+02, 1.7251e+02, 9.4577e-01],\n",
      "         [7.3846e+01, 1.8115e+02, 7.9141e-01],\n",
      "         [1.0645e+02, 1.8686e+02, 8.3220e-01],\n",
      "         [7.8938e+01, 1.8999e+02, 9.9596e-01],\n",
      "         [9.2049e+01, 1.9013e+02, 9.9663e-01],\n",
      "         [7.5858e+01, 2.0964e+02, 9.8892e-01],\n",
      "         [8.7122e+01, 2.1432e+02, 9.9048e-01],\n",
      "         [7.8836e+01, 2.3377e+02, 9.6756e-01],\n",
      "         [8.5445e+01, 2.4055e+02, 9.7019e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.7145, 145.3925],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.0996, 157.5160],\n",
      "         [ 89.1713, 156.0877],\n",
      "         [ 66.9320, 173.6188],\n",
      "         [100.2133, 172.5052],\n",
      "         [ 73.8457, 181.1501],\n",
      "         [106.4525, 186.8640],\n",
      "         [ 78.9384, 189.9906],\n",
      "         [ 92.0491, 190.1277],\n",
      "         [ 75.8584, 209.6380],\n",
      "         [ 87.1225, 214.3206],\n",
      "         [ 78.8360, 233.7749],\n",
      "         [ 85.4449, 240.5522]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1937, 0.4039],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1919, 0.4375],\n",
      "         [0.2477, 0.4336],\n",
      "         [0.1859, 0.4823],\n",
      "         [0.2784, 0.4792],\n",
      "         [0.2051, 0.5032],\n",
      "         [0.2957, 0.5191],\n",
      "         [0.2193, 0.5278],\n",
      "         [0.2557, 0.5281],\n",
      "         [0.2107, 0.5823],\n",
      "         [0.2420, 0.5953],\n",
      "         [0.2190, 0.6494],\n",
      "         [0.2373, 0.6682]]])\n",
      "\n",
      "0: 640x640 1 person, 120.3ms\n",
      "Speed: 1.3ms preprocess, 120.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2919902801513672, 'inference': 120.33629417419434, 'postprocess': 0.42319297790527344}]\n",
      "Bounding Box: tensor([[ 87.0000, 191.5000,  50.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3392, 0.2151, 0.1030, 0.7313, 0.4231, 0.9861, 0.9855, 0.9263, 0.9270, 0.7835, 0.7809, 0.9956, 0.9958, 0.9881, 0.9882, 0.9658, 0.9655]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.3915e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1506e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0296e-01],\n",
      "         [6.9841e+01, 1.4562e+02, 7.3131e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2308e-01],\n",
      "         [6.9401e+01, 1.5765e+02, 9.8612e-01],\n",
      "         [8.9274e+01, 1.5625e+02, 9.8555e-01],\n",
      "         [6.7369e+01, 1.7428e+02, 9.2627e-01],\n",
      "         [1.0025e+02, 1.7264e+02, 9.2702e-01],\n",
      "         [7.4329e+01, 1.8370e+02, 7.8351e-01],\n",
      "         [1.0740e+02, 1.8702e+02, 7.8091e-01],\n",
      "         [7.9903e+01, 1.9055e+02, 9.9559e-01],\n",
      "         [9.2714e+01, 1.9064e+02, 9.9580e-01],\n",
      "         [7.7082e+01, 2.1034e+02, 9.8808e-01],\n",
      "         [8.7202e+01, 2.1422e+02, 9.8818e-01],\n",
      "         [8.0571e+01, 2.3511e+02, 9.6579e-01],\n",
      "         [8.5573e+01, 2.4047e+02, 9.6549e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.8413, 145.6225],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.4006, 157.6474],\n",
      "         [ 89.2739, 156.2527],\n",
      "         [ 67.3694, 174.2800],\n",
      "         [100.2546, 172.6419],\n",
      "         [ 74.3293, 183.7007],\n",
      "         [107.4028, 187.0217],\n",
      "         [ 79.9027, 190.5496],\n",
      "         [ 92.7138, 190.6418],\n",
      "         [ 77.0818, 210.3394],\n",
      "         [ 87.2017, 214.2181],\n",
      "         [ 80.5707, 235.1149],\n",
      "         [ 85.5735, 240.4700]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1940, 0.4045],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1928, 0.4379],\n",
      "         [0.2480, 0.4340],\n",
      "         [0.1871, 0.4841],\n",
      "         [0.2785, 0.4796],\n",
      "         [0.2065, 0.5103],\n",
      "         [0.2983, 0.5195],\n",
      "         [0.2220, 0.5293],\n",
      "         [0.2575, 0.5296],\n",
      "         [0.2141, 0.5843],\n",
      "         [0.2422, 0.5951],\n",
      "         [0.2238, 0.6531],\n",
      "         [0.2377, 0.6680]]])\n",
      "\n",
      "0: 640x640 1 person, 130.9ms\n",
      "Speed: 1.2ms preprocess, 130.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2180805206298828, 'inference': 130.93209266662598, 'postprocess': 0.39696693420410156}]\n",
      "Bounding Box: tensor([[ 88.0000, 191.5000,  52.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3582, 0.1998, 0.1086, 0.7099, 0.4520, 0.9877, 0.9904, 0.9403, 0.9582, 0.8283, 0.8620, 0.9962, 0.9968, 0.9871, 0.9885, 0.9543, 0.9583]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.5820e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9983e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0862e-01],\n",
      "         [6.9884e+01, 1.4590e+02, 7.0992e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5198e-01],\n",
      "         [7.0083e+01, 1.5801e+02, 9.8769e-01],\n",
      "         [8.9979e+01, 1.5642e+02, 9.9038e-01],\n",
      "         [6.8182e+01, 1.7490e+02, 9.4034e-01],\n",
      "         [1.0166e+02, 1.7367e+02, 9.5816e-01],\n",
      "         [7.4545e+01, 1.8367e+02, 8.2833e-01],\n",
      "         [1.0947e+02, 1.8798e+02, 8.6198e-01],\n",
      "         [8.1755e+01, 1.9124e+02, 9.9617e-01],\n",
      "         [9.4087e+01, 1.9078e+02, 9.9675e-01],\n",
      "         [7.9427e+01, 2.1252e+02, 9.8714e-01],\n",
      "         [8.6611e+01, 2.1330e+02, 9.8847e-01],\n",
      "         [8.3791e+01, 2.3876e+02, 9.5433e-01],\n",
      "         [8.4400e+01, 2.3950e+02, 9.5835e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.8835, 145.9008],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.0825, 158.0111],\n",
      "         [ 89.9791, 156.4215],\n",
      "         [ 68.1817, 174.8966],\n",
      "         [101.6565, 173.6650],\n",
      "         [ 74.5448, 183.6748],\n",
      "         [109.4742, 187.9794],\n",
      "         [ 81.7552, 191.2416],\n",
      "         [ 94.0869, 190.7807],\n",
      "         [ 79.4271, 212.5151],\n",
      "         [ 86.6107, 213.2973],\n",
      "         [ 83.7912, 238.7578],\n",
      "         [ 84.4002, 239.5006]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1941, 0.4053],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1947, 0.4389],\n",
      "         [0.2499, 0.4345],\n",
      "         [0.1894, 0.4858],\n",
      "         [0.2824, 0.4824],\n",
      "         [0.2071, 0.5102],\n",
      "         [0.3041, 0.5222],\n",
      "         [0.2271, 0.5312],\n",
      "         [0.2614, 0.5299],\n",
      "         [0.2206, 0.5903],\n",
      "         [0.2406, 0.5925],\n",
      "         [0.2328, 0.6632],\n",
      "         [0.2344, 0.6653]]])\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.2ms preprocess, 116.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2259483337402344, 'inference': 116.4863109588623, 'postprocess': 0.61798095703125}]\n",
      "Bounding Box: tensor([[ 89.5000, 191.0000,  51.0000, 114.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3429, 0.1978, 0.1067, 0.7112, 0.4487, 0.9854, 0.9891, 0.9245, 0.9500, 0.7854, 0.8322, 0.9951, 0.9960, 0.9844, 0.9864, 0.9490, 0.9546]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.4286e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9780e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0671e-01],\n",
      "         [7.0274e+01, 1.4614e+02, 7.1123e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4867e-01],\n",
      "         [7.0858e+01, 1.5829e+02, 9.8541e-01],\n",
      "         [9.0112e+01, 1.5660e+02, 9.8905e-01],\n",
      "         [6.9173e+01, 1.7526e+02, 9.2447e-01],\n",
      "         [1.0157e+02, 1.7347e+02, 9.5002e-01],\n",
      "         [7.4202e+01, 1.8419e+02, 7.8539e-01],\n",
      "         [1.1062e+02, 1.8667e+02, 8.3223e-01],\n",
      "         [8.2612e+01, 1.9160e+02, 9.9510e-01],\n",
      "         [9.4470e+01, 1.9102e+02, 9.9597e-01],\n",
      "         [8.0118e+01, 2.1280e+02, 9.8442e-01],\n",
      "         [8.6927e+01, 2.1298e+02, 9.8644e-01],\n",
      "         [8.4189e+01, 2.3919e+02, 9.4895e-01],\n",
      "         [8.4238e+01, 2.3907e+02, 9.5456e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.2745, 146.1432],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8584, 158.2869],\n",
      "         [ 90.1120, 156.6039],\n",
      "         [ 69.1732, 175.2570],\n",
      "         [101.5711, 173.4696],\n",
      "         [ 74.2017, 184.1873],\n",
      "         [110.6215, 186.6685],\n",
      "         [ 82.6122, 191.5988],\n",
      "         [ 94.4696, 191.0245],\n",
      "         [ 80.1184, 212.7996],\n",
      "         [ 86.9272, 212.9830],\n",
      "         [ 84.1885, 239.1924],\n",
      "         [ 84.2380, 239.0717]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1952, 0.4060],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1968, 0.4397],\n",
      "         [0.2503, 0.4350],\n",
      "         [0.1921, 0.4868],\n",
      "         [0.2821, 0.4819],\n",
      "         [0.2061, 0.5116],\n",
      "         [0.3073, 0.5185],\n",
      "         [0.2295, 0.5322],\n",
      "         [0.2624, 0.5306],\n",
      "         [0.2226, 0.5911],\n",
      "         [0.2415, 0.5916],\n",
      "         [0.2339, 0.6644],\n",
      "         [0.2340, 0.6641]]])\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.3ms preprocess, 116.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.257181167602539, 'inference': 116.14203453063965, 'postprocess': 0.42510032653808594}]\n",
      "Bounding Box: tensor([[ 83.5000, 191.5000,  39.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2490, 0.2059, 0.0758, 0.7198, 0.3180, 0.9684, 0.9661, 0.8383, 0.8244, 0.5861, 0.5674, 0.9924, 0.9928, 0.9777, 0.9779, 0.9289, 0.9304]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.4903e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0590e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.5801e-02],\n",
      "         [7.0571e+01, 1.4606e+02, 7.1975e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1795e-01],\n",
      "         [7.0533e+01, 1.5759e+02, 9.6841e-01],\n",
      "         [9.0497e+01, 1.5663e+02, 9.6608e-01],\n",
      "         [7.0608e+01, 1.7349e+02, 8.3834e-01],\n",
      "         [1.0129e+02, 1.7365e+02, 8.2441e-01],\n",
      "         [7.4665e+01, 1.8334e+02, 5.8606e-01],\n",
      "         [1.0764e+02, 1.8618e+02, 5.6743e-01],\n",
      "         [8.1588e+01, 1.9043e+02, 9.9242e-01],\n",
      "         [9.4422e+01, 1.9030e+02, 9.9277e-01],\n",
      "         [7.6120e+01, 2.1142e+02, 9.7766e-01],\n",
      "         [8.7292e+01, 2.1382e+02, 9.7788e-01],\n",
      "         [8.0604e+01, 2.3535e+02, 9.2889e-01],\n",
      "         [8.4291e+01, 2.3982e+02, 9.3038e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.5710, 146.0571],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.5327, 157.5939],\n",
      "         [ 90.4965, 156.6274],\n",
      "         [ 70.6082, 173.4933],\n",
      "         [101.2874, 173.6494],\n",
      "         [ 74.6650, 183.3375],\n",
      "         [107.6430, 186.1836],\n",
      "         [ 81.5882, 190.4255],\n",
      "         [ 94.4218, 190.3044],\n",
      "         [ 76.1198, 211.4169],\n",
      "         [ 87.2921, 213.8160],\n",
      "         [ 80.6044, 235.3515],\n",
      "         [ 84.2909, 239.8179]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1960, 0.4057],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1959, 0.4378],\n",
      "         [0.2514, 0.4351],\n",
      "         [0.1961, 0.4819],\n",
      "         [0.2814, 0.4824],\n",
      "         [0.2074, 0.5093],\n",
      "         [0.2990, 0.5172],\n",
      "         [0.2266, 0.5290],\n",
      "         [0.2623, 0.5286],\n",
      "         [0.2114, 0.5873],\n",
      "         [0.2425, 0.5939],\n",
      "         [0.2239, 0.6538],\n",
      "         [0.2341, 0.6662]]])\n",
      "\n",
      "0: 640x640 1 person, 116.6ms\n",
      "Speed: 1.3ms preprocess, 116.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3060569763183594, 'inference': 116.64986610412598, 'postprocess': 0.4038810729980469}]\n",
      "Bounding Box: tensor([[ 91.0000, 191.5000,  52.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3035, 0.1835, 0.0956, 0.7126, 0.4278, 0.9811, 0.9874, 0.8889, 0.9369, 0.6967, 0.7799, 0.9942, 0.9956, 0.9840, 0.9873, 0.9547, 0.9620]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.0352e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8348e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.5556e-02],\n",
      "         [7.0515e+01, 1.4625e+02, 7.1262e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2781e-01],\n",
      "         [7.1548e+01, 1.5815e+02, 9.8111e-01],\n",
      "         [9.0325e+01, 1.5707e+02, 9.8742e-01],\n",
      "         [7.0394e+01, 1.7408e+02, 8.8889e-01],\n",
      "         [1.0265e+02, 1.7356e+02, 9.3695e-01],\n",
      "         [7.4848e+01, 1.8342e+02, 6.9672e-01],\n",
      "         [1.1228e+02, 1.8579e+02, 7.7989e-01],\n",
      "         [8.3185e+01, 1.9179e+02, 9.9418e-01],\n",
      "         [9.4881e+01, 1.9140e+02, 9.9562e-01],\n",
      "         [7.9009e+01, 2.1386e+02, 9.8403e-01],\n",
      "         [8.7089e+01, 2.1450e+02, 9.8727e-01],\n",
      "         [8.1643e+01, 2.4003e+02, 9.5470e-01],\n",
      "         [8.4490e+01, 2.4032e+02, 9.6201e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.5147, 146.2520],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 71.5478, 158.1476],\n",
      "         [ 90.3247, 157.0715],\n",
      "         [ 70.3938, 174.0764],\n",
      "         [102.6521, 173.5575],\n",
      "         [ 74.8475, 183.4211],\n",
      "         [112.2759, 185.7938],\n",
      "         [ 83.1851, 191.7859],\n",
      "         [ 94.8814, 191.4002],\n",
      "         [ 79.0089, 213.8630],\n",
      "         [ 87.0887, 214.4983],\n",
      "         [ 81.6430, 240.0346],\n",
      "         [ 84.4904, 240.3233]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1959, 0.4063],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1987, 0.4393],\n",
      "         [0.2509, 0.4363],\n",
      "         [0.1955, 0.4835],\n",
      "         [0.2851, 0.4821],\n",
      "         [0.2079, 0.5095],\n",
      "         [0.3119, 0.5161],\n",
      "         [0.2311, 0.5327],\n",
      "         [0.2636, 0.5317],\n",
      "         [0.2195, 0.5941],\n",
      "         [0.2419, 0.5958],\n",
      "         [0.2268, 0.6668],\n",
      "         [0.2347, 0.6676]]])\n",
      "\n",
      "0: 640x640 1 person, 126.6ms\n",
      "Speed: 1.2ms preprocess, 126.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        [151, 173, 195],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        [150, 172, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2052059173583984, 'inference': 126.57690048217773, 'postprocess': 0.4839897155761719}]\n",
      "Bounding Box: tensor([[ 90.5000, 191.5000,  53.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3512, 0.2274, 0.0862, 0.7429, 0.3467, 0.9895, 0.9867, 0.9575, 0.9435, 0.8707, 0.8341, 0.9969, 0.9966, 0.9903, 0.9887, 0.9646, 0.9616]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.5115e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2741e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.6250e-02],\n",
      "         [7.0618e+01, 1.4703e+02, 7.4288e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4665e-01],\n",
      "         [7.2536e+01, 1.5861e+02, 9.8950e-01],\n",
      "         [9.0299e+01, 1.5747e+02, 9.8674e-01],\n",
      "         [7.2120e+01, 1.7453e+02, 9.5745e-01],\n",
      "         [1.0317e+02, 1.7400e+02, 9.4353e-01],\n",
      "         [7.3959e+01, 1.8345e+02, 8.7069e-01],\n",
      "         [1.1258e+02, 1.8571e+02, 8.3414e-01],\n",
      "         [8.4419e+01, 1.9197e+02, 9.9686e-01],\n",
      "         [9.5224e+01, 1.9142e+02, 9.9661e-01],\n",
      "         [7.8278e+01, 2.1427e+02, 9.9032e-01],\n",
      "         [8.5215e+01, 2.1475e+02, 9.8870e-01],\n",
      "         [8.1808e+01, 2.4088e+02, 9.6464e-01],\n",
      "         [8.2182e+01, 2.4035e+02, 9.6157e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.6180, 147.0341],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.5361, 158.6081],\n",
      "         [ 90.2993, 157.4680],\n",
      "         [ 72.1204, 174.5344],\n",
      "         [103.1704, 173.9976],\n",
      "         [ 73.9588, 183.4540],\n",
      "         [112.5783, 185.7077],\n",
      "         [ 84.4186, 191.9703],\n",
      "         [ 95.2237, 191.4190],\n",
      "         [ 78.2776, 214.2691],\n",
      "         [ 85.2151, 214.7462],\n",
      "         [ 81.8084, 240.8840],\n",
      "         [ 82.1822, 240.3546]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1962, 0.4084],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2015, 0.4406],\n",
      "         [0.2508, 0.4374],\n",
      "         [0.2003, 0.4848],\n",
      "         [0.2866, 0.4833],\n",
      "         [0.2054, 0.5096],\n",
      "         [0.3127, 0.5159],\n",
      "         [0.2345, 0.5333],\n",
      "         [0.2645, 0.5317],\n",
      "         [0.2174, 0.5952],\n",
      "         [0.2367, 0.5965],\n",
      "         [0.2272, 0.6691],\n",
      "         [0.2283, 0.6677]]])\n",
      "\n",
      "0: 640x640 1 person, 114.3ms\n",
      "Speed: 1.2ms preprocess, 114.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 174, 198],\n",
      "        [152, 174, 198],\n",
      "        [152, 174, 198],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [178, 161, 144]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [178, 161, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2481212615966797, 'inference': 114.33291435241699, 'postprocess': 0.4010200500488281}]\n",
      "Bounding Box: tensor([[ 83.0000, 191.5000,  40.0000, 115.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2391, 0.3183, 0.0308, 0.8832, 0.1381, 0.9920, 0.9228, 0.9705, 0.5706, 0.8637, 0.3421, 0.9968, 0.9906, 0.9936, 0.9798, 0.9798, 0.9563]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3907e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1834e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0783e-02],\n",
      "         [7.0228e+01, 1.4643e+02, 8.8321e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3813e-01],\n",
      "         [7.3481e+01, 1.5865e+02, 9.9202e-01],\n",
      "         [8.9486e+01, 1.5720e+02, 9.2283e-01],\n",
      "         [7.4876e+01, 1.7543e+02, 9.7046e-01],\n",
      "         [1.0125e+02, 1.7523e+02, 5.7057e-01],\n",
      "         [7.3326e+01, 1.8576e+02, 8.6371e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4206e-01],\n",
      "         [8.5274e+01, 1.9186e+02, 9.9677e-01],\n",
      "         [9.5214e+01, 1.9113e+02, 9.9060e-01],\n",
      "         [7.8609e+01, 2.1397e+02, 9.9357e-01],\n",
      "         [8.6297e+01, 2.1477e+02, 9.7976e-01],\n",
      "         [8.2994e+01, 2.4023e+02, 9.7981e-01],\n",
      "         [8.4552e+01, 2.4092e+02, 9.5628e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.2277, 146.4256],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.4812, 158.6537],\n",
      "         [ 89.4863, 157.2023],\n",
      "         [ 74.8757, 175.4252],\n",
      "         [101.2501, 175.2265],\n",
      "         [ 73.3262, 185.7618],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 85.2738, 191.8590],\n",
      "         [ 95.2140, 191.1323],\n",
      "         [ 78.6090, 213.9734],\n",
      "         [ 86.2968, 214.7659],\n",
      "         [ 82.9936, 240.2312],\n",
      "         [ 84.5525, 240.9186]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1951, 0.4067],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2041, 0.4407],\n",
      "         [0.2486, 0.4367],\n",
      "         [0.2080, 0.4873],\n",
      "         [0.2813, 0.4867],\n",
      "         [0.2037, 0.5160],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2369, 0.5329],\n",
      "         [0.2645, 0.5309],\n",
      "         [0.2184, 0.5944],\n",
      "         [0.2397, 0.5966],\n",
      "         [0.2305, 0.6673],\n",
      "         [0.2349, 0.6692]]])\n",
      "\n",
      "0: 640x640 1 person, 112.2ms\n",
      "Speed: 1.2ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[152, 174, 198],\n",
      "        [152, 174, 198],\n",
      "        [152, 174, 198],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2350082397460938, 'inference': 112.16497421264648, 'postprocess': 0.3998279571533203}]\n",
      "Bounding Box: tensor([[ 90.5000, 192.5000,  55.0000, 117.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7124, 0.6745, 0.1919, 0.8801, 0.2149, 0.9944, 0.9779, 0.9824, 0.8930, 0.9573, 0.8233, 0.9977, 0.9958, 0.9957, 0.9910, 0.9828, 0.9719]])\n",
      "data: tensor([[[6.6702e+01, 1.4774e+02, 7.1244e-01],\n",
      "         [6.7486e+01, 1.4526e+02, 6.7449e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9190e-01],\n",
      "         [7.2319e+01, 1.4577e+02, 8.8006e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1492e-01],\n",
      "         [7.4088e+01, 1.5831e+02, 9.9440e-01],\n",
      "         [9.0602e+01, 1.5732e+02, 9.7786e-01],\n",
      "         [7.4984e+01, 1.7373e+02, 9.8237e-01],\n",
      "         [1.0401e+02, 1.7374e+02, 8.9303e-01],\n",
      "         [7.3338e+01, 1.8467e+02, 9.5733e-01],\n",
      "         [1.1340e+02, 1.8634e+02, 8.2328e-01],\n",
      "         [8.8067e+01, 1.9091e+02, 9.9775e-01],\n",
      "         [9.8661e+01, 1.9049e+02, 9.9577e-01],\n",
      "         [7.3296e+01, 2.1121e+02, 9.9570e-01],\n",
      "         [8.5925e+01, 2.1313e+02, 9.9098e-01],\n",
      "         [8.1747e+01, 2.3964e+02, 9.8278e-01],\n",
      "         [8.5483e+01, 2.4205e+02, 9.7193e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 66.7018, 147.7375],\n",
      "         [ 67.4857, 145.2590],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.3187, 145.7684],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.0879, 158.3110],\n",
      "         [ 90.6020, 157.3235],\n",
      "         [ 74.9839, 173.7348],\n",
      "         [104.0067, 173.7373],\n",
      "         [ 73.3378, 184.6657],\n",
      "         [113.4003, 186.3438],\n",
      "         [ 88.0667, 190.9108],\n",
      "         [ 98.6609, 190.4855],\n",
      "         [ 73.2961, 211.2101],\n",
      "         [ 85.9253, 213.1268],\n",
      "         [ 81.7468, 239.6375],\n",
      "         [ 85.4830, 242.0504]]])\n",
      "xyn: tensor([[[0.1853, 0.4104],\n",
      "         [0.1875, 0.4035],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2009, 0.4049],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2058, 0.4398],\n",
      "         [0.2517, 0.4370],\n",
      "         [0.2083, 0.4826],\n",
      "         [0.2889, 0.4826],\n",
      "         [0.2037, 0.5130],\n",
      "         [0.3150, 0.5176],\n",
      "         [0.2446, 0.5303],\n",
      "         [0.2741, 0.5291],\n",
      "         [0.2036, 0.5867],\n",
      "         [0.2387, 0.5920],\n",
      "         [0.2271, 0.6657],\n",
      "         [0.2375, 0.6724]]])\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.3ms preprocess, 114.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2891292572021484, 'inference': 114.2268180847168, 'postprocess': 0.6437301635742188}]\n",
      "Bounding Box: tensor([[ 91.0000, 193.5000,  54.0000, 119.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6170, 0.6341, 0.1556, 0.8833, 0.1834, 0.9927, 0.9649, 0.9705, 0.8135, 0.9122, 0.6819, 0.9979, 0.9960, 0.9954, 0.9906, 0.9820, 0.9714]])\n",
      "data: tensor([[[6.7014e+01, 1.4748e+02, 6.1696e-01],\n",
      "         [6.7689e+01, 1.4506e+02, 6.3407e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5556e-01],\n",
      "         [7.2126e+01, 1.4574e+02, 8.8328e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8343e-01],\n",
      "         [7.4835e+01, 1.5864e+02, 9.9267e-01],\n",
      "         [9.0698e+01, 1.5755e+02, 9.6493e-01],\n",
      "         [7.6169e+01, 1.7540e+02, 9.7053e-01],\n",
      "         [1.0491e+02, 1.7408e+02, 8.1351e-01],\n",
      "         [7.2498e+01, 1.8616e+02, 9.1222e-01],\n",
      "         [1.1406e+02, 1.8663e+02, 6.8187e-01],\n",
      "         [8.8724e+01, 1.9216e+02, 9.9787e-01],\n",
      "         [9.8483e+01, 1.9114e+02, 9.9598e-01],\n",
      "         [7.4058e+01, 2.1123e+02, 9.9542e-01],\n",
      "         [8.3864e+01, 2.1241e+02, 9.9056e-01],\n",
      "         [8.5619e+01, 2.4056e+02, 9.8203e-01],\n",
      "         [8.7183e+01, 2.4194e+02, 9.7142e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 67.0141, 147.4792],\n",
      "         [ 67.6892, 145.0627],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.1256, 145.7434],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.8351, 158.6396],\n",
      "         [ 90.6978, 157.5519],\n",
      "         [ 76.1687, 175.4005],\n",
      "         [104.9132, 174.0761],\n",
      "         [ 72.4979, 186.1578],\n",
      "         [114.0612, 186.6335],\n",
      "         [ 88.7242, 192.1565],\n",
      "         [ 98.4830, 191.1362],\n",
      "         [ 74.0583, 211.2259],\n",
      "         [ 83.8641, 212.4085],\n",
      "         [ 85.6190, 240.5568],\n",
      "         [ 87.1826, 241.9381]]])\n",
      "xyn: tensor([[[0.1862, 0.4097],\n",
      "         [0.1880, 0.4030],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2003, 0.4048],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2079, 0.4407],\n",
      "         [0.2519, 0.4376],\n",
      "         [0.2116, 0.4872],\n",
      "         [0.2914, 0.4835],\n",
      "         [0.2014, 0.5171],\n",
      "         [0.3168, 0.5184],\n",
      "         [0.2465, 0.5338],\n",
      "         [0.2736, 0.5309],\n",
      "         [0.2057, 0.5867],\n",
      "         [0.2330, 0.5900],\n",
      "         [0.2378, 0.6682],\n",
      "         [0.2422, 0.6721]]])\n",
      "\n",
      "0: 640x640 1 person, 137.7ms\n",
      "Speed: 1.3ms preprocess, 137.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2748241424560547, 'inference': 137.72273063659668, 'postprocess': 0.4611015319824219}]\n",
      "Bounding Box: tensor([[ 91., 194.,  54., 120.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6748, 0.6783, 0.1674, 0.8871, 0.1913, 0.9933, 0.9632, 0.9794, 0.8194, 0.9446, 0.7140, 0.9962, 0.9917, 0.9925, 0.9815, 0.9711, 0.9483]])\n",
      "data: tensor([[[6.7537e+01, 1.4816e+02, 6.7484e-01],\n",
      "         [6.8269e+01, 1.4557e+02, 6.7835e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6738e-01],\n",
      "         [7.2917e+01, 1.4608e+02, 8.8705e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9133e-01],\n",
      "         [7.7828e+01, 1.5864e+02, 9.9325e-01],\n",
      "         [8.8353e+01, 1.5763e+02, 9.6315e-01],\n",
      "         [8.0795e+01, 1.7604e+02, 9.7936e-01],\n",
      "         [1.0309e+02, 1.7484e+02, 8.1944e-01],\n",
      "         [7.4959e+01, 1.8620e+02, 9.4460e-01],\n",
      "         [1.1316e+02, 1.8555e+02, 7.1402e-01],\n",
      "         [9.1709e+01, 1.9181e+02, 9.9623e-01],\n",
      "         [9.7683e+01, 1.9111e+02, 9.9169e-01],\n",
      "         [7.7949e+01, 2.1239e+02, 9.9254e-01],\n",
      "         [8.3078e+01, 2.1278e+02, 9.8153e-01],\n",
      "         [8.7787e+01, 2.4211e+02, 9.7106e-01],\n",
      "         [8.5988e+01, 2.4139e+02, 9.4826e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 67.5373, 148.1636],\n",
      "         [ 68.2693, 145.5703],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.9168, 146.0761],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.8284, 158.6393],\n",
      "         [ 88.3530, 157.6290],\n",
      "         [ 80.7954, 176.0426],\n",
      "         [103.0916, 174.8404],\n",
      "         [ 74.9588, 186.1952],\n",
      "         [113.1650, 185.5522],\n",
      "         [ 91.7090, 191.8066],\n",
      "         [ 97.6830, 191.1120],\n",
      "         [ 77.9494, 212.3914],\n",
      "         [ 83.0784, 212.7771],\n",
      "         [ 87.7871, 242.1097],\n",
      "         [ 85.9883, 241.3946]]])\n",
      "xyn: tensor([[[0.1876, 0.4116],\n",
      "         [0.1896, 0.4044],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2025, 0.4058],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2162, 0.4407],\n",
      "         [0.2454, 0.4379],\n",
      "         [0.2244, 0.4890],\n",
      "         [0.2864, 0.4857],\n",
      "         [0.2082, 0.5172],\n",
      "         [0.3143, 0.5154],\n",
      "         [0.2547, 0.5328],\n",
      "         [0.2713, 0.5309],\n",
      "         [0.2165, 0.5900],\n",
      "         [0.2308, 0.5910],\n",
      "         [0.2439, 0.6725],\n",
      "         [0.2389, 0.6705]]])\n",
      "\n",
      "0: 640x640 1 person, 124.7ms\n",
      "Speed: 1.2ms preprocess, 124.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.1990070343017578, 'inference': 124.6500015258789, 'postprocess': 0.42510032653808594}]\n",
      "Bounding Box: tensor([[ 91., 193.,  52., 118.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6943, 0.6421, 0.1851, 0.8413, 0.2069, 0.9930, 0.9720, 0.9832, 0.8931, 0.9635, 0.8412, 0.9975, 0.9952, 0.9947, 0.9884, 0.9771, 0.9619]])\n",
      "data: tensor([[[6.8384e+01, 1.4823e+02, 6.9430e-01],\n",
      "         [6.9256e+01, 1.4580e+02, 6.4206e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8513e-01],\n",
      "         [7.3558e+01, 1.4662e+02, 8.4129e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0685e-01],\n",
      "         [7.6631e+01, 1.5863e+02, 9.9299e-01],\n",
      "         [9.0440e+01, 1.5746e+02, 9.7196e-01],\n",
      "         [8.1060e+01, 1.7523e+02, 9.8324e-01],\n",
      "         [1.0516e+02, 1.7518e+02, 8.9311e-01],\n",
      "         [7.5815e+01, 1.8620e+02, 9.6350e-01],\n",
      "         [1.1274e+02, 1.8795e+02, 8.4120e-01],\n",
      "         [9.1257e+01, 1.9128e+02, 9.9752e-01],\n",
      "         [9.9892e+01, 1.9060e+02, 9.9522e-01],\n",
      "         [7.4845e+01, 2.1451e+02, 9.9471e-01],\n",
      "         [8.5657e+01, 2.1548e+02, 9.8843e-01],\n",
      "         [8.9720e+01, 2.4001e+02, 9.7712e-01],\n",
      "         [8.8789e+01, 2.4203e+02, 9.6192e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 68.3838, 148.2318],\n",
      "         [ 69.2559, 145.7954],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 73.5576, 146.6228],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.6310, 158.6308],\n",
      "         [ 90.4400, 157.4643],\n",
      "         [ 81.0603, 175.2342],\n",
      "         [105.1628, 175.1809],\n",
      "         [ 75.8155, 186.1997],\n",
      "         [112.7407, 187.9475],\n",
      "         [ 91.2573, 191.2760],\n",
      "         [ 99.8917, 190.5972],\n",
      "         [ 74.8451, 214.5050],\n",
      "         [ 85.6566, 215.4763],\n",
      "         [ 89.7202, 240.0110],\n",
      "         [ 88.7889, 242.0301]]])\n",
      "xyn: tensor([[[0.1900, 0.4118],\n",
      "         [0.1924, 0.4050],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2043, 0.4073],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2129, 0.4406],\n",
      "         [0.2512, 0.4374],\n",
      "         [0.2252, 0.4868],\n",
      "         [0.2921, 0.4866],\n",
      "         [0.2106, 0.5172],\n",
      "         [0.3132, 0.5221],\n",
      "         [0.2535, 0.5313],\n",
      "         [0.2775, 0.5294],\n",
      "         [0.2079, 0.5958],\n",
      "         [0.2379, 0.5985],\n",
      "         [0.2492, 0.6667],\n",
      "         [0.2466, 0.6723]]])\n",
      "\n",
      "0: 640x640 1 person, 147.1ms\n",
      "Speed: 1.4ms preprocess, 147.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4362335205078125, 'inference': 147.10307121276855, 'postprocess': 0.39315223693847656}]\n",
      "Bounding Box: tensor([[ 89.5000, 195.5000,  47.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7961, 0.8192, 0.2772, 0.8773, 0.1612, 0.9913, 0.9413, 0.9808, 0.7675, 0.9592, 0.7270, 0.9977, 0.9942, 0.9958, 0.9878, 0.9829, 0.9665]])\n",
      "data: tensor([[[6.9259e+01, 1.4865e+02, 7.9608e-01],\n",
      "         [7.0425e+01, 1.4603e+02, 8.1924e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7719e-01],\n",
      "         [7.5996e+01, 1.4624e+02, 8.7732e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6122e-01],\n",
      "         [7.8966e+01, 1.5914e+02, 9.9130e-01],\n",
      "         [8.9291e+01, 1.5773e+02, 9.4132e-01],\n",
      "         [8.3373e+01, 1.7697e+02, 9.8078e-01],\n",
      "         [1.0328e+02, 1.7488e+02, 7.6754e-01],\n",
      "         [7.4727e+01, 1.8627e+02, 9.5923e-01],\n",
      "         [1.0890e+02, 1.8721e+02, 7.2699e-01],\n",
      "         [9.1207e+01, 1.9157e+02, 9.9772e-01],\n",
      "         [9.6621e+01, 1.9066e+02, 9.9420e-01],\n",
      "         [8.1311e+01, 2.1796e+02, 9.9576e-01],\n",
      "         [8.3120e+01, 2.1716e+02, 9.8779e-01],\n",
      "         [9.8305e+01, 2.4404e+02, 9.8289e-01],\n",
      "         [8.6034e+01, 2.4330e+02, 9.6655e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 69.2590, 148.6473],\n",
      "         [ 70.4248, 146.0319],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 75.9960, 146.2388],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.9660, 159.1433],\n",
      "         [ 89.2913, 157.7335],\n",
      "         [ 83.3729, 176.9697],\n",
      "         [103.2765, 174.8826],\n",
      "         [ 74.7268, 186.2671],\n",
      "         [108.9018, 187.2137],\n",
      "         [ 91.2067, 191.5678],\n",
      "         [ 96.6213, 190.6636],\n",
      "         [ 81.3112, 217.9558],\n",
      "         [ 83.1198, 217.1561],\n",
      "         [ 98.3045, 244.0440],\n",
      "         [ 86.0342, 243.3044]]])\n",
      "xyn: tensor([[[0.1924, 0.4129],\n",
      "         [0.1956, 0.4056],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2111, 0.4062],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2193, 0.4421],\n",
      "         [0.2480, 0.4381],\n",
      "         [0.2316, 0.4916],\n",
      "         [0.2869, 0.4858],\n",
      "         [0.2076, 0.5174],\n",
      "         [0.3025, 0.5200],\n",
      "         [0.2534, 0.5321],\n",
      "         [0.2684, 0.5296],\n",
      "         [0.2259, 0.6054],\n",
      "         [0.2309, 0.6032],\n",
      "         [0.2731, 0.6779],\n",
      "         [0.2390, 0.6758]]])\n",
      "\n",
      "0: 640x640 1 person, 171.3ms\n",
      "Speed: 1.5ms preprocess, 171.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4698505401611328, 'inference': 171.2818145751953, 'postprocess': 0.49877166748046875}]\n",
      "Bounding Box: tensor([[ 90.5000, 197.5000,  43.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7135, 0.7171, 0.2146, 0.8120, 0.1706, 0.9892, 0.9327, 0.9817, 0.7853, 0.9619, 0.7536, 0.9980, 0.9949, 0.9961, 0.9886, 0.9841, 0.9682]])\n",
      "data: tensor([[[7.1688e+01, 1.4928e+02, 7.1354e-01],\n",
      "         [7.2850e+01, 1.4702e+02, 7.1712e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1457e-01],\n",
      "         [7.6782e+01, 1.4844e+02, 8.1197e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7057e-01],\n",
      "         [7.8822e+01, 1.6075e+02, 9.8922e-01],\n",
      "         [9.0497e+01, 1.5905e+02, 9.3273e-01],\n",
      "         [8.5211e+01, 1.7833e+02, 9.8165e-01],\n",
      "         [1.0376e+02, 1.7702e+02, 7.8526e-01],\n",
      "         [7.7071e+01, 1.8837e+02, 9.6193e-01],\n",
      "         [1.0613e+02, 1.9013e+02, 7.5357e-01],\n",
      "         [9.3755e+01, 1.9232e+02, 9.9798e-01],\n",
      "         [1.0010e+02, 1.9121e+02, 9.9490e-01],\n",
      "         [8.0494e+01, 2.1915e+02, 9.9614e-01],\n",
      "         [8.3228e+01, 2.1746e+02, 9.8857e-01],\n",
      "         [1.0295e+02, 2.4375e+02, 9.8409e-01],\n",
      "         [8.6838e+01, 2.4243e+02, 9.6816e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 71.6882, 149.2845],\n",
      "         [ 72.8503, 147.0215],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.7823, 148.4369],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.8219, 160.7518],\n",
      "         [ 90.4975, 159.0531],\n",
      "         [ 85.2110, 178.3311],\n",
      "         [103.7569, 177.0196],\n",
      "         [ 77.0707, 188.3720],\n",
      "         [106.1326, 190.1342],\n",
      "         [ 93.7546, 192.3163],\n",
      "         [100.1040, 191.2133],\n",
      "         [ 80.4943, 219.1528],\n",
      "         [ 83.2283, 217.4579],\n",
      "         [102.9513, 243.7509],\n",
      "         [ 86.8380, 242.4339]]])\n",
      "xyn: tensor([[[0.1991, 0.4147],\n",
      "         [0.2024, 0.4084],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2133, 0.4123],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2189, 0.4465],\n",
      "         [0.2514, 0.4418],\n",
      "         [0.2367, 0.4954],\n",
      "         [0.2882, 0.4917],\n",
      "         [0.2141, 0.5233],\n",
      "         [0.2948, 0.5282],\n",
      "         [0.2604, 0.5342],\n",
      "         [0.2781, 0.5311],\n",
      "         [0.2236, 0.6088],\n",
      "         [0.2312, 0.6040],\n",
      "         [0.2860, 0.6771],\n",
      "         [0.2412, 0.6734]]])\n",
      "\n",
      "0: 640x640 1 person, 118.3ms\n",
      "Speed: 1.3ms preprocess, 118.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2710094451904297, 'inference': 118.27874183654785, 'postprocess': 0.42700767517089844}]\n",
      "Bounding Box: tensor([[ 90.5000, 197.5000,  43.0000, 117.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8419, 0.8872, 0.3171, 0.8633, 0.1062, 0.9885, 0.8533, 0.9817, 0.5604, 0.9655, 0.6069, 0.9962, 0.9867, 0.9959, 0.9833, 0.9887, 0.9716]])\n",
      "data: tensor([[[7.2348e+01, 1.5033e+02, 8.4195e-01],\n",
      "         [7.4197e+01, 1.4795e+02, 8.8723e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1715e-01],\n",
      "         [7.9963e+01, 1.4907e+02, 8.6329e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0615e-01],\n",
      "         [8.3406e+01, 1.6191e+02, 9.8847e-01],\n",
      "         [8.7233e+01, 1.6044e+02, 8.5327e-01],\n",
      "         [8.7944e+01, 1.8093e+02, 9.8175e-01],\n",
      "         [9.2407e+01, 1.7985e+02, 5.6037e-01],\n",
      "         [7.7493e+01, 1.8936e+02, 9.6548e-01],\n",
      "         [8.1506e+01, 1.8963e+02, 6.0691e-01],\n",
      "         [9.4383e+01, 1.9464e+02, 9.9624e-01],\n",
      "         [9.5979e+01, 1.9342e+02, 9.8666e-01],\n",
      "         [8.4340e+01, 2.2167e+02, 9.9590e-01],\n",
      "         [8.4619e+01, 2.1897e+02, 9.8331e-01],\n",
      "         [1.0485e+02, 2.4378e+02, 9.8873e-01],\n",
      "         [8.6905e+01, 2.4301e+02, 9.7156e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 72.3477, 150.3324],\n",
      "         [ 74.1972, 147.9487],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.9635, 149.0714],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 83.4056, 161.9063],\n",
      "         [ 87.2326, 160.4350],\n",
      "         [ 87.9438, 180.9310],\n",
      "         [ 92.4066, 179.8467],\n",
      "         [ 77.4935, 189.3624],\n",
      "         [ 81.5064, 189.6282],\n",
      "         [ 94.3835, 194.6385],\n",
      "         [ 95.9795, 193.4194],\n",
      "         [ 84.3403, 221.6742],\n",
      "         [ 84.6187, 218.9713],\n",
      "         [104.8509, 243.7834],\n",
      "         [ 86.9045, 243.0093]]])\n",
      "xyn: tensor([[[0.2010, 0.4176],\n",
      "         [0.2061, 0.4110],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2221, 0.4141],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2317, 0.4497],\n",
      "         [0.2423, 0.4457],\n",
      "         [0.2443, 0.5026],\n",
      "         [0.2567, 0.4996],\n",
      "         [0.2153, 0.5260],\n",
      "         [0.2264, 0.5267],\n",
      "         [0.2622, 0.5407],\n",
      "         [0.2666, 0.5373],\n",
      "         [0.2343, 0.6158],\n",
      "         [0.2351, 0.6083],\n",
      "         [0.2913, 0.6772],\n",
      "         [0.2414, 0.6750]]])\n",
      "\n",
      "0: 640x640 1 person, 118.9ms\n",
      "Speed: 1.3ms preprocess, 118.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3229846954345703, 'inference': 118.86811256408691, 'postprocess': 0.43511390686035156}]\n",
      "Bounding Box: tensor([[ 93., 198.,  44., 118.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8763, 0.9124, 0.3812, 0.8561, 0.0938, 0.9872, 0.8387, 0.9816, 0.5598, 0.9700, 0.6335, 0.9961, 0.9859, 0.9959, 0.9832, 0.9889, 0.9722]])\n",
      "data: tensor([[[7.3928e+01, 1.5141e+02, 8.7631e-01],\n",
      "         [7.6093e+01, 1.4904e+02, 9.1240e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8124e-01],\n",
      "         [8.2643e+01, 1.5055e+02, 8.5611e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.3831e-02],\n",
      "         [8.7287e+01, 1.6431e+02, 9.8718e-01],\n",
      "         [8.5443e+01, 1.6162e+02, 8.3872e-01],\n",
      "         [9.2273e+01, 1.8438e+02, 9.8165e-01],\n",
      "         [8.9520e+01, 1.8027e+02, 5.5980e-01],\n",
      "         [7.8902e+01, 1.9038e+02, 9.6997e-01],\n",
      "         [8.3483e+01, 1.9188e+02, 6.3348e-01],\n",
      "         [9.5761e+01, 1.9604e+02, 9.9607e-01],\n",
      "         [9.3511e+01, 1.9412e+02, 9.8589e-01],\n",
      "         [8.6329e+01, 2.2351e+02, 9.9594e-01],\n",
      "         [8.2947e+01, 2.1895e+02, 9.8318e-01],\n",
      "         [1.0863e+02, 2.4687e+02, 9.8889e-01],\n",
      "         [8.7174e+01, 2.4299e+02, 9.7218e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 73.9279, 151.4066],\n",
      "         [ 76.0925, 149.0425],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 82.6432, 150.5542],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 87.2866, 164.3102],\n",
      "         [ 85.4431, 161.6237],\n",
      "         [ 92.2728, 184.3817],\n",
      "         [ 89.5204, 180.2663],\n",
      "         [ 78.9023, 190.3821],\n",
      "         [ 83.4825, 191.8838],\n",
      "         [ 95.7609, 196.0423],\n",
      "         [ 93.5108, 194.1180],\n",
      "         [ 86.3293, 223.5087],\n",
      "         [ 82.9469, 218.9496],\n",
      "         [108.6298, 246.8671],\n",
      "         [ 87.1737, 242.9869]]])\n",
      "xyn: tensor([[[0.2054, 0.4206],\n",
      "         [0.2114, 0.4140],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2296, 0.4182],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2425, 0.4564],\n",
      "         [0.2373, 0.4490],\n",
      "         [0.2563, 0.5122],\n",
      "         [0.2487, 0.5007],\n",
      "         [0.2192, 0.5288],\n",
      "         [0.2319, 0.5330],\n",
      "         [0.2660, 0.5446],\n",
      "         [0.2598, 0.5392],\n",
      "         [0.2398, 0.6209],\n",
      "         [0.2304, 0.6082],\n",
      "         [0.3017, 0.6857],\n",
      "         [0.2421, 0.6750]]])\n",
      "\n",
      "0: 640x640 1 person, 134.5ms\n",
      "Speed: 1.2ms preprocess, 134.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2447834014892578, 'inference': 134.51123237609863, 'postprocess': 0.5047321319580078}]\n",
      "Bounding Box: tensor([[ 95.0000, 199.5000,  46.0000, 119.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8434, 0.8884, 0.3204, 0.8427, 0.0869, 0.9868, 0.8304, 0.9820, 0.5554, 0.9703, 0.6225, 0.9969, 0.9884, 0.9968, 0.9863, 0.9920, 0.9792]])\n",
      "data: tensor([[[7.6283e+01, 1.5208e+02, 8.4339e-01],\n",
      "         [7.8248e+01, 1.4976e+02, 8.8841e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2042e-01],\n",
      "         [8.3922e+01, 1.5139e+02, 8.4270e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.6941e-02],\n",
      "         [8.9711e+01, 1.6499e+02, 9.8679e-01],\n",
      "         [8.6035e+01, 1.6374e+02, 8.3038e-01],\n",
      "         [9.7531e+01, 1.8285e+02, 9.8199e-01],\n",
      "         [9.0076e+01, 1.8344e+02, 5.5539e-01],\n",
      "         [8.5965e+01, 1.9287e+02, 9.7029e-01],\n",
      "         [8.5415e+01, 1.9446e+02, 6.2252e-01],\n",
      "         [9.7083e+01, 1.9798e+02, 9.9688e-01],\n",
      "         [9.3854e+01, 1.9625e+02, 9.8842e-01],\n",
      "         [8.7279e+01, 2.2374e+02, 9.9683e-01],\n",
      "         [7.9637e+01, 2.1781e+02, 9.8627e-01],\n",
      "         [1.1020e+02, 2.4604e+02, 9.9197e-01],\n",
      "         [8.7102e+01, 2.4271e+02, 9.7923e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 76.2831, 152.0799],\n",
      "         [ 78.2476, 149.7617],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 83.9223, 151.3851],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 89.7107, 164.9880],\n",
      "         [ 86.0349, 163.7427],\n",
      "         [ 97.5310, 182.8453],\n",
      "         [ 90.0763, 183.4439],\n",
      "         [ 85.9646, 192.8664],\n",
      "         [ 85.4148, 194.4601],\n",
      "         [ 97.0831, 197.9800],\n",
      "         [ 93.8536, 196.2510],\n",
      "         [ 87.2789, 223.7429],\n",
      "         [ 79.6370, 217.8081],\n",
      "         [110.2045, 246.0387],\n",
      "         [ 87.1017, 242.7137]]])\n",
      "xyn: tensor([[[0.2119, 0.4224],\n",
      "         [0.2174, 0.4160],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2331, 0.4205],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2492, 0.4583],\n",
      "         [0.2390, 0.4548],\n",
      "         [0.2709, 0.5079],\n",
      "         [0.2502, 0.5096],\n",
      "         [0.2388, 0.5357],\n",
      "         [0.2373, 0.5402],\n",
      "         [0.2697, 0.5499],\n",
      "         [0.2607, 0.5451],\n",
      "         [0.2424, 0.6215],\n",
      "         [0.2212, 0.6050],\n",
      "         [0.3061, 0.6834],\n",
      "         [0.2419, 0.6742]]])\n",
      "\n",
      "0: 640x640 1 person, 147.6ms\n",
      "Speed: 1.7ms preprocess, 147.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.682281494140625, 'inference': 147.60899543762207, 'postprocess': 0.49996376037597656}]\n",
      "Bounding Box: tensor([[ 96.5000, 200.5000,  49.0000, 117.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8645, 0.9056, 0.3755, 0.8314, 0.0858, 0.9839, 0.7950, 0.9809, 0.5331, 0.9726, 0.6277, 0.9962, 0.9852, 0.9966, 0.9843, 0.9930, 0.9814]])\n",
      "data: tensor([[[8.0096e+01, 1.5251e+02, 8.6449e-01],\n",
      "         [8.2728e+01, 1.5068e+02, 9.0560e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7552e-01],\n",
      "         [8.8882e+01, 1.5376e+02, 8.3137e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.5832e-02],\n",
      "         [9.4374e+01, 1.6741e+02, 9.8390e-01],\n",
      "         [8.7195e+01, 1.6533e+02, 7.9501e-01],\n",
      "         [1.0010e+02, 1.8287e+02, 9.8088e-01],\n",
      "         [9.1968e+01, 1.8312e+02, 5.3310e-01],\n",
      "         [8.7683e+01, 1.9236e+02, 9.7262e-01],\n",
      "         [8.7904e+01, 1.9422e+02, 6.2769e-01],\n",
      "         [9.9372e+01, 1.9954e+02, 9.9618e-01],\n",
      "         [9.3334e+01, 1.9725e+02, 9.8522e-01],\n",
      "         [8.9705e+01, 2.2374e+02, 9.9655e-01],\n",
      "         [7.7019e+01, 2.1649e+02, 9.8432e-01],\n",
      "         [1.1226e+02, 2.4689e+02, 9.9301e-01],\n",
      "         [8.6492e+01, 2.4232e+02, 9.8137e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 80.0957, 152.5143],\n",
      "         [ 82.7277, 150.6760],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 88.8816, 153.7564],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 94.3741, 167.4108],\n",
      "         [ 87.1949, 165.3342],\n",
      "         [100.0982, 182.8715],\n",
      "         [ 91.9677, 183.1225],\n",
      "         [ 87.6830, 192.3557],\n",
      "         [ 87.9040, 194.2171],\n",
      "         [ 99.3719, 199.5446],\n",
      "         [ 93.3337, 197.2478],\n",
      "         [ 89.7052, 223.7374],\n",
      "         [ 77.0195, 216.4923],\n",
      "         [112.2598, 246.8941],\n",
      "         [ 86.4923, 242.3249]]])\n",
      "xyn: tensor([[[0.2225, 0.4237],\n",
      "         [0.2298, 0.4185],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2469, 0.4271],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2622, 0.4650],\n",
      "         [0.2422, 0.4593],\n",
      "         [0.2781, 0.5080],\n",
      "         [0.2555, 0.5087],\n",
      "         [0.2436, 0.5343],\n",
      "         [0.2442, 0.5395],\n",
      "         [0.2760, 0.5543],\n",
      "         [0.2593, 0.5479],\n",
      "         [0.2492, 0.6215],\n",
      "         [0.2139, 0.6014],\n",
      "         [0.3118, 0.6858],\n",
      "         [0.2403, 0.6731]]])\n",
      "\n",
      "0: 640x640 1 person, 157.3ms\n",
      "Speed: 1.5ms preprocess, 157.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4641284942626953, 'inference': 157.31096267700195, 'postprocess': 0.4401206970214844}]\n",
      "Bounding Box: tensor([[ 97.0000, 204.5000,  52.0000, 123.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9616, 0.9355, 0.8202, 0.7665, 0.4006, 0.9916, 0.9770, 0.9834, 0.9286, 0.9761, 0.9265, 0.9976, 0.9959, 0.9959, 0.9917, 0.9879, 0.9811]])\n",
      "data: tensor([[[ 87.1302, 154.6437,   0.9616],\n",
      "         [ 89.9795, 152.5392,   0.9355],\n",
      "         [ 85.6906, 151.9048,   0.8202],\n",
      "         [ 94.2439, 155.1228,   0.7665],\n",
      "         [  0.0000,   0.0000,   0.4006],\n",
      "         [ 99.4974, 169.0031,   0.9916],\n",
      "         [ 84.1547, 166.6887,   0.9770],\n",
      "         [106.5377, 186.8356,   0.9834],\n",
      "         [ 84.2756, 182.5318,   0.9286],\n",
      "         [ 94.0786, 192.0770,   0.9761],\n",
      "         [ 79.2977, 189.8447,   0.9265],\n",
      "         [102.0093, 200.4503,   0.9976],\n",
      "         [ 90.5600, 197.9420,   0.9959],\n",
      "         [ 95.3001, 223.4945,   0.9959],\n",
      "         [ 77.8707, 217.3517,   0.9917],\n",
      "         [113.7070, 250.5186,   0.9879],\n",
      "         [ 85.8173, 241.9066,   0.9811]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 87.1302, 154.6437],\n",
      "         [ 89.9795, 152.5392],\n",
      "         [ 85.6906, 151.9048],\n",
      "         [ 94.2439, 155.1228],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 99.4974, 169.0031],\n",
      "         [ 84.1547, 166.6887],\n",
      "         [106.5377, 186.8356],\n",
      "         [ 84.2756, 182.5318],\n",
      "         [ 94.0786, 192.0770],\n",
      "         [ 79.2977, 189.8447],\n",
      "         [102.0093, 200.4503],\n",
      "         [ 90.5600, 197.9420],\n",
      "         [ 95.3001, 223.4945],\n",
      "         [ 77.8707, 217.3517],\n",
      "         [113.7070, 250.5186],\n",
      "         [ 85.8173, 241.9066]]])\n",
      "xyn: tensor([[[0.2420, 0.4296],\n",
      "         [0.2499, 0.4237],\n",
      "         [0.2380, 0.4220],\n",
      "         [0.2618, 0.4309],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2764, 0.4695],\n",
      "         [0.2338, 0.4630],\n",
      "         [0.2959, 0.5190],\n",
      "         [0.2341, 0.5070],\n",
      "         [0.2613, 0.5335],\n",
      "         [0.2203, 0.5273],\n",
      "         [0.2834, 0.5568],\n",
      "         [0.2516, 0.5498],\n",
      "         [0.2647, 0.6208],\n",
      "         [0.2163, 0.6038],\n",
      "         [0.3159, 0.6959],\n",
      "         [0.2384, 0.6720]]])\n",
      "\n",
      "0: 640x640 1 person, 168.6ms\n",
      "Speed: 1.3ms preprocess, 168.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [178, 161, 144]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [178, 161, 144]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3310909271240234, 'inference': 168.593168258667, 'postprocess': 0.6568431854248047}]\n",
      "Bounding Box: tensor([[ 91.5000, 197.5000,  51.0000, 109.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2548, 0.1658, 0.0787, 0.6210, 0.4029, 0.9751, 0.9732, 0.8930, 0.8319, 0.7154, 0.5997, 0.9954, 0.9950, 0.9889, 0.9862, 0.9641, 0.9569]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5482e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6576e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.8712e-02],\n",
      "         [8.8779e+01, 1.5250e+02, 6.2097e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0290e-01],\n",
      "         [8.4844e+01, 1.6370e+02, 9.7509e-01],\n",
      "         [1.0367e+02, 1.6634e+02, 9.7316e-01],\n",
      "         [7.9928e+01, 1.7854e+02, 8.9300e-01],\n",
      "         [1.0902e+02, 1.8552e+02, 8.3193e-01],\n",
      "         [7.3303e+01, 1.8670e+02, 7.1538e-01],\n",
      "         [1.0510e+02, 2.0646e+02, 5.9974e-01],\n",
      "         [9.0870e+01, 1.9639e+02, 9.9539e-01],\n",
      "         [1.0487e+02, 1.9777e+02, 9.9500e-01],\n",
      "         [7.7115e+01, 2.1497e+02, 9.8895e-01],\n",
      "         [1.0106e+02, 2.2134e+02, 9.8619e-01],\n",
      "         [8.5522e+01, 2.3987e+02, 9.6414e-01],\n",
      "         [1.1015e+02, 2.4438e+02, 9.5690e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 88.7787, 152.5031],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 84.8441, 163.7045],\n",
      "         [103.6742, 166.3428],\n",
      "         [ 79.9280, 178.5379],\n",
      "         [109.0239, 185.5152],\n",
      "         [ 73.3031, 186.6984],\n",
      "         [105.0967, 206.4611],\n",
      "         [ 90.8696, 196.3931],\n",
      "         [104.8703, 197.7690],\n",
      "         [ 77.1149, 214.9750],\n",
      "         [101.0604, 221.3437],\n",
      "         [ 85.5225, 239.8713],\n",
      "         [110.1485, 244.3820]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2466, 0.4236],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2357, 0.4547],\n",
      "         [0.2880, 0.4621],\n",
      "         [0.2220, 0.4959],\n",
      "         [0.3028, 0.5153],\n",
      "         [0.2036, 0.5186],\n",
      "         [0.2919, 0.5735],\n",
      "         [0.2524, 0.5455],\n",
      "         [0.2913, 0.5494],\n",
      "         [0.2142, 0.5972],\n",
      "         [0.2807, 0.6148],\n",
      "         [0.2376, 0.6663],\n",
      "         [0.3060, 0.6788]]])\n",
      "\n",
      "0: 640x640 1 person, 160.7ms\n",
      "Speed: 1.7ms preprocess, 160.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        [151, 173, 197],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.7032623291015625, 'inference': 160.73203086853027, 'postprocess': 0.5791187286376953}]\n",
      "Bounding Box: tensor([[ 91.0000, 202.5000,  62.0000, 121.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9823, 0.9344, 0.9387, 0.6911, 0.7423, 0.9948, 0.9973, 0.9839, 0.9917, 0.9793, 0.9871, 0.9991, 0.9993, 0.9974, 0.9976, 0.9870, 0.9878]])\n",
      "data: tensor([[[ 99.2234, 151.9515,   0.9823],\n",
      "         [102.1315, 149.9352,   0.9344],\n",
      "         [ 97.5354, 149.2796,   0.9387],\n",
      "         [104.7526, 151.9419,   0.6911],\n",
      "         [ 93.4117, 150.1636,   0.7423],\n",
      "         [108.0536, 166.4726,   0.9948],\n",
      "         [ 87.2814, 163.2681,   0.9973],\n",
      "         [114.4865, 184.2590,   0.9839],\n",
      "         [ 78.6706, 175.3613,   0.9917],\n",
      "         [103.7503, 190.3519,   0.9793],\n",
      "         [ 67.8516, 183.9996,   0.9871],\n",
      "         [105.0406, 198.0952,   0.9991],\n",
      "         [ 90.8551, 195.5156,   0.9993],\n",
      "         [102.9565, 220.4674,   0.9974],\n",
      "         [ 79.0593, 216.5946,   0.9976],\n",
      "         [112.9912, 248.6959,   0.9870],\n",
      "         [ 84.9273, 241.8756,   0.9878]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 99.2234, 151.9515],\n",
      "         [102.1315, 149.9352],\n",
      "         [ 97.5354, 149.2796],\n",
      "         [104.7526, 151.9419],\n",
      "         [ 93.4117, 150.1636],\n",
      "         [108.0536, 166.4726],\n",
      "         [ 87.2814, 163.2681],\n",
      "         [114.4865, 184.2590],\n",
      "         [ 78.6706, 175.3613],\n",
      "         [103.7503, 190.3519],\n",
      "         [ 67.8516, 183.9996],\n",
      "         [105.0406, 198.0952],\n",
      "         [ 90.8551, 195.5156],\n",
      "         [102.9565, 220.4674],\n",
      "         [ 79.0593, 216.5946],\n",
      "         [112.9912, 248.6959],\n",
      "         [ 84.9273, 241.8756]]])\n",
      "xyn: tensor([[[0.2756, 0.4221],\n",
      "         [0.2837, 0.4165],\n",
      "         [0.2709, 0.4147],\n",
      "         [0.2910, 0.4221],\n",
      "         [0.2595, 0.4171],\n",
      "         [0.3001, 0.4624],\n",
      "         [0.2424, 0.4535],\n",
      "         [0.3180, 0.5118],\n",
      "         [0.2185, 0.4871],\n",
      "         [0.2882, 0.5288],\n",
      "         [0.1885, 0.5111],\n",
      "         [0.2918, 0.5503],\n",
      "         [0.2524, 0.5431],\n",
      "         [0.2860, 0.6124],\n",
      "         [0.2196, 0.6017],\n",
      "         [0.3139, 0.6908],\n",
      "         [0.2359, 0.6719]]])\n",
      "\n",
      "0: 640x640 1 person, 151.9ms\n",
      "Speed: 1.4ms preprocess, 151.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3899803161621094, 'inference': 151.88980102539062, 'postprocess': 0.42891502380371094}]\n",
      "Bounding Box: tensor([[ 89., 199.,  66., 114.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2279, 0.1541, 0.0568, 0.7134, 0.4762, 0.9945, 0.9722, 0.9801, 0.7950, 0.9076, 0.5985, 0.9988, 0.9971, 0.9969, 0.9913, 0.9875, 0.9718]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2787e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5413e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.6766e-02],\n",
      "         [1.0046e+02, 1.5176e+02, 7.1336e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7622e-01],\n",
      "         [9.1815e+01, 1.6085e+02, 9.9453e-01],\n",
      "         [1.1371e+02, 1.6623e+02, 9.7218e-01],\n",
      "         [7.7799e+01, 1.7095e+02, 9.8011e-01],\n",
      "         [1.1608e+02, 1.8378e+02, 7.9505e-01],\n",
      "         [6.4034e+01, 1.7907e+02, 9.0755e-01],\n",
      "         [1.0998e+02, 1.8845e+02, 5.9849e-01],\n",
      "         [9.4408e+01, 1.9603e+02, 9.9876e-01],\n",
      "         [1.1010e+02, 1.9785e+02, 9.9706e-01],\n",
      "         [8.1019e+01, 2.1594e+02, 9.9692e-01],\n",
      "         [1.0847e+02, 2.2012e+02, 9.9128e-01],\n",
      "         [8.5064e+01, 2.4084e+02, 9.8746e-01],\n",
      "         [1.1165e+02, 2.4775e+02, 9.7180e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.4632, 151.7613],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 91.8150, 160.8537],\n",
      "         [113.7067, 166.2253],\n",
      "         [ 77.7994, 170.9512],\n",
      "         [116.0750, 183.7811],\n",
      "         [ 64.0335, 179.0688],\n",
      "         [109.9773, 188.4510],\n",
      "         [ 94.4080, 196.0258],\n",
      "         [110.1005, 197.8483],\n",
      "         [ 81.0193, 215.9426],\n",
      "         [108.4742, 220.1181],\n",
      "         [ 85.0639, 240.8365],\n",
      "         [111.6455, 247.7516]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2791, 0.4216],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2550, 0.4468],\n",
      "         [0.3159, 0.4617],\n",
      "         [0.2161, 0.4749],\n",
      "         [0.3224, 0.5105],\n",
      "         [0.1779, 0.4974],\n",
      "         [0.3055, 0.5235],\n",
      "         [0.2622, 0.5445],\n",
      "         [0.3058, 0.5496],\n",
      "         [0.2251, 0.5998],\n",
      "         [0.3013, 0.6114],\n",
      "         [0.2363, 0.6690],\n",
      "         [0.3101, 0.6882]]])\n",
      "\n",
      "0: 640x640 1 person, 122.8ms\n",
      "Speed: 1.4ms preprocess, 122.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3527870178222656, 'inference': 122.77579307556152, 'postprocess': 0.4642009735107422}]\n",
      "Bounding Box: tensor([[ 91., 208.,  72., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5087, 0.2686, 0.2481, 0.4711, 0.6053, 0.9891, 0.9768, 0.9672, 0.8950, 0.9169, 0.8314, 0.9985, 0.9977, 0.9978, 0.9962, 0.9937, 0.9892]])\n",
      "data: tensor([[[1.1701e+02, 1.5058e+02, 5.0870e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6856e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4814e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7106e-01],\n",
      "         [1.1136e+02, 1.5125e+02, 6.0531e-01],\n",
      "         [9.8270e+01, 1.6414e+02, 9.8912e-01],\n",
      "         [1.1388e+02, 1.6427e+02, 9.7682e-01],\n",
      "         [7.7882e+01, 1.7309e+02, 9.6717e-01],\n",
      "         [1.1132e+02, 1.7460e+02, 8.9503e-01],\n",
      "         [6.2118e+01, 1.7746e+02, 9.1687e-01],\n",
      "         [1.0685e+02, 1.8463e+02, 8.3142e-01],\n",
      "         [9.3926e+01, 1.9913e+02, 9.9847e-01],\n",
      "         [1.0588e+02, 1.9990e+02, 9.9771e-01],\n",
      "         [8.2123e+01, 2.1603e+02, 9.9775e-01],\n",
      "         [1.1150e+02, 2.2350e+02, 9.9618e-01],\n",
      "         [7.9526e+01, 2.3863e+02, 9.9370e-01],\n",
      "         [1.1076e+02, 2.5818e+02, 9.8915e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[117.0132, 150.5775],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.3649, 151.2463],\n",
      "         [ 98.2702, 164.1410],\n",
      "         [113.8798, 164.2733],\n",
      "         [ 77.8822, 173.0856],\n",
      "         [111.3182, 174.5975],\n",
      "         [ 62.1179, 177.4630],\n",
      "         [106.8451, 184.6290],\n",
      "         [ 93.9257, 199.1343],\n",
      "         [105.8767, 199.8954],\n",
      "         [ 82.1230, 216.0316],\n",
      "         [111.4956, 223.4953],\n",
      "         [ 79.5264, 238.6312],\n",
      "         [110.7598, 258.1799]]])\n",
      "xyn: tensor([[[0.3250, 0.4183],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3093, 0.4201],\n",
      "         [0.2730, 0.4559],\n",
      "         [0.3163, 0.4563],\n",
      "         [0.2163, 0.4808],\n",
      "         [0.3092, 0.4850],\n",
      "         [0.1725, 0.4930],\n",
      "         [0.2968, 0.5129],\n",
      "         [0.2609, 0.5532],\n",
      "         [0.2941, 0.5553],\n",
      "         [0.2281, 0.6001],\n",
      "         [0.3097, 0.6208],\n",
      "         [0.2209, 0.6629],\n",
      "         [0.3077, 0.7172]]])\n",
      "\n",
      "0: 640x640 1 person, 133.5ms\n",
      "Speed: 1.4ms preprocess, 133.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        [150, 172, 196],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3544559478759766, 'inference': 133.49103927612305, 'postprocess': 0.45299530029296875}]\n",
      "Bounding Box: tensor([[ 91., 207.,  76., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6131, 0.4122, 0.2147, 0.6313, 0.3004, 0.9936, 0.9940, 0.9879, 0.9845, 0.9669, 0.9566, 0.9997, 0.9997, 0.9982, 0.9977, 0.9881, 0.9872]])\n",
      "data: tensor([[[9.8090e+01, 1.6348e+02, 6.1310e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1223e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1468e-01],\n",
      "         [1.0297e+02, 1.5937e+02, 6.3131e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0044e-01],\n",
      "         [1.1473e+02, 1.6451e+02, 9.9356e-01],\n",
      "         [9.2688e+01, 1.7037e+02, 9.9400e-01],\n",
      "         [1.2036e+02, 1.5234e+02, 9.8792e-01],\n",
      "         [7.6306e+01, 1.6776e+02, 9.8446e-01],\n",
      "         [1.1794e+02, 1.4864e+02, 9.6693e-01],\n",
      "         [5.9743e+01, 1.6953e+02, 9.5663e-01],\n",
      "         [1.1011e+02, 1.9847e+02, 9.9968e-01],\n",
      "         [9.3894e+01, 1.9745e+02, 9.9966e-01],\n",
      "         [1.1546e+02, 2.2628e+02, 9.9822e-01],\n",
      "         [8.2999e+01, 2.1551e+02, 9.9771e-01],\n",
      "         [1.1007e+02, 2.5574e+02, 9.8806e-01],\n",
      "         [7.6786e+01, 2.3841e+02, 9.8720e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 98.0897, 163.4817],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.9717, 159.3682],\n",
      "         [  0.0000,   0.0000],\n",
      "         [114.7304, 164.5099],\n",
      "         [ 92.6879, 170.3669],\n",
      "         [120.3561, 152.3360],\n",
      "         [ 76.3058, 167.7634],\n",
      "         [117.9364, 148.6407],\n",
      "         [ 59.7426, 169.5255],\n",
      "         [110.1054, 198.4742],\n",
      "         [ 93.8944, 197.4521],\n",
      "         [115.4577, 226.2833],\n",
      "         [ 82.9988, 215.5102],\n",
      "         [110.0712, 255.7383],\n",
      "         [ 76.7863, 238.4086]]])\n",
      "xyn: tensor([[[0.2725, 0.4541],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2860, 0.4427],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3187, 0.4570],\n",
      "         [0.2575, 0.4732],\n",
      "         [0.3343, 0.4232],\n",
      "         [0.2120, 0.4660],\n",
      "         [0.3276, 0.4129],\n",
      "         [0.1660, 0.4709],\n",
      "         [0.3058, 0.5513],\n",
      "         [0.2608, 0.5485],\n",
      "         [0.3207, 0.6286],\n",
      "         [0.2306, 0.5986],\n",
      "         [0.3058, 0.7104],\n",
      "         [0.2133, 0.6622]]])\n",
      "\n",
      "0: 640x640 1 person, 159.4ms\n",
      "Speed: 1.3ms preprocess, 159.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        [148, 171, 195],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [174, 158, 140],\n",
      "        [174, 158, 140]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [172, 155, 138],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3089179992675781, 'inference': 159.40618515014648, 'postprocess': 0.4239082336425781}]\n",
      "Bounding Box: tensor([[ 92.0000, 207.5000,  84.0000, 129.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8672, 0.4863, 0.7579, 0.2943, 0.7963, 0.9899, 0.9980, 0.9716, 0.9956, 0.9548, 0.9888, 0.9993, 0.9996, 0.9970, 0.9980, 0.9802, 0.9857]])\n",
      "data: tensor([[[126.0372, 155.0806,   0.8672],\n",
      "         [  0.0000,   0.0000,   0.4863],\n",
      "         [124.7354, 152.2947,   0.7579],\n",
      "         [  0.0000,   0.0000,   0.2943],\n",
      "         [117.8800, 151.6660,   0.7963],\n",
      "         [126.4225, 171.0049,   0.9899],\n",
      "         [102.8269, 161.4195,   0.9980],\n",
      "         [124.2778, 188.5014,   0.9716],\n",
      "         [ 80.5870, 162.9651,   0.9956],\n",
      "         [114.7597, 184.1292,   0.9548],\n",
      "         [ 59.3921, 168.4682,   0.9888],\n",
      "         [111.5981, 200.0761,   0.9993],\n",
      "         [ 94.8949, 194.1398,   0.9996],\n",
      "         [118.0865, 227.6427,   0.9970],\n",
      "         [ 84.6744, 218.0304,   0.9980],\n",
      "         [106.7154, 259.9569,   0.9802],\n",
      "         [ 77.1831, 238.8010,   0.9857]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[126.0372, 155.0806],\n",
      "         [  0.0000,   0.0000],\n",
      "         [124.7354, 152.2947],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.8800, 151.6660],\n",
      "         [126.4225, 171.0049],\n",
      "         [102.8269, 161.4195],\n",
      "         [124.2778, 188.5014],\n",
      "         [ 80.5870, 162.9651],\n",
      "         [114.7597, 184.1292],\n",
      "         [ 59.3921, 168.4682],\n",
      "         [111.5981, 200.0761],\n",
      "         [ 94.8949, 194.1398],\n",
      "         [118.0865, 227.6427],\n",
      "         [ 84.6744, 218.0304],\n",
      "         [106.7154, 259.9569],\n",
      "         [ 77.1831, 238.8010]]])\n",
      "xyn: tensor([[[0.3501, 0.4308],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3465, 0.4230],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3274, 0.4213],\n",
      "         [0.3512, 0.4750],\n",
      "         [0.2856, 0.4484],\n",
      "         [0.3452, 0.5236],\n",
      "         [0.2239, 0.4527],\n",
      "         [0.3188, 0.5115],\n",
      "         [0.1650, 0.4680],\n",
      "         [0.3100, 0.5558],\n",
      "         [0.2636, 0.5393],\n",
      "         [0.3280, 0.6323],\n",
      "         [0.2352, 0.6056],\n",
      "         [0.2964, 0.7221],\n",
      "         [0.2144, 0.6633]]])\n",
      "\n",
      "0: 640x640 1 person, 151.0ms\n",
      "Speed: 1.6ms preprocess, 151.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        [147, 169, 194],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 158, 140],\n",
      "        [174, 158, 140],\n",
      "        [175, 159, 141]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [173, 157, 139],\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.6169548034667969, 'inference': 151.01385116577148, 'postprocess': 0.4451274871826172}]\n",
      "Bounding Box: tensor([[ 93., 210.,  84., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6517, 0.1912, 0.5055, 0.0912, 0.6224, 0.9684, 0.9940, 0.9585, 0.9940, 0.9464, 0.9881, 0.9988, 0.9994, 0.9962, 0.9976, 0.9803, 0.9862]])\n",
      "data: tensor([[[1.1852e+02, 1.6760e+02, 6.5173e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9124e-01],\n",
      "         [1.1664e+02, 1.6568e+02, 5.0546e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.1211e-02],\n",
      "         [1.0874e+02, 1.6444e+02, 6.2239e-01],\n",
      "         [1.2079e+02, 1.7745e+02, 9.6839e-01],\n",
      "         [9.7233e+01, 1.6987e+02, 9.9400e-01],\n",
      "         [1.2755e+02, 1.8477e+02, 9.5850e-01],\n",
      "         [7.8294e+01, 1.6529e+02, 9.9405e-01],\n",
      "         [1.2635e+02, 1.6976e+02, 9.4636e-01],\n",
      "         [5.8723e+01, 1.6862e+02, 9.8812e-01],\n",
      "         [1.1266e+02, 2.0344e+02, 9.9881e-01],\n",
      "         [9.6328e+01, 1.9855e+02, 9.9941e-01],\n",
      "         [1.1798e+02, 2.2778e+02, 9.9619e-01],\n",
      "         [8.5496e+01, 2.1844e+02, 9.9758e-01],\n",
      "         [1.0571e+02, 2.6100e+02, 9.8034e-01],\n",
      "         [7.5718e+01, 2.3806e+02, 9.8618e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[118.5155, 167.6016],\n",
      "         [  0.0000,   0.0000],\n",
      "         [116.6373, 165.6782],\n",
      "         [  0.0000,   0.0000],\n",
      "         [108.7410, 164.4406],\n",
      "         [120.7883, 177.4452],\n",
      "         [ 97.2333, 169.8748],\n",
      "         [127.5491, 184.7748],\n",
      "         [ 78.2941, 165.2868],\n",
      "         [126.3547, 169.7621],\n",
      "         [ 58.7233, 168.6201],\n",
      "         [112.6602, 203.4385],\n",
      "         [ 96.3283, 198.5508],\n",
      "         [117.9773, 227.7807],\n",
      "         [ 85.4965, 218.4423],\n",
      "         [105.7135, 261.0049],\n",
      "         [ 75.7177, 238.0631]]])\n",
      "xyn: tensor([[[0.3292, 0.4656],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3240, 0.4602],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3021, 0.4568],\n",
      "         [0.3355, 0.4929],\n",
      "         [0.2701, 0.4719],\n",
      "         [0.3543, 0.5133],\n",
      "         [0.2175, 0.4591],\n",
      "         [0.3510, 0.4716],\n",
      "         [0.1631, 0.4684],\n",
      "         [0.3129, 0.5651],\n",
      "         [0.2676, 0.5515],\n",
      "         [0.3277, 0.6327],\n",
      "         [0.2375, 0.6068],\n",
      "         [0.2936, 0.7250],\n",
      "         [0.2103, 0.6613]]])\n",
      "\n",
      "0: 640x640 1 person, 147.1ms\n",
      "Speed: 1.8ms preprocess, 147.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[145, 167, 192],\n",
      "        [145, 167, 192],\n",
      "        [145, 167, 192],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [175, 159, 141],\n",
      "        [176, 160, 143],\n",
      "        [176, 160, 143]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [176, 160, 143],\n",
      "        [178, 161, 144],\n",
      "        [178, 161, 144]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [179, 162, 145],\n",
      "        [178, 161, 144],\n",
      "        [176, 160, 143]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.8410682678222656, 'inference': 147.06683158874512, 'postprocess': 0.41604042053222656}]\n",
      "Bounding Box: tensor([[ 95.5000, 212.0000,  85.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4842, 0.1327, 0.3120, 0.1092, 0.5410, 0.9713, 0.9916, 0.9651, 0.9905, 0.9448, 0.9785, 0.9992, 0.9995, 0.9969, 0.9975, 0.9816, 0.9842]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.8423e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3270e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1197e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0917e-01],\n",
      "         [1.2408e+02, 1.6024e+02, 5.4096e-01],\n",
      "         [1.2814e+02, 1.7383e+02, 9.7133e-01],\n",
      "         [1.0797e+02, 1.6518e+02, 9.9162e-01],\n",
      "         [1.2863e+02, 1.8259e+02, 9.6510e-01],\n",
      "         [8.6290e+01, 1.6478e+02, 9.9046e-01],\n",
      "         [1.3023e+02, 1.7067e+02, 9.4477e-01],\n",
      "         [6.2555e+01, 1.6895e+02, 9.7853e-01],\n",
      "         [1.1299e+02, 2.0239e+02, 9.9921e-01],\n",
      "         [9.7860e+01, 1.9708e+02, 9.9950e-01],\n",
      "         [1.1779e+02, 2.2770e+02, 9.9690e-01],\n",
      "         [8.5608e+01, 2.2046e+02, 9.9748e-01],\n",
      "         [1.0601e+02, 2.5958e+02, 9.8165e-01],\n",
      "         [7.3585e+01, 2.3952e+02, 9.8416e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [124.0751, 160.2357],\n",
      "         [128.1353, 173.8348],\n",
      "         [107.9732, 165.1848],\n",
      "         [128.6344, 182.5883],\n",
      "         [ 86.2897, 164.7792],\n",
      "         [130.2262, 170.6682],\n",
      "         [ 62.5546, 168.9498],\n",
      "         [112.9862, 202.3891],\n",
      "         [ 97.8597, 197.0826],\n",
      "         [117.7947, 227.6969],\n",
      "         [ 85.6076, 220.4626],\n",
      "         [106.0114, 259.5818],\n",
      "         [ 73.5845, 239.5240]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3447, 0.4451],\n",
      "         [0.3559, 0.4829],\n",
      "         [0.2999, 0.4588],\n",
      "         [0.3573, 0.5072],\n",
      "         [0.2397, 0.4577],\n",
      "         [0.3617, 0.4741],\n",
      "         [0.1738, 0.4693],\n",
      "         [0.3139, 0.5622],\n",
      "         [0.2718, 0.5475],\n",
      "         [0.3272, 0.6325],\n",
      "         [0.2378, 0.6124],\n",
      "         [0.2945, 0.7211],\n",
      "         [0.2044, 0.6653]]])\n",
      "\n",
      "0: 640x640 1 person, 160.0ms\n",
      "Speed: 1.3ms preprocess, 160.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        [146, 168, 193],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [160, 144, 126],\n",
      "        [143, 126, 109],\n",
      "        [136, 119, 102]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [161, 145, 127],\n",
      "        [144, 127, 110],\n",
      "        [138, 122, 104]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [164, 147, 130],\n",
      "        [145, 129, 111],\n",
      "        [139, 123, 105]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2881755828857422, 'inference': 159.99484062194824, 'postprocess': 0.5369186401367188}]\n",
      "Bounding Box: tensor([[103.5000, 210.0000,  83.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4115, 0.1178, 0.3642, 0.1242, 0.6283, 0.8860, 0.9924, 0.6494, 0.9832, 0.4464, 0.9311, 0.9963, 0.9990, 0.9902, 0.9969, 0.9754, 0.9890]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.1155e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1781e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6424e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2424e-01],\n",
      "         [1.3031e+02, 1.5900e+02, 6.2833e-01],\n",
      "         [1.3173e+02, 1.7655e+02, 8.8600e-01],\n",
      "         [1.1449e+02, 1.6650e+02, 9.9240e-01],\n",
      "         [1.2286e+02, 1.9686e+02, 6.4943e-01],\n",
      "         [9.3040e+01, 1.6618e+02, 9.8322e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4642e-01],\n",
      "         [6.9655e+01, 1.6944e+02, 9.3115e-01],\n",
      "         [1.1411e+02, 2.0618e+02, 9.9627e-01],\n",
      "         [1.0012e+02, 2.0102e+02, 9.9903e-01],\n",
      "         [1.1949e+02, 2.3018e+02, 9.9023e-01],\n",
      "         [8.6406e+01, 2.2715e+02, 9.9692e-01],\n",
      "         [1.0277e+02, 2.5983e+02, 9.7537e-01],\n",
      "         [7.1209e+01, 2.4096e+02, 9.8899e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [130.3112, 158.9982],\n",
      "         [131.7269, 176.5477],\n",
      "         [114.4862, 166.4991],\n",
      "         [122.8559, 196.8613],\n",
      "         [ 93.0399, 166.1819],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.6553, 169.4438],\n",
      "         [114.1089, 206.1755],\n",
      "         [100.1201, 201.0249],\n",
      "         [119.4853, 230.1764],\n",
      "         [ 86.4062, 227.1512],\n",
      "         [102.7694, 259.8319],\n",
      "         [ 71.2089, 240.9600]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3620, 0.4417],\n",
      "         [0.3659, 0.4904],\n",
      "         [0.3180, 0.4625],\n",
      "         [0.3413, 0.5468],\n",
      "         [0.2584, 0.4616],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1935, 0.4707],\n",
      "         [0.3170, 0.5727],\n",
      "         [0.2781, 0.5584],\n",
      "         [0.3319, 0.6394],\n",
      "         [0.2400, 0.6310],\n",
      "         [0.2855, 0.7218],\n",
      "         [0.1978, 0.6693]]])\n",
      "\n",
      "0: 640x640 1 person, 146.8ms\n",
      "Speed: 1.4ms preprocess, 146.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [154, 171, 197],\n",
      "        [154, 171, 197]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [134, 111,  96],\n",
      "        [138, 115, 100],\n",
      "        [139, 116, 101]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [136, 116, 100],\n",
      "        [137, 117, 101],\n",
      "        [134, 115,  98]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [140, 120, 104],\n",
      "        [137, 117, 101],\n",
      "        [138, 118, 102]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.361846923828125, 'inference': 146.84200286865234, 'postprocess': 0.44989585876464844}]\n",
      "Bounding Box: tensor([[104.5000, 212.0000,  81.0000, 122.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2285, 0.0529, 0.2135, 0.1564, 0.6780, 0.8991, 0.9949, 0.5783, 0.9812, 0.2797, 0.8708, 0.9967, 0.9992, 0.9890, 0.9967, 0.9754, 0.9893]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2849e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.2905e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1346e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5637e-01],\n",
      "         [1.3295e+02, 1.6230e+02, 6.7799e-01],\n",
      "         [1.2933e+02, 1.7313e+02, 8.9910e-01],\n",
      "         [1.1704e+02, 1.6852e+02, 9.9492e-01],\n",
      "         [1.1396e+02, 1.8478e+02, 5.7827e-01],\n",
      "         [9.7610e+01, 1.6666e+02, 9.8122e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7966e-01],\n",
      "         [7.7191e+01, 1.7092e+02, 8.7081e-01],\n",
      "         [1.1331e+02, 2.0929e+02, 9.9674e-01],\n",
      "         [1.0181e+02, 2.0592e+02, 9.9925e-01],\n",
      "         [1.1750e+02, 2.3431e+02, 9.8900e-01],\n",
      "         [8.7762e+01, 2.3340e+02, 9.9674e-01],\n",
      "         [9.9929e+01, 2.6140e+02, 9.7536e-01],\n",
      "         [7.2442e+01, 2.4057e+02, 9.8928e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [132.9456, 162.3020],\n",
      "         [129.3330, 173.1270],\n",
      "         [117.0434, 168.5242],\n",
      "         [113.9565, 184.7837],\n",
      "         [ 97.6100, 166.6649],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.1908, 170.9226],\n",
      "         [113.3055, 209.2909],\n",
      "         [101.8142, 205.9231],\n",
      "         [117.5000, 234.3073],\n",
      "         [ 87.7624, 233.3966],\n",
      "         [ 99.9294, 261.4044],\n",
      "         [ 72.4418, 240.5682]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3693, 0.4508],\n",
      "         [0.3593, 0.4809],\n",
      "         [0.3251, 0.4681],\n",
      "         [0.3165, 0.5133],\n",
      "         [0.2711, 0.4630],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2144, 0.4748],\n",
      "         [0.3147, 0.5814],\n",
      "         [0.2828, 0.5720],\n",
      "         [0.3264, 0.6509],\n",
      "         [0.2438, 0.6483],\n",
      "         [0.2776, 0.7261],\n",
      "         [0.2012, 0.6682]]])\n",
      "\n",
      "0: 640x640 1 person, 119.3ms\n",
      "Speed: 1.3ms preprocess, 119.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201]],\n",
      "\n",
      "       [[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201]],\n",
      "\n",
      "       [[143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        ...,\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201],\n",
      "        [158, 174, 201]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [189, 182, 174],\n",
      "        [230, 222, 218],\n",
      "        [229, 221, 217]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [174, 159, 151],\n",
      "        [217, 202, 197],\n",
      "        [231, 216, 211]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [151, 136, 127],\n",
      "        [193, 178, 173],\n",
      "        [224, 209, 204]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2500286102294922, 'inference': 119.33302879333496, 'postprocess': 0.7190704345703125}]\n",
      "Bounding Box: tensor([[104., 215.,  80., 118.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2583, 0.0554, 0.2111, 0.0508, 0.4333, 0.8443, 0.9856, 0.7767, 0.9878, 0.7056, 0.9645, 0.9978, 0.9993, 0.9935, 0.9977, 0.9823, 0.9912]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5826e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.5400e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1107e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0806e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3331e-01],\n",
      "         [1.2514e+02, 1.7918e+02, 8.4426e-01],\n",
      "         [1.1042e+02, 1.7624e+02, 9.8557e-01],\n",
      "         [1.3040e+02, 1.7043e+02, 7.7666e-01],\n",
      "         [9.6832e+01, 1.7054e+02, 9.8782e-01],\n",
      "         [1.3505e+02, 1.6514e+02, 7.0557e-01],\n",
      "         [8.7274e+01, 1.7565e+02, 9.6452e-01],\n",
      "         [1.1325e+02, 2.1298e+02, 9.9781e-01],\n",
      "         [1.0150e+02, 2.1060e+02, 9.9933e-01],\n",
      "         [1.1217e+02, 2.3904e+02, 9.9353e-01],\n",
      "         [9.5906e+01, 2.3742e+02, 9.9769e-01],\n",
      "         [9.8195e+01, 2.6367e+02, 9.8226e-01],\n",
      "         [7.2357e+01, 2.4869e+02, 9.9120e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [125.1368, 179.1774],\n",
      "         [110.4157, 176.2366],\n",
      "         [130.3999, 170.4334],\n",
      "         [ 96.8322, 170.5399],\n",
      "         [135.0526, 165.1396],\n",
      "         [ 87.2735, 175.6544],\n",
      "         [113.2467, 212.9750],\n",
      "         [101.5043, 210.6019],\n",
      "         [112.1669, 239.0401],\n",
      "         [ 95.9064, 237.4220],\n",
      "         [ 98.1949, 263.6687],\n",
      "         [ 72.3573, 248.6907]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3476, 0.4977],\n",
      "         [0.3067, 0.4895],\n",
      "         [0.3622, 0.4734],\n",
      "         [0.2690, 0.4737],\n",
      "         [0.3751, 0.4587],\n",
      "         [0.2424, 0.4879],\n",
      "         [0.3146, 0.5916],\n",
      "         [0.2820, 0.5850],\n",
      "         [0.3116, 0.6640],\n",
      "         [0.2664, 0.6595],\n",
      "         [0.2728, 0.7324],\n",
      "         [0.2010, 0.6908]]])\n",
      "\n",
      "0: 640x640 1 person, 121.8ms\n",
      "Speed: 1.2ms preprocess, 121.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [222, 226, 211],\n",
      "        [214, 216, 205],\n",
      "        [185, 187, 176]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [230, 236, 217],\n",
      "        [219, 224, 209],\n",
      "        [200, 204, 189]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [233, 239, 221],\n",
      "        [228, 232, 217],\n",
      "        [216, 221, 205]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2421607971191406, 'inference': 121.78683280944824, 'postprocess': 0.4150867462158203}]\n",
      "Bounding Box: tensor([[105.5000, 214.0000,  77.0000, 120.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1539, 0.0335, 0.1644, 0.1315, 0.7062, 0.8757, 0.9910, 0.4223, 0.9656, 0.2032, 0.8222, 0.9941, 0.9986, 0.9875, 0.9967, 0.9794, 0.9912]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.5389e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3484e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6444e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3153e-01],\n",
      "         [1.3485e+02, 1.6314e+02, 7.0624e-01],\n",
      "         [1.2752e+02, 1.7306e+02, 8.7570e-01],\n",
      "         [1.2405e+02, 1.7055e+02, 9.9101e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2231e-01],\n",
      "         [1.0455e+02, 1.7855e+02, 9.6562e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0325e-01],\n",
      "         [9.8741e+01, 2.0421e+02, 8.2221e-01],\n",
      "         [1.0724e+02, 2.0651e+02, 9.9413e-01],\n",
      "         [1.0617e+02, 2.0540e+02, 9.9857e-01],\n",
      "         [1.0848e+02, 2.3450e+02, 9.8751e-01],\n",
      "         [1.1126e+02, 2.3554e+02, 9.9673e-01],\n",
      "         [7.9924e+01, 2.5237e+02, 9.7943e-01],\n",
      "         [9.2027e+01, 2.6091e+02, 9.9117e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [134.8481, 163.1443],\n",
      "         [127.5205, 173.0585],\n",
      "         [124.0542, 170.5477],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.5512, 178.5457],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 98.7412, 204.2079],\n",
      "         [107.2370, 206.5103],\n",
      "         [106.1737, 205.3953],\n",
      "         [108.4805, 234.4954],\n",
      "         [111.2572, 235.5359],\n",
      "         [ 79.9236, 252.3689],\n",
      "         [ 92.0266, 260.9134]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3746, 0.4532],\n",
      "         [0.3542, 0.4807],\n",
      "         [0.3446, 0.4737],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2904, 0.4960],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2743, 0.5672],\n",
      "         [0.2979, 0.5736],\n",
      "         [0.2949, 0.5705],\n",
      "         [0.3013, 0.6514],\n",
      "         [0.3090, 0.6543],\n",
      "         [0.2220, 0.7010],\n",
      "         [0.2556, 0.7248]]])\n",
      "\n",
      "0: 640x640 1 person, 127.6ms\n",
      "Speed: 1.3ms preprocess, 127.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194],\n",
      "        [151, 167, 194]],\n",
      "\n",
      "       [[143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        ...,\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195],\n",
      "        [152, 168, 195]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196],\n",
      "        [153, 169, 196]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [125, 109, 108],\n",
      "        [137, 118, 115],\n",
      "        [146, 127, 124]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [152, 139, 137],\n",
      "        [140, 124, 119],\n",
      "        [144, 127, 123]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [192, 179, 176],\n",
      "        [157, 140, 136],\n",
      "        [151, 134, 130]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3167858123779297, 'inference': 127.58588790893555, 'postprocess': 0.41794776916503906}]\n",
      "Bounding Box: tensor([[108.5000, 210.5000,  67.0000, 129.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1729, 0.0447, 0.0962, 0.1873, 0.6159, 0.9739, 0.9837, 0.9246, 0.9539, 0.7857, 0.8560, 0.9974, 0.9977, 0.9905, 0.9912, 0.9593, 0.9571]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.7292e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4661e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 9.6203e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8732e-01],\n",
      "         [1.3540e+02, 1.5594e+02, 6.1588e-01],\n",
      "         [1.2176e+02, 1.6708e+02, 9.7391e-01],\n",
      "         [1.2985e+02, 1.6613e+02, 9.8368e-01],\n",
      "         [1.1469e+02, 1.9035e+02, 9.2456e-01],\n",
      "         [1.2690e+02, 1.7997e+02, 9.5390e-01],\n",
      "         [1.1270e+02, 2.1398e+02, 7.8569e-01],\n",
      "         [1.2375e+02, 1.9035e+02, 8.5605e-01],\n",
      "         [9.9770e+01, 2.0342e+02, 9.9742e-01],\n",
      "         [1.0405e+02, 2.0315e+02, 9.9772e-01],\n",
      "         [1.1446e+02, 2.3255e+02, 9.9051e-01],\n",
      "         [1.1408e+02, 2.3520e+02, 9.9120e-01],\n",
      "         [8.7496e+01, 2.5774e+02, 9.5934e-01],\n",
      "         [8.5208e+01, 2.5806e+02, 9.5712e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [135.4043, 155.9350],\n",
      "         [121.7644, 167.0765],\n",
      "         [129.8490, 166.1330],\n",
      "         [114.6855, 190.3504],\n",
      "         [126.8963, 179.9689],\n",
      "         [112.6985, 213.9752],\n",
      "         [123.7503, 190.3504],\n",
      "         [ 99.7697, 203.4179],\n",
      "         [104.0491, 203.1521],\n",
      "         [114.4640, 232.5514],\n",
      "         [114.0777, 235.2021],\n",
      "         [ 87.4959, 257.7353],\n",
      "         [ 85.2081, 258.0583]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3761, 0.4332],\n",
      "         [0.3382, 0.4641],\n",
      "         [0.3607, 0.4615],\n",
      "         [0.3186, 0.5288],\n",
      "         [0.3525, 0.4999],\n",
      "         [0.3131, 0.5944],\n",
      "         [0.3438, 0.5288],\n",
      "         [0.2771, 0.5650],\n",
      "         [0.2890, 0.5643],\n",
      "         [0.3180, 0.6460],\n",
      "         [0.3169, 0.6533],\n",
      "         [0.2430, 0.7159],\n",
      "         [0.2367, 0.7168]]])\n",
      "\n",
      "0: 640x640 1 person, 123.8ms\n",
      "Speed: 1.7ms preprocess, 123.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       [[143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        [143, 165, 189],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [147, 125, 122],\n",
      "        [148, 126, 120],\n",
      "        [155, 133, 127]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [148, 130, 125],\n",
      "        [148, 130, 123],\n",
      "        [147, 129, 122]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [148, 130, 125],\n",
      "        [147, 129, 122],\n",
      "        [141, 123, 116]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.728057861328125, 'inference': 123.81291389465332, 'postprocess': 0.4169940948486328}]\n",
      "Bounding Box: tensor([[111.5000, 210.5000,  65.0000, 133.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1914, 0.0227, 0.2319, 0.1083, 0.8892, 0.9332, 0.9978, 0.5722, 0.9937, 0.3142, 0.9514, 0.9951, 0.9991, 0.9804, 0.9965, 0.9336, 0.9777]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.9144e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2742e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3194e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0831e-01],\n",
      "         [1.3416e+02, 1.5592e+02, 8.8915e-01],\n",
      "         [1.1531e+02, 1.6798e+02, 9.3323e-01],\n",
      "         [1.2825e+02, 1.6435e+02, 9.9784e-01],\n",
      "         [1.1343e+02, 1.8804e+02, 5.7215e-01],\n",
      "         [1.2707e+02, 1.7175e+02, 9.9371e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1423e-01],\n",
      "         [1.3683e+02, 1.8065e+02, 9.5145e-01],\n",
      "         [9.7328e+01, 2.0258e+02, 9.9515e-01],\n",
      "         [1.0524e+02, 2.0208e+02, 9.9912e-01],\n",
      "         [1.1679e+02, 2.2977e+02, 9.8037e-01],\n",
      "         [1.2495e+02, 2.3260e+02, 9.9652e-01],\n",
      "         [9.1601e+01, 2.5790e+02, 9.3362e-01],\n",
      "         [9.1928e+01, 2.5725e+02, 9.7771e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [134.1551, 155.9194],\n",
      "         [115.3086, 167.9825],\n",
      "         [128.2516, 164.3486],\n",
      "         [113.4286, 188.0392],\n",
      "         [127.0693, 171.7501],\n",
      "         [  0.0000,   0.0000],\n",
      "         [136.8292, 180.6461],\n",
      "         [ 97.3285, 202.5808],\n",
      "         [105.2357, 202.0785],\n",
      "         [116.7896, 229.7747],\n",
      "         [124.9473, 232.6020],\n",
      "         [ 91.6013, 257.8996],\n",
      "         [ 91.9278, 257.2488]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3727, 0.4331],\n",
      "         [0.3203, 0.4666],\n",
      "         [0.3563, 0.4565],\n",
      "         [0.3151, 0.5223],\n",
      "         [0.3530, 0.4771],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3801, 0.5018],\n",
      "         [0.2704, 0.5627],\n",
      "         [0.2923, 0.5613],\n",
      "         [0.3244, 0.6383],\n",
      "         [0.3471, 0.6461],\n",
      "         [0.2544, 0.7164],\n",
      "         [0.2554, 0.7146]]])\n",
      "\n",
      "0: 640x640 1 person, 118.6ms\n",
      "Speed: 1.2ms preprocess, 118.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [146, 166, 189],\n",
      "        [146, 166, 189],\n",
      "        [145, 165, 188]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [145, 165, 188],\n",
      "        [145, 165, 188],\n",
      "        [144, 164, 187]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [144, 164, 187],\n",
      "        [145, 165, 188],\n",
      "        [143, 162, 186]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [154, 126, 119],\n",
      "        [155, 125, 117],\n",
      "        [155, 125, 117]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [154, 126, 119],\n",
      "        [155, 125, 117],\n",
      "        [155, 125, 117]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [154, 126, 119],\n",
      "        [155, 125, 117],\n",
      "        [155, 125, 117]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.1818408966064453, 'inference': 118.64686012268066, 'postprocess': 0.4239082336425781}]\n",
      "Bounding Box: tensor([[116., 209.,  80., 126.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0793, 0.0066, 0.0924, 0.0731, 0.8659, 0.9480, 0.9980, 0.6642, 0.9949, 0.3764, 0.9571, 0.9982, 0.9996, 0.9935, 0.9987, 0.9759, 0.9917]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 7.9264e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6357e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 9.2375e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 7.3096e-02],\n",
      "         [1.3229e+02, 1.5470e+02, 8.6594e-01],\n",
      "         [1.1335e+02, 1.6364e+02, 9.4801e-01],\n",
      "         [1.2944e+02, 1.6197e+02, 9.9796e-01],\n",
      "         [1.1275e+02, 1.8004e+02, 6.6415e-01],\n",
      "         [1.3665e+02, 1.7272e+02, 9.9493e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7641e-01],\n",
      "         [1.4942e+02, 1.8182e+02, 9.5708e-01],\n",
      "         [9.7039e+01, 2.0041e+02, 9.9819e-01],\n",
      "         [1.0750e+02, 1.9973e+02, 9.9964e-01],\n",
      "         [1.1131e+02, 2.3011e+02, 9.9349e-01],\n",
      "         [1.2993e+02, 2.2705e+02, 9.9869e-01],\n",
      "         [8.9546e+01, 2.5983e+02, 9.7585e-01],\n",
      "         [1.0305e+02, 2.5532e+02, 9.9165e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [132.2923, 154.7044],\n",
      "         [113.3495, 163.6375],\n",
      "         [129.4418, 161.9694],\n",
      "         [112.7513, 180.0413],\n",
      "         [136.6461, 172.7213],\n",
      "         [  0.0000,   0.0000],\n",
      "         [149.4178, 181.8182],\n",
      "         [ 97.0386, 200.4136],\n",
      "         [107.5000, 199.7258],\n",
      "         [111.3053, 230.1135],\n",
      "         [129.9278, 227.0451],\n",
      "         [ 89.5463, 259.8310],\n",
      "         [103.0518, 255.3212]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3675, 0.4297],\n",
      "         [0.3149, 0.4545],\n",
      "         [0.3596, 0.4499],\n",
      "         [0.3132, 0.5001],\n",
      "         [0.3796, 0.4798],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4150, 0.5051],\n",
      "         [0.2696, 0.5567],\n",
      "         [0.2986, 0.5548],\n",
      "         [0.3092, 0.6392],\n",
      "         [0.3609, 0.6307],\n",
      "         [0.2487, 0.7218],\n",
      "         [0.2863, 0.7092]]])\n",
      "\n",
      "0: 640x640 1 person, 134.5ms\n",
      "Speed: 1.3ms preprocess, 134.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [146, 166, 189],\n",
      "        [146, 166, 189],\n",
      "        [145, 165, 188]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [145, 165, 188],\n",
      "        [145, 165, 188],\n",
      "        [144, 164, 187]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [144, 164, 187],\n",
      "        [145, 165, 188],\n",
      "        [143, 162, 186]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [145, 130, 116],\n",
      "        [147, 130, 117],\n",
      "        [150, 132, 119]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [147, 132, 120],\n",
      "        [146, 129, 116],\n",
      "        [151, 133, 120]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [146, 131, 119],\n",
      "        [148, 131, 118],\n",
      "        [151, 133, 120]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3272762298583984, 'inference': 134.5059871673584, 'postprocess': 0.43320655822753906}]\n",
      "Bounding Box: tensor([[117.5000, 206.0000,  91.0000, 136.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1660, 0.0201, 0.1888, 0.1793, 0.9136, 0.9646, 0.9978, 0.6186, 0.9894, 0.3076, 0.9093, 0.9964, 0.9991, 0.9902, 0.9975, 0.9719, 0.9889]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.6599e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0053e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8878e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7929e-01],\n",
      "         [1.3085e+02, 1.5022e+02, 9.1358e-01],\n",
      "         [1.1041e+02, 1.6115e+02, 9.6463e-01],\n",
      "         [1.2945e+02, 1.6008e+02, 9.9782e-01],\n",
      "         [1.0557e+02, 1.7758e+02, 6.1860e-01],\n",
      "         [1.4216e+02, 1.7122e+02, 9.8944e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0756e-01],\n",
      "         [1.5746e+02, 1.7824e+02, 9.0930e-01],\n",
      "         [9.6180e+01, 1.9873e+02, 9.9639e-01],\n",
      "         [1.0981e+02, 1.9810e+02, 9.9911e-01],\n",
      "         [1.0696e+02, 2.2974e+02, 9.9018e-01],\n",
      "         [1.3202e+02, 2.2421e+02, 9.9754e-01],\n",
      "         [8.2739e+01, 2.5894e+02, 9.7190e-01],\n",
      "         [1.1677e+02, 2.5500e+02, 9.8887e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [130.8494, 150.2176],\n",
      "         [110.4073, 161.1496],\n",
      "         [129.4466, 160.0782],\n",
      "         [105.5732, 177.5759],\n",
      "         [142.1635, 171.2186],\n",
      "         [  0.0000,   0.0000],\n",
      "         [157.4567, 178.2390],\n",
      "         [ 96.1803, 198.7305],\n",
      "         [109.8126, 198.0990],\n",
      "         [106.9631, 229.7390],\n",
      "         [132.0217, 224.2083],\n",
      "         [ 82.7392, 258.9439],\n",
      "         [116.7696, 254.9997]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3635, 0.4173],\n",
      "         [0.3067, 0.4476],\n",
      "         [0.3596, 0.4447],\n",
      "         [0.2933, 0.4933],\n",
      "         [0.3949, 0.4756],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4374, 0.4951],\n",
      "         [0.2672, 0.5520],\n",
      "         [0.3050, 0.5503],\n",
      "         [0.2971, 0.6382],\n",
      "         [0.3667, 0.6228],\n",
      "         [0.2298, 0.7193],\n",
      "         [0.3244, 0.7083]]])\n",
      "\n",
      "0: 640x640 1 person, 129.1ms\n",
      "Speed: 1.3ms preprocess, 129.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       [[141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        [141, 164, 188],\n",
      "        ...,\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200],\n",
      "        [157, 173, 200]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [110, 100,  62],\n",
      "        [107,  96,  59],\n",
      "        [104,  94,  56]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [107, 100,  61],\n",
      "        [103,  96,  58],\n",
      "        [101,  94,  55]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [105,  98,  60],\n",
      "        [103,  96,  58],\n",
      "        [100,  93,  54]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2958049774169922, 'inference': 129.11319732666016, 'postprocess': 0.7219314575195312}]\n",
      "Bounding Box: tensor([[121.5000, 201.0000, 105.0000, 146.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0891, 0.0205, 0.0668, 0.4155, 0.8338, 0.9782, 0.9963, 0.6641, 0.9653, 0.2521, 0.7082, 0.9973, 0.9989, 0.9935, 0.9974, 0.9836, 0.9912]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 8.9056e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0486e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6794e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1547e-01],\n",
      "         [1.2821e+02, 1.4667e+02, 8.3381e-01],\n",
      "         [1.0890e+02, 1.6093e+02, 9.7817e-01],\n",
      "         [1.3066e+02, 1.5792e+02, 9.9631e-01],\n",
      "         [1.0043e+02, 1.7773e+02, 6.6408e-01],\n",
      "         [1.4790e+02, 1.6798e+02, 9.6529e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5210e-01],\n",
      "         [1.6615e+02, 1.7813e+02, 7.0817e-01],\n",
      "         [9.8937e+01, 1.9788e+02, 9.9726e-01],\n",
      "         [1.1505e+02, 1.9658e+02, 9.9893e-01],\n",
      "         [1.0416e+02, 2.2848e+02, 9.9353e-01],\n",
      "         [1.3223e+02, 2.2308e+02, 9.9742e-01],\n",
      "         [8.0120e+01, 2.5738e+02, 9.8358e-01],\n",
      "         [1.2671e+02, 2.5505e+02, 9.9124e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [128.2050, 146.6681],\n",
      "         [108.9039, 160.9312],\n",
      "         [130.6617, 157.9231],\n",
      "         [100.4252, 177.7314],\n",
      "         [147.8996, 167.9829],\n",
      "         [  0.0000,   0.0000],\n",
      "         [166.1509, 178.1295],\n",
      "         [ 98.9366, 197.8770],\n",
      "         [115.0534, 196.5822],\n",
      "         [104.1580, 228.4814],\n",
      "         [132.2327, 223.0835],\n",
      "         [ 80.1205, 257.3773],\n",
      "         [126.7089, 255.0530]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3561, 0.4074],\n",
      "         [0.3025, 0.4470],\n",
      "         [0.3629, 0.4387],\n",
      "         [0.2790, 0.4937],\n",
      "         [0.4108, 0.4666],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4615, 0.4948],\n",
      "         [0.2748, 0.5497],\n",
      "         [0.3196, 0.5461],\n",
      "         [0.2893, 0.6347],\n",
      "         [0.3673, 0.6197],\n",
      "         [0.2226, 0.7149],\n",
      "         [0.3520, 0.7085]]])\n",
      "\n",
      "0: 640x640 1 person, 113.7ms\n",
      "Speed: 1.3ms preprocess, 113.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [107,  97,  56],\n",
      "        [104,  95,  54],\n",
      "        [103,  94,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [103,  97,  55],\n",
      "        [101,  95,  53],\n",
      "        [101,  95,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [103,  97,  55],\n",
      "        [100,  94,  52],\n",
      "        [100,  94,  52]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3318061828613281, 'inference': 113.74902725219727, 'postprocess': 0.4119873046875}]\n",
      "Bounding Box: tensor([[115.0000, 202.5000, 100.0000, 145.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0634, 0.0276, 0.0338, 0.5334, 0.7104, 0.9772, 0.9873, 0.6755, 0.8207, 0.2038, 0.3195, 0.9971, 0.9979, 0.9939, 0.9954, 0.9848, 0.9870]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 6.3448e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7612e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3833e-02],\n",
      "         [1.1394e+02, 1.4490e+02, 5.3343e-01],\n",
      "         [1.2519e+02, 1.4489e+02, 7.1043e-01],\n",
      "         [1.0619e+02, 1.6046e+02, 9.7715e-01],\n",
      "         [1.2960e+02, 1.5509e+02, 9.8731e-01],\n",
      "         [1.0063e+02, 1.8177e+02, 6.7550e-01],\n",
      "         [1.4764e+02, 1.6321e+02, 8.2068e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0379e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1951e-01],\n",
      "         [9.9748e+01, 1.9778e+02, 9.9709e-01],\n",
      "         [1.1744e+02, 1.9535e+02, 9.9790e-01],\n",
      "         [1.0069e+02, 2.2982e+02, 9.9394e-01],\n",
      "         [1.3347e+02, 2.2415e+02, 9.9537e-01],\n",
      "         [7.9131e+01, 2.5838e+02, 9.8476e-01],\n",
      "         [1.3292e+02, 2.5515e+02, 9.8697e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [113.9401, 144.8954],\n",
      "         [125.1885, 144.8907],\n",
      "         [106.1933, 160.4637],\n",
      "         [129.6040, 155.0928],\n",
      "         [100.6318, 181.7716],\n",
      "         [147.6427, 163.2125],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 99.7484, 197.7789],\n",
      "         [117.4369, 195.3452],\n",
      "         [100.6855, 229.8162],\n",
      "         [133.4692, 224.1519],\n",
      "         [ 79.1310, 258.3847],\n",
      "         [132.9229, 255.1537]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3165, 0.4025],\n",
      "         [0.3477, 0.4025],\n",
      "         [0.2950, 0.4457],\n",
      "         [0.3600, 0.4308],\n",
      "         [0.2795, 0.5049],\n",
      "         [0.4101, 0.4534],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2771, 0.5494],\n",
      "         [0.3262, 0.5426],\n",
      "         [0.2797, 0.6384],\n",
      "         [0.3707, 0.6226],\n",
      "         [0.2198, 0.7177],\n",
      "         [0.3692, 0.7088]]])\n",
      "\n",
      "0: 640x640 1 person, 119.4ms\n",
      "Speed: 1.3ms preprocess, 119.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[137, 159, 183],\n",
      "        [137, 159, 183],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[137, 159, 183],\n",
      "        [137, 159, 183],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[137, 159, 183],\n",
      "        [137, 159, 183],\n",
      "        [138, 160, 185],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [107, 101,  59],\n",
      "        [103,  97,  55],\n",
      "        [102,  96,  54]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [101,  97,  54],\n",
      "        [100,  96,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [103, 100,  56],\n",
      "        [101,  97,  54],\n",
      "        [ 98,  95,  52]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2607574462890625, 'inference': 119.41027641296387, 'postprocess': 0.4322528839111328}]\n",
      "Bounding Box: tensor([[122.5000, 201.5000, 117.0000, 147.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0765, 0.0378, 0.0281, 0.7009, 0.6261, 0.9835, 0.9915, 0.7304, 0.8599, 0.2700, 0.3744, 0.9964, 0.9975, 0.9897, 0.9923, 0.9641, 0.9710]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 7.6531e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7821e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8071e-02],\n",
      "         [1.1031e+02, 1.4345e+02, 7.0087e-01],\n",
      "         [1.2259e+02, 1.4298e+02, 6.2614e-01],\n",
      "         [1.0444e+02, 1.5897e+02, 9.8350e-01],\n",
      "         [1.3002e+02, 1.5451e+02, 9.9147e-01],\n",
      "         [9.6670e+01, 1.7860e+02, 7.3039e-01],\n",
      "         [1.5263e+02, 1.6453e+02, 8.5987e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6997e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7442e-01],\n",
      "         [1.0214e+02, 1.9713e+02, 9.9640e-01],\n",
      "         [1.2130e+02, 1.9493e+02, 9.9747e-01],\n",
      "         [9.9606e+01, 2.2945e+02, 9.8974e-01],\n",
      "         [1.3260e+02, 2.2485e+02, 9.9228e-01],\n",
      "         [7.8551e+01, 2.5891e+02, 9.6408e-01],\n",
      "         [1.3921e+02, 2.5280e+02, 9.7098e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [110.3092, 143.4451],\n",
      "         [122.5917, 142.9758],\n",
      "         [104.4448, 158.9688],\n",
      "         [130.0199, 154.5143],\n",
      "         [ 96.6705, 178.6038],\n",
      "         [152.6262, 164.5345],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.1379, 197.1301],\n",
      "         [121.2955, 194.9304],\n",
      "         [ 99.6058, 229.4503],\n",
      "         [132.5967, 224.8499],\n",
      "         [ 78.5508, 258.9106],\n",
      "         [139.2076, 252.8040]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3064, 0.3985],\n",
      "         [0.3405, 0.3972],\n",
      "         [0.2901, 0.4416],\n",
      "         [0.3612, 0.4292],\n",
      "         [0.2685, 0.4961],\n",
      "         [0.4240, 0.4570],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2837, 0.5476],\n",
      "         [0.3369, 0.5415],\n",
      "         [0.2767, 0.6374],\n",
      "         [0.3683, 0.6246],\n",
      "         [0.2182, 0.7192],\n",
      "         [0.3867, 0.7022]]])\n",
      "\n",
      "0: 640x640 1 person, 136.0ms\n",
      "Speed: 1.3ms preprocess, 136.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        [144, 166, 190],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [105, 102,  59],\n",
      "        [102,  98,  55],\n",
      "        [101,  97,  54]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [101,  97,  54],\n",
      "        [100,  96,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [103, 100,  56],\n",
      "        [101,  97,  54],\n",
      "        [ 98,  95,  52]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3468265533447266, 'inference': 135.98895072937012, 'postprocess': 0.4780292510986328}]\n",
      "Bounding Box: tensor([[123., 201., 116., 146.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0618, 0.0262, 0.0230, 0.6138, 0.6310, 0.9828, 0.9920, 0.7677, 0.8984, 0.3201, 0.4739, 0.9971, 0.9981, 0.9909, 0.9935, 0.9647, 0.9719]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 6.1840e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6178e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3003e-02],\n",
      "         [1.0671e+02, 1.4492e+02, 6.1381e-01],\n",
      "         [1.1922e+02, 1.4390e+02, 6.3096e-01],\n",
      "         [1.0229e+02, 1.5970e+02, 9.8280e-01],\n",
      "         [1.2740e+02, 1.5319e+02, 9.9196e-01],\n",
      "         [9.5419e+01, 1.8025e+02, 7.6772e-01],\n",
      "         [1.5103e+02, 1.5992e+02, 8.9838e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2010e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7392e-01],\n",
      "         [1.0580e+02, 1.9762e+02, 9.9713e-01],\n",
      "         [1.2424e+02, 1.9525e+02, 9.9812e-01],\n",
      "         [1.0062e+02, 2.2900e+02, 9.9089e-01],\n",
      "         [1.3228e+02, 2.2711e+02, 9.9351e-01],\n",
      "         [8.1447e+01, 2.5914e+02, 9.6470e-01],\n",
      "         [1.3937e+02, 2.5532e+02, 9.7188e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [106.7122, 144.9174],\n",
      "         [119.2151, 143.9044],\n",
      "         [102.2860, 159.6974],\n",
      "         [127.3994, 153.1938],\n",
      "         [ 95.4190, 180.2494],\n",
      "         [151.0295, 159.9210],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.8009, 197.6208],\n",
      "         [124.2376, 195.2451],\n",
      "         [100.6239, 228.9956],\n",
      "         [132.2844, 227.1053],\n",
      "         [ 81.4471, 259.1445],\n",
      "         [139.3708, 255.3229]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2964, 0.4025],\n",
      "         [0.3312, 0.3997],\n",
      "         [0.2841, 0.4436],\n",
      "         [0.3539, 0.4255],\n",
      "         [0.2651, 0.5007],\n",
      "         [0.4195, 0.4442],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2939, 0.5489],\n",
      "         [0.3451, 0.5423],\n",
      "         [0.2795, 0.6361],\n",
      "         [0.3675, 0.6308],\n",
      "         [0.2262, 0.7198],\n",
      "         [0.3871, 0.7092]]])\n",
      "\n",
      "0: 640x640 1 person, 137.4ms\n",
      "Speed: 1.3ms preprocess, 137.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 167, 182],\n",
      "        [141, 167, 182],\n",
      "        [141, 166, 185],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[140, 166, 181],\n",
      "        [140, 166, 181],\n",
      "        [140, 165, 183],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[140, 165, 183],\n",
      "        [140, 165, 183],\n",
      "        [140, 165, 183],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [105, 102,  59],\n",
      "        [103,  97,  55],\n",
      "        [102,  96,  54]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [102,  96,  54],\n",
      "        [101,  95,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [103,  97,  55],\n",
      "        [101,  95,  53]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3408660888671875, 'inference': 137.4201774597168, 'postprocess': 1.1119842529296875}]\n",
      "Bounding Box: tensor([[118.5000, 204.0000,  93.0000, 142.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0879, 0.0489, 0.0222, 0.6845, 0.4598, 0.9871, 0.9873, 0.8866, 0.8735, 0.5711, 0.5127, 0.9982, 0.9983, 0.9949, 0.9944, 0.9781, 0.9767]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 8.7913e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8878e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2166e-02],\n",
      "         [1.0620e+02, 1.4685e+02, 6.8449e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5976e-01],\n",
      "         [1.0280e+02, 1.5995e+02, 9.8713e-01],\n",
      "         [1.2614e+02, 1.5514e+02, 9.8729e-01],\n",
      "         [9.5708e+01, 1.7545e+02, 8.8657e-01],\n",
      "         [1.4751e+02, 1.5947e+02, 8.7345e-01],\n",
      "         [1.0134e+02, 1.8592e+02, 5.7107e-01],\n",
      "         [1.6084e+02, 1.6617e+02, 5.1267e-01],\n",
      "         [1.0915e+02, 1.9725e+02, 9.9823e-01],\n",
      "         [1.2613e+02, 1.9549e+02, 9.9828e-01],\n",
      "         [1.0188e+02, 2.2914e+02, 9.9486e-01],\n",
      "         [1.3032e+02, 2.2846e+02, 9.9443e-01],\n",
      "         [8.7531e+01, 2.5879e+02, 9.7806e-01],\n",
      "         [1.3946e+02, 2.5559e+02, 9.7670e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [106.1957, 146.8478],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.8044, 159.9483],\n",
      "         [126.1370, 155.1379],\n",
      "         [ 95.7080, 175.4467],\n",
      "         [147.5116, 159.4665],\n",
      "         [101.3448, 185.9244],\n",
      "         [160.8376, 166.1705],\n",
      "         [109.1490, 197.2511],\n",
      "         [126.1330, 195.4874],\n",
      "         [101.8811, 229.1355],\n",
      "         [130.3155, 228.4601],\n",
      "         [ 87.5312, 258.7851],\n",
      "         [139.4612, 255.5878]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2950, 0.4079],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2856, 0.4443],\n",
      "         [0.3504, 0.4309],\n",
      "         [0.2659, 0.4874],\n",
      "         [0.4098, 0.4430],\n",
      "         [0.2815, 0.5165],\n",
      "         [0.4468, 0.4616],\n",
      "         [0.3032, 0.5479],\n",
      "         [0.3504, 0.5430],\n",
      "         [0.2830, 0.6365],\n",
      "         [0.3620, 0.6346],\n",
      "         [0.2431, 0.7188],\n",
      "         [0.3874, 0.7100]]])\n",
      "\n",
      "0: 640x640 2 persons, 139.4ms\n",
      "Speed: 1.8ms preprocess, 139.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 168, 187],\n",
      "        [141, 168, 187],\n",
      "        [141, 168, 187],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[140, 167, 186],\n",
      "        [140, 167, 186],\n",
      "        [140, 167, 186],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[139, 166, 189],\n",
      "        [139, 166, 189],\n",
      "        [139, 166, 187],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [105, 102,  59],\n",
      "        [103,  97,  55],\n",
      "        [102,  96,  54]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [102,  96,  54],\n",
      "        [101,  95,  53]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [104, 101,  58],\n",
      "        [103,  97,  55],\n",
      "        [101,  95,  53]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.7557144165039062, 'inference': 139.41597938537598, 'postprocess': 0.4239082336425781}]\n",
      "Bounding Box: tensor([[115.0000, 205.5000,  66.0000, 145.0000]])\n",
      "Bounding Box: tensor([[131.5000, 206.0000, 101.0000, 144.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3479, 0.3805, 0.0503, 0.9270, 0.2325, 0.9938, 0.9735, 0.9488, 0.6895, 0.7866, 0.3516, 0.9970, 0.9940, 0.9953, 0.9900, 0.9833, 0.9717]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.4791e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8053e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0320e-02],\n",
      "         [1.0521e+02, 1.4717e+02, 9.2702e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3251e-01],\n",
      "         [1.0254e+02, 1.6177e+02, 9.9382e-01],\n",
      "         [1.2690e+02, 1.5726e+02, 9.7351e-01],\n",
      "         [9.6504e+01, 1.8018e+02, 9.4879e-01],\n",
      "         [1.4713e+02, 1.6729e+02, 6.8948e-01],\n",
      "         [1.0571e+02, 1.9167e+02, 7.8665e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5157e-01],\n",
      "         [1.1142e+02, 2.0020e+02, 9.9696e-01],\n",
      "         [1.2903e+02, 1.9921e+02, 9.9395e-01],\n",
      "         [1.0169e+02, 2.2904e+02, 9.9531e-01],\n",
      "         [1.2885e+02, 2.3292e+02, 9.8997e-01],\n",
      "         [9.4947e+01, 2.6328e+02, 9.8331e-01],\n",
      "         [1.3789e+02, 2.6286e+02, 9.7170e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.2052, 147.1662],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.5357, 161.7717],\n",
      "         [126.9032, 157.2567],\n",
      "         [ 96.5039, 180.1775],\n",
      "         [147.1313, 167.2906],\n",
      "         [105.7091, 191.6687],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.4184, 200.2040],\n",
      "         [129.0337, 199.2064],\n",
      "         [101.6930, 229.0390],\n",
      "         [128.8491, 232.9171],\n",
      "         [ 94.9466, 263.2765],\n",
      "         [137.8946, 262.8605]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2922, 0.4088],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2848, 0.4494],\n",
      "         [0.3525, 0.4368],\n",
      "         [0.2681, 0.5005],\n",
      "         [0.4087, 0.4647],\n",
      "         [0.2936, 0.5324],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3095, 0.5561],\n",
      "         [0.3584, 0.5534],\n",
      "         [0.2825, 0.6362],\n",
      "         [0.3579, 0.6470],\n",
      "         [0.2637, 0.7313],\n",
      "         [0.3830, 0.7302]]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5005, 0.4086, 0.1541, 0.8286, 0.2430, 0.9820, 0.9653, 0.8324, 0.7858, 0.6183, 0.5758, 0.9913, 0.9910, 0.9863, 0.9863, 0.9509, 0.9536]])\n",
      "data: tensor([[[1.0120e+02, 1.4811e+02, 5.0051e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0860e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5405e-01],\n",
      "         [1.0586e+02, 1.4743e+02, 8.2865e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4299e-01],\n",
      "         [1.0317e+02, 1.6309e+02, 9.8203e-01],\n",
      "         [1.2704e+02, 1.5786e+02, 9.6529e-01],\n",
      "         [9.6345e+01, 1.8112e+02, 8.3243e-01],\n",
      "         [1.4864e+02, 1.6530e+02, 7.8583e-01],\n",
      "         [1.0592e+02, 1.9310e+02, 6.1827e-01],\n",
      "         [1.7126e+02, 1.7502e+02, 5.7584e-01],\n",
      "         [1.1351e+02, 2.0012e+02, 9.9129e-01],\n",
      "         [1.2963e+02, 1.9856e+02, 9.9099e-01],\n",
      "         [1.0354e+02, 2.2792e+02, 9.8626e-01],\n",
      "         [1.2700e+02, 2.2983e+02, 9.8631e-01],\n",
      "         [1.0100e+02, 2.6038e+02, 9.5091e-01],\n",
      "         [1.3689e+02, 2.6141e+02, 9.5357e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[101.2026, 148.1107],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.8603, 147.4257],\n",
      "         [  0.0000,   0.0000],\n",
      "         [103.1699, 163.0930],\n",
      "         [127.0431, 157.8639],\n",
      "         [ 96.3455, 181.1245],\n",
      "         [148.6429, 165.3022],\n",
      "         [105.9171, 193.0958],\n",
      "         [171.2581, 175.0226],\n",
      "         [113.5123, 200.1215],\n",
      "         [129.6251, 198.5597],\n",
      "         [103.5400, 227.9203],\n",
      "         [126.9964, 229.8327],\n",
      "         [100.9965, 260.3792],\n",
      "         [136.8853, 261.4084]]])\n",
      "xyn: tensor([[[0.2811, 0.4114],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2941, 0.4095],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2866, 0.4530],\n",
      "         [0.3529, 0.4385],\n",
      "         [0.2676, 0.5031],\n",
      "         [0.4129, 0.4592],\n",
      "         [0.2942, 0.5364],\n",
      "         [0.4757, 0.4862],\n",
      "         [0.3153, 0.5559],\n",
      "         [0.3601, 0.5516],\n",
      "         [0.2876, 0.6331],\n",
      "         [0.3528, 0.6384],\n",
      "         [0.2805, 0.7233],\n",
      "         [0.3802, 0.7261]]])\n",
      "\n",
      "0: 640x640 1 person, 122.4ms\n",
      "Speed: 1.8ms preprocess, 122.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[140, 167, 190],\n",
      "        [140, 167, 190],\n",
      "        [140, 166, 196],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[139, 166, 189],\n",
      "        [139, 166, 189],\n",
      "        [140, 166, 196],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[138, 164, 189],\n",
      "        [138, 164, 189],\n",
      "        [139, 164, 198],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [100,  96,  52],\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [100,  96,  52],\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 98,  95,  51],\n",
      "        [ 95,  94,  49],\n",
      "        [ 94,  93,  48]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.7638206481933594, 'inference': 122.41196632385254, 'postprocess': 0.4138946533203125}]\n",
      "Bounding Box: tensor([[116.5000, 209.0000,  53.0000, 146.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5486, 0.6211, 0.0830, 0.9077, 0.1122, 0.9889, 0.9268, 0.9596, 0.5903, 0.8929, 0.4377, 0.9967, 0.9916, 0.9965, 0.9899, 0.9875, 0.9749]])\n",
      "data: tensor([[[9.7972e+01, 1.5017e+02, 5.4862e-01],\n",
      "         [9.9177e+01, 1.4770e+02, 6.2106e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.3021e-02],\n",
      "         [1.0444e+02, 1.4858e+02, 9.0773e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1224e-01],\n",
      "         [1.0070e+02, 1.6196e+02, 9.8893e-01],\n",
      "         [1.2650e+02, 1.5924e+02, 9.2680e-01],\n",
      "         [9.7416e+01, 1.8012e+02, 9.5958e-01],\n",
      "         [1.4232e+02, 1.7430e+02, 5.9025e-01],\n",
      "         [1.0559e+02, 1.9461e+02, 8.9290e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3774e-01],\n",
      "         [1.1718e+02, 2.0180e+02, 9.9666e-01],\n",
      "         [1.3428e+02, 2.0162e+02, 9.9157e-01],\n",
      "         [1.0257e+02, 2.2927e+02, 9.9645e-01],\n",
      "         [1.2400e+02, 2.3316e+02, 9.8990e-01],\n",
      "         [1.1480e+02, 2.6495e+02, 9.8754e-01],\n",
      "         [1.2937e+02, 2.6666e+02, 9.7491e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 97.9718, 150.1718],\n",
      "         [ 99.1773, 147.7046],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.4377, 148.5824],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.7008, 161.9605],\n",
      "         [126.4964, 159.2415],\n",
      "         [ 97.4165, 180.1228],\n",
      "         [142.3235, 174.2973],\n",
      "         [105.5866, 194.6143],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.1789, 201.7963],\n",
      "         [134.2772, 201.6167],\n",
      "         [102.5746, 229.2747],\n",
      "         [124.0023, 233.1633],\n",
      "         [114.7964, 264.9496],\n",
      "         [129.3745, 266.6627]]])\n",
      "xyn: tensor([[[0.2721, 0.4171],\n",
      "         [0.2755, 0.4103],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2901, 0.4127],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2797, 0.4499],\n",
      "         [0.3514, 0.4423],\n",
      "         [0.2706, 0.5003],\n",
      "         [0.3953, 0.4842],\n",
      "         [0.2933, 0.5406],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3255, 0.5605],\n",
      "         [0.3730, 0.5600],\n",
      "         [0.2849, 0.6369],\n",
      "         [0.3445, 0.6477],\n",
      "         [0.3189, 0.7360],\n",
      "         [0.3594, 0.7407]]])\n",
      "\n",
      "0: 640x640 2 persons, 135.3ms\n",
      "Speed: 1.2ms preprocess, 135.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 159, 198],\n",
      "        [140, 158, 197],\n",
      "        [136, 157, 201],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[141, 159, 198],\n",
      "        [139, 157, 196],\n",
      "        [137, 158, 202],\n",
      "        ...,\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190],\n",
      "        [150, 166, 190]],\n",
      "\n",
      "       [[138, 155, 195],\n",
      "        [139, 157, 196],\n",
      "        [137, 158, 198],\n",
      "        ...,\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188],\n",
      "        [147, 164, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 97,  96,  52],\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 97,  96,  52],\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49],\n",
      "        [ 94,  93,  48]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2180805206298828, 'inference': 135.31184196472168, 'postprocess': 0.4200935363769531}]\n",
      "Bounding Box: tensor([[119.0000, 210.5000,  52.0000, 141.0000]])\n",
      "Bounding Box: tensor([[131.0000, 210.5000,  76.0000, 141.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7345, 0.8031, 0.1777, 0.8836, 0.0904, 0.9772, 0.8388, 0.9474, 0.4820, 0.9058, 0.4690, 0.9903, 0.9740, 0.9872, 0.9621, 0.9524, 0.9062]])\n",
      "data: tensor([[[9.5900e+01, 1.5158e+02, 7.3449e-01],\n",
      "         [9.7606e+01, 1.4903e+02, 8.0307e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7768e-01],\n",
      "         [1.0425e+02, 1.5070e+02, 8.8364e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.0396e-02],\n",
      "         [1.0257e+02, 1.6446e+02, 9.7721e-01],\n",
      "         [1.2374e+02, 1.6243e+02, 8.3883e-01],\n",
      "         [1.0196e+02, 1.8492e+02, 9.4744e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8195e-01],\n",
      "         [1.0103e+02, 1.9563e+02, 9.0583e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6900e-01],\n",
      "         [1.2237e+02, 2.0451e+02, 9.9027e-01],\n",
      "         [1.3512e+02, 2.0462e+02, 9.7403e-01],\n",
      "         [1.0649e+02, 2.3286e+02, 9.8717e-01],\n",
      "         [1.2231e+02, 2.3637e+02, 9.6206e-01],\n",
      "         [1.2912e+02, 2.6357e+02, 9.5236e-01],\n",
      "         [1.2578e+02, 2.6721e+02, 9.0623e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 95.9004, 151.5797],\n",
      "         [ 97.6063, 149.0289],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.2546, 150.7043],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.5652, 164.4557],\n",
      "         [123.7411, 162.4299],\n",
      "         [101.9643, 184.9241],\n",
      "         [  0.0000,   0.0000],\n",
      "         [101.0341, 195.6349],\n",
      "         [  0.0000,   0.0000],\n",
      "         [122.3662, 204.5129],\n",
      "         [135.1176, 204.6164],\n",
      "         [106.4922, 232.8614],\n",
      "         [122.3136, 236.3663],\n",
      "         [129.1244, 263.5728],\n",
      "         [125.7793, 267.2091]]])\n",
      "xyn: tensor([[[0.2664, 0.4211],\n",
      "         [0.2711, 0.4140],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2896, 0.4186],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2849, 0.4568],\n",
      "         [0.3437, 0.4512],\n",
      "         [0.2832, 0.5137],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2807, 0.5434],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3399, 0.5681],\n",
      "         [0.3753, 0.5684],\n",
      "         [0.2958, 0.6468],\n",
      "         [0.3398, 0.6566],\n",
      "         [0.3587, 0.7321],\n",
      "         [0.3494, 0.7422]]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6493, 0.7469, 0.1662, 0.9143, 0.1113, 0.9845, 0.8909, 0.9312, 0.5485, 0.8335, 0.4353, 0.9951, 0.9895, 0.9929, 0.9844, 0.9788, 0.9638]])\n",
      "data: tensor([[[9.5689e+01, 1.5210e+02, 6.4929e-01],\n",
      "         [9.7442e+01, 1.4917e+02, 7.4692e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6615e-01],\n",
      "         [1.0342e+02, 1.5051e+02, 9.1425e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1134e-01],\n",
      "         [1.0245e+02, 1.6484e+02, 9.8447e-01],\n",
      "         [1.2342e+02, 1.6301e+02, 8.9088e-01],\n",
      "         [1.0015e+02, 1.8419e+02, 9.3116e-01],\n",
      "         [1.4172e+02, 1.7644e+02, 5.4850e-01],\n",
      "         [1.0051e+02, 1.9505e+02, 8.3352e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3529e-01],\n",
      "         [1.2250e+02, 2.0551e+02, 9.9510e-01],\n",
      "         [1.3460e+02, 2.0521e+02, 9.8947e-01],\n",
      "         [1.0831e+02, 2.3240e+02, 9.9294e-01],\n",
      "         [1.2189e+02, 2.3731e+02, 9.8436e-01],\n",
      "         [1.2977e+02, 2.6317e+02, 9.7876e-01],\n",
      "         [1.2643e+02, 2.6895e+02, 9.6377e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 95.6889, 152.0975],\n",
      "         [ 97.4425, 149.1675],\n",
      "         [  0.0000,   0.0000],\n",
      "         [103.4187, 150.5086],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.4520, 164.8399],\n",
      "         [123.4206, 163.0121],\n",
      "         [100.1459, 184.1854],\n",
      "         [141.7180, 176.4437],\n",
      "         [100.5065, 195.0474],\n",
      "         [  0.0000,   0.0000],\n",
      "         [122.4966, 205.5087],\n",
      "         [134.6042, 205.2147],\n",
      "         [108.3051, 232.3973],\n",
      "         [121.8898, 237.3095],\n",
      "         [129.7715, 263.1656],\n",
      "         [126.4263, 268.9533]]])\n",
      "xyn: tensor([[[0.2658, 0.4225],\n",
      "         [0.2707, 0.4144],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2873, 0.4181],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2846, 0.4579],\n",
      "         [0.3428, 0.4528],\n",
      "         [0.2782, 0.5116],\n",
      "         [0.3937, 0.4901],\n",
      "         [0.2792, 0.5418],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3403, 0.5709],\n",
      "         [0.3739, 0.5700],\n",
      "         [0.3008, 0.6455],\n",
      "         [0.3386, 0.6592],\n",
      "         [0.3605, 0.7310],\n",
      "         [0.3512, 0.7471]]])\n",
      "\n",
      "0: 640x640 1 person, 115.9ms\n",
      "Speed: 1.3ms preprocess, 115.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[103, 148, 197],\n",
      "        [103, 148, 197],\n",
      "        [103, 148, 197],\n",
      "        ...,\n",
      "        [147, 167, 193],\n",
      "        [147, 167, 193],\n",
      "        [147, 167, 193]],\n",
      "\n",
      "       [[104, 150, 198],\n",
      "        [104, 150, 198],\n",
      "        [104, 150, 198],\n",
      "        ...,\n",
      "        [147, 167, 193],\n",
      "        [147, 167, 193],\n",
      "        [147, 167, 193]],\n",
      "\n",
      "       [[107, 148, 196],\n",
      "        [107, 148, 196],\n",
      "        [107, 148, 196],\n",
      "        ...,\n",
      "        [145, 165, 190],\n",
      "        [145, 165, 190],\n",
      "        [145, 165, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 98,  97,  53],\n",
      "        [ 97,  96,  52],\n",
      "        [ 96,  95,  51]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 97,  96,  52],\n",
      "        [ 96,  95,  51],\n",
      "        [ 95,  94,  49]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 97,  96,  52],\n",
      "        [ 95,  94,  49],\n",
      "        [ 94,  93,  48]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2710094451904297, 'inference': 115.86523056030273, 'postprocess': 0.4191398620605469}]\n",
      "Bounding Box: tensor([[121.0000, 211.5000,  54.0000, 147.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7440, 0.8441, 0.1912, 0.9272, 0.0976, 0.9827, 0.8363, 0.9538, 0.4238, 0.8982, 0.3892, 0.9850, 0.9568, 0.9800, 0.9348, 0.9477, 0.8944]])\n",
      "data: tensor([[[9.5779e+01, 1.5348e+02, 7.4398e-01],\n",
      "         [9.7841e+01, 1.5043e+02, 8.4412e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9124e-01],\n",
      "         [1.0407e+02, 1.5173e+02, 9.2717e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.7566e-02],\n",
      "         [1.0677e+02, 1.6726e+02, 9.8268e-01],\n",
      "         [1.2294e+02, 1.6469e+02, 8.3628e-01],\n",
      "         [1.0673e+02, 1.8759e+02, 9.5378e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2383e-01],\n",
      "         [1.0002e+02, 1.9888e+02, 8.9821e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8923e-01],\n",
      "         [1.2700e+02, 2.0803e+02, 9.8499e-01],\n",
      "         [1.3709e+02, 2.0600e+02, 9.5679e-01],\n",
      "         [1.0745e+02, 2.4125e+02, 9.8003e-01],\n",
      "         [1.1610e+02, 2.3702e+02, 9.3479e-01],\n",
      "         [1.3514e+02, 2.6260e+02, 9.4771e-01],\n",
      "         [1.3165e+02, 2.5890e+02, 8.9438e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 95.7791, 153.4764],\n",
      "         [ 97.8408, 150.4335],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.0719, 151.7258],\n",
      "         [  0.0000,   0.0000],\n",
      "         [106.7683, 167.2587],\n",
      "         [122.9412, 164.6929],\n",
      "         [106.7297, 187.5897],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.0154, 198.8759],\n",
      "         [  0.0000,   0.0000],\n",
      "         [126.9956, 208.0268],\n",
      "         [137.0941, 205.9971],\n",
      "         [107.4510, 241.2549],\n",
      "         [116.0978, 237.0182],\n",
      "         [135.1437, 262.5996],\n",
      "         [131.6479, 258.9044]]])\n",
      "xyn: tensor([[[0.2661, 0.4263],\n",
      "         [0.2718, 0.4179],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2891, 0.4215],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2966, 0.4646],\n",
      "         [0.3415, 0.4575],\n",
      "         [0.2965, 0.5211],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2778, 0.5524],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3528, 0.5779],\n",
      "         [0.3808, 0.5722],\n",
      "         [0.2985, 0.6702],\n",
      "         [0.3225, 0.6584],\n",
      "         [0.3754, 0.7294],\n",
      "         [0.3657, 0.7192]]])\n",
      "\n",
      "0: 640x640 1 person, 119.1ms\n",
      "Speed: 1.3ms preprocess, 119.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[103, 148, 197],\n",
      "        [104, 150, 198],\n",
      "        [108, 153, 202],\n",
      "        ...,\n",
      "        [130, 161, 192],\n",
      "        [129, 160, 190],\n",
      "        [129, 160, 190]],\n",
      "\n",
      "       [[104, 150, 198],\n",
      "        [105, 151, 200],\n",
      "        [107, 152, 201],\n",
      "        ...,\n",
      "        [130, 161, 192],\n",
      "        [129, 160, 190],\n",
      "        [129, 160, 190]],\n",
      "\n",
      "       [[107, 148, 196],\n",
      "        [108, 150, 197],\n",
      "        [108, 150, 197],\n",
      "        ...,\n",
      "        [130, 161, 194],\n",
      "        [129, 160, 193],\n",
      "        [127, 159, 192]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3010501861572266, 'inference': 119.10605430603027, 'postprocess': 0.4112720489501953}]\n",
      "Bounding Box: tensor([[131., 213.,  78., 148.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8095, 0.8955, 0.2292, 0.9054, 0.0556, 0.9871, 0.7561, 0.9817, 0.4243, 0.9620, 0.4829, 0.9968, 0.9869, 0.9950, 0.9767, 0.9818, 0.9509]])\n",
      "data: tensor([[[9.5102e+01, 1.5421e+02, 8.0948e-01],\n",
      "         [9.7757e+01, 1.5088e+02, 8.9547e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2923e-01],\n",
      "         [1.0440e+02, 1.5196e+02, 9.0538e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.5630e-02],\n",
      "         [1.0908e+02, 1.6621e+02, 9.8705e-01],\n",
      "         [1.1569e+02, 1.6374e+02, 7.5613e-01],\n",
      "         [1.1408e+02, 1.8681e+02, 9.8165e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2433e-01],\n",
      "         [1.0241e+02, 1.9906e+02, 9.6201e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8289e-01],\n",
      "         [1.2866e+02, 2.0909e+02, 9.9680e-01],\n",
      "         [1.3128e+02, 2.0669e+02, 9.8692e-01],\n",
      "         [1.1353e+02, 2.4878e+02, 9.9497e-01],\n",
      "         [1.1249e+02, 2.4404e+02, 9.7668e-01],\n",
      "         [1.5690e+02, 2.6556e+02, 9.8184e-01],\n",
      "         [1.2760e+02, 2.6916e+02, 9.5094e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 95.1016, 154.2107],\n",
      "         [ 97.7574, 150.8827],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.4031, 151.9582],\n",
      "         [  0.0000,   0.0000],\n",
      "         [109.0761, 166.2131],\n",
      "         [115.6869, 163.7379],\n",
      "         [114.0812, 186.8138],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.4074, 199.0590],\n",
      "         [  0.0000,   0.0000],\n",
      "         [128.6635, 209.0926],\n",
      "         [131.2837, 206.6897],\n",
      "         [113.5275, 248.7823],\n",
      "         [112.4893, 244.0371],\n",
      "         [156.8954, 265.5558],\n",
      "         [127.5970, 269.1568]]])\n",
      "xyn: tensor([[[0.2642, 0.4284],\n",
      "         [0.2715, 0.4191],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2900, 0.4221],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3030, 0.4617],\n",
      "         [0.3214, 0.4548],\n",
      "         [0.3169, 0.5189],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2845, 0.5529],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3574, 0.5808],\n",
      "         [0.3647, 0.5741],\n",
      "         [0.3154, 0.6911],\n",
      "         [0.3125, 0.6779],\n",
      "         [0.4358, 0.7377],\n",
      "         [0.3544, 0.7477]]])\n",
      "\n",
      "0: 640x640 1 person, 113.0ms\n",
      "Speed: 1.3ms preprocess, 113.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[113, 155, 205],\n",
      "        [113, 155, 205],\n",
      "        [115, 157, 207],\n",
      "        ...,\n",
      "        [127, 159, 194],\n",
      "        [126, 158, 193],\n",
      "        [125, 157, 192]],\n",
      "\n",
      "       [[113, 155, 205],\n",
      "        [115, 157, 207],\n",
      "        [116, 158, 208],\n",
      "        ...,\n",
      "        [126, 158, 193],\n",
      "        [125, 157, 192],\n",
      "        [125, 157, 192]],\n",
      "\n",
      "       [[117, 158, 205],\n",
      "        [118, 159, 207],\n",
      "        [120, 161, 209],\n",
      "        ...,\n",
      "        [126, 158, 193],\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2516975402832031, 'inference': 113.03997039794922, 'postprocess': 0.4069805145263672}]\n",
      "Bounding Box: tensor([[135.5000, 212.0000,  87.0000, 146.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8049, 0.8825, 0.2369, 0.8486, 0.0582, 0.9855, 0.7121, 0.9878, 0.5247, 0.9804, 0.6260, 0.9985, 0.9929, 0.9985, 0.9918, 0.9959, 0.9878]])\n",
      "data: tensor([[[9.8130e+01, 1.5372e+02, 8.0487e-01],\n",
      "         [1.0108e+02, 1.5072e+02, 8.8245e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3694e-01],\n",
      "         [1.0720e+02, 1.5333e+02, 8.4859e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.8200e-02],\n",
      "         [1.1162e+02, 1.6703e+02, 9.8548e-01],\n",
      "         [1.1596e+02, 1.6274e+02, 7.1207e-01],\n",
      "         [1.2303e+02, 1.8537e+02, 9.8777e-01],\n",
      "         [1.3611e+02, 1.8055e+02, 5.2473e-01],\n",
      "         [1.0864e+02, 1.9805e+02, 9.8045e-01],\n",
      "         [1.4840e+02, 1.8858e+02, 6.2596e-01],\n",
      "         [1.2891e+02, 2.1052e+02, 9.9849e-01],\n",
      "         [1.2794e+02, 2.0819e+02, 9.9293e-01],\n",
      "         [1.1755e+02, 2.4643e+02, 9.9847e-01],\n",
      "         [1.0914e+02, 2.3769e+02, 9.9175e-01],\n",
      "         [1.6752e+02, 2.6428e+02, 9.9591e-01],\n",
      "         [1.1756e+02, 2.6953e+02, 9.8779e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 98.1302, 153.7150],\n",
      "         [101.0787, 150.7168],\n",
      "         [  0.0000,   0.0000],\n",
      "         [107.2036, 153.3330],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.6213, 167.0346],\n",
      "         [115.9557, 162.7419],\n",
      "         [123.0314, 185.3708],\n",
      "         [136.1123, 180.5518],\n",
      "         [108.6436, 198.0470],\n",
      "         [148.3986, 188.5781],\n",
      "         [128.9056, 210.5212],\n",
      "         [127.9447, 208.1902],\n",
      "         [117.5533, 246.4267],\n",
      "         [109.1431, 237.6883],\n",
      "         [167.5169, 264.2840],\n",
      "         [117.5621, 269.5287]]])\n",
      "xyn: tensor([[[0.2726, 0.4270],\n",
      "         [0.2808, 0.4187],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2978, 0.4259],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3101, 0.4640],\n",
      "         [0.3221, 0.4521],\n",
      "         [0.3418, 0.5149],\n",
      "         [0.3781, 0.5015],\n",
      "         [0.3018, 0.5501],\n",
      "         [0.4122, 0.5238],\n",
      "         [0.3581, 0.5848],\n",
      "         [0.3554, 0.5783],\n",
      "         [0.3265, 0.6845],\n",
      "         [0.3032, 0.6602],\n",
      "         [0.4653, 0.7341],\n",
      "         [0.3266, 0.7487]]])\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.2ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[115, 157, 207],\n",
      "        [111, 153, 203],\n",
      "        [105, 151, 201],\n",
      "        ...,\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[118, 160, 210],\n",
      "        [115, 157, 207],\n",
      "        [109, 154, 204],\n",
      "        ...,\n",
      "        [124, 155, 190],\n",
      "        [123, 154, 189],\n",
      "        [123, 154, 189]],\n",
      "\n",
      "       [[123, 164, 211],\n",
      "        [119, 160, 208],\n",
      "        [115, 155, 205],\n",
      "        ...,\n",
      "        [122, 152, 190],\n",
      "        [120, 151, 189],\n",
      "        [122, 152, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2311935424804688, 'inference': 116.48201942443848, 'postprocess': 0.41413307189941406}]\n",
      "Bounding Box: tensor([[140.5000, 228.5000, 101.0000, 137.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7239, 0.8233, 0.2404, 0.8081, 0.0527, 0.9508, 0.6892, 0.9330, 0.4198, 0.8776, 0.4100, 0.9905, 0.9750, 0.9903, 0.9709, 0.9801, 0.9605]])\n",
      "data: tensor([[[1.1341e+02, 1.7896e+02, 7.2387e-01],\n",
      "         [1.1523e+02, 1.7563e+02, 8.2327e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4040e-01],\n",
      "         [1.2246e+02, 1.7530e+02, 8.0810e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.2727e-02],\n",
      "         [1.2769e+02, 1.8409e+02, 9.5081e-01],\n",
      "         [1.2202e+02, 1.8446e+02, 6.8921e-01],\n",
      "         [1.2077e+02, 1.9425e+02, 9.3305e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1982e-01],\n",
      "         [1.1511e+02, 1.9425e+02, 8.7758e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.1003e-01],\n",
      "         [1.3148e+02, 2.1643e+02, 9.9050e-01],\n",
      "         [1.2305e+02, 2.1413e+02, 9.7495e-01],\n",
      "         [1.3016e+02, 2.4417e+02, 9.9029e-01],\n",
      "         [1.0213e+02, 2.3447e+02, 9.7086e-01],\n",
      "         [1.6351e+02, 2.6972e+02, 9.8011e-01],\n",
      "         [1.0903e+02, 2.6437e+02, 9.6049e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[113.4076, 178.9580],\n",
      "         [115.2329, 175.6325],\n",
      "         [  0.0000,   0.0000],\n",
      "         [122.4601, 175.2992],\n",
      "         [  0.0000,   0.0000],\n",
      "         [127.6910, 184.0915],\n",
      "         [122.0234, 184.4566],\n",
      "         [120.7666, 194.2482],\n",
      "         [  0.0000,   0.0000],\n",
      "         [115.1102, 194.2465],\n",
      "         [  0.0000,   0.0000],\n",
      "         [131.4756, 216.4256],\n",
      "         [123.0496, 214.1277],\n",
      "         [130.1606, 244.1677],\n",
      "         [102.1284, 234.4734],\n",
      "         [163.5101, 269.7173],\n",
      "         [109.0282, 264.3742]]])\n",
      "xyn: tensor([[[0.3150, 0.4971],\n",
      "         [0.3201, 0.4879],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3402, 0.4869],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3547, 0.5114],\n",
      "         [0.3390, 0.5124],\n",
      "         [0.3355, 0.5396],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3198, 0.5396],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3652, 0.6012],\n",
      "         [0.3418, 0.5948],\n",
      "         [0.3616, 0.6782],\n",
      "         [0.2837, 0.6513],\n",
      "         [0.4542, 0.7492],\n",
      "         [0.3029, 0.7344]]])\n",
      "\n",
      "0: 640x640 1 person, 115.0ms\n",
      "Speed: 1.5ms preprocess, 115.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 98, 143, 196],\n",
      "        [ 97, 141, 195],\n",
      "        [ 94, 143, 194],\n",
      "        ...,\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[ 98, 143, 196],\n",
      "        [ 97, 141, 195],\n",
      "        [ 94, 143, 194],\n",
      "        ...,\n",
      "        [123, 154, 189],\n",
      "        [122, 153, 188],\n",
      "        [123, 154, 189]],\n",
      "\n",
      "       [[ 98, 144, 194],\n",
      "        [ 97, 143, 193],\n",
      "        [ 94, 143, 194],\n",
      "        ...,\n",
      "        [120, 152, 187],\n",
      "        [119, 151, 186],\n",
      "        [120, 152, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.5459060668945312, 'inference': 114.99786376953125, 'postprocess': 0.42510032653808594}]\n",
      "Bounding Box: tensor([[139., 218., 108., 154.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7435, 0.8142, 0.2064, 0.7966, 0.0666, 0.9882, 0.7973, 0.9894, 0.6068, 0.9761, 0.6363, 0.9988, 0.9949, 0.9980, 0.9895, 0.9921, 0.9782]])\n",
      "data: tensor([[[1.0635e+02, 1.5360e+02, 7.4354e-01],\n",
      "         [1.0894e+02, 1.5006e+02, 8.1419e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0638e-01],\n",
      "         [1.1556e+02, 1.5292e+02, 7.9665e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6633e-02],\n",
      "         [1.2922e+02, 1.7044e+02, 9.8820e-01],\n",
      "         [1.1044e+02, 1.6853e+02, 7.9731e-01],\n",
      "         [1.4090e+02, 1.8497e+02, 9.8942e-01],\n",
      "         [1.1810e+02, 1.8656e+02, 6.0680e-01],\n",
      "         [1.2524e+02, 1.9547e+02, 9.7608e-01],\n",
      "         [1.2224e+02, 1.9598e+02, 6.3632e-01],\n",
      "         [1.3867e+02, 2.1517e+02, 9.9882e-01],\n",
      "         [1.2119e+02, 2.1098e+02, 9.9487e-01],\n",
      "         [1.3977e+02, 2.5052e+02, 9.9796e-01],\n",
      "         [9.7527e+01, 2.3235e+02, 9.8945e-01],\n",
      "         [1.7306e+02, 2.8035e+02, 9.9207e-01],\n",
      "         [1.0506e+02, 2.6702e+02, 9.7825e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[106.3524, 153.6049],\n",
      "         [108.9375, 150.0649],\n",
      "         [  0.0000,   0.0000],\n",
      "         [115.5561, 152.9153],\n",
      "         [  0.0000,   0.0000],\n",
      "         [129.2172, 170.4441],\n",
      "         [110.4436, 168.5314],\n",
      "         [140.8995, 184.9728],\n",
      "         [118.0970, 186.5591],\n",
      "         [125.2430, 195.4685],\n",
      "         [122.2414, 195.9830],\n",
      "         [138.6683, 215.1687],\n",
      "         [121.1903, 210.9836],\n",
      "         [139.7749, 250.5161],\n",
      "         [ 97.5270, 232.3466],\n",
      "         [173.0565, 280.3493],\n",
      "         [105.0627, 267.0168]]])\n",
      "xyn: tensor([[[0.2954, 0.4267],\n",
      "         [0.3026, 0.4168],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3210, 0.4248],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3589, 0.4735],\n",
      "         [0.3068, 0.4681],\n",
      "         [0.3914, 0.5138],\n",
      "         [0.3280, 0.5182],\n",
      "         [0.3479, 0.5430],\n",
      "         [0.3396, 0.5444],\n",
      "         [0.3852, 0.5977],\n",
      "         [0.3366, 0.5861],\n",
      "         [0.3883, 0.6959],\n",
      "         [0.2709, 0.6454],\n",
      "         [0.4807, 0.7787],\n",
      "         [0.2918, 0.7417]]])\n",
      "\n",
      "0: 640x640 1 person, 118.2ms\n",
      "Speed: 1.5ms preprocess, 118.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 90, 144, 196],\n",
      "        [ 90, 144, 196],\n",
      "        [ 89, 145, 198],\n",
      "        ...,\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[ 89, 143, 195],\n",
      "        [ 89, 143, 195],\n",
      "        [ 88, 144, 197],\n",
      "        ...,\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[ 89, 143, 195],\n",
      "        [ 89, 143, 195],\n",
      "        [ 88, 144, 197],\n",
      "        ...,\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.5461444854736328, 'inference': 118.18599700927734, 'postprocess': 0.43320655822753906}]\n",
      "Bounding Box: tensor([[139., 221., 116., 164.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9484, 0.9377, 0.7368, 0.8103, 0.3255, 0.9936, 0.9503, 0.9899, 0.8539, 0.9797, 0.8484, 0.9991, 0.9974, 0.9980, 0.9931, 0.9910, 0.9804]])\n",
      "data: tensor([[[114.7559, 149.6694,   0.9484],\n",
      "         [117.3026, 146.4222,   0.9377],\n",
      "         [111.4418, 146.3329,   0.7368],\n",
      "         [120.9765, 149.9361,   0.8103],\n",
      "         [  0.0000,   0.0000,   0.3255],\n",
      "         [133.3550, 167.6796,   0.9936],\n",
      "         [108.1061, 168.8931,   0.9503],\n",
      "         [150.0552, 182.3964,   0.9899],\n",
      "         [113.5247, 187.0389,   0.8539],\n",
      "         [136.3645, 194.6821,   0.9797],\n",
      "         [113.1655, 197.5942,   0.8484],\n",
      "         [141.4727, 214.0445,   0.9991],\n",
      "         [119.6509, 211.8750,   0.9974],\n",
      "         [146.6602, 248.8539,   0.9980],\n",
      "         [ 95.2302, 236.4623,   0.9931],\n",
      "         [177.6554, 282.1860,   0.9910],\n",
      "         [101.9546, 270.6512,   0.9804]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[114.7559, 149.6694],\n",
      "         [117.3026, 146.4222],\n",
      "         [111.4418, 146.3329],\n",
      "         [120.9765, 149.9361],\n",
      "         [  0.0000,   0.0000],\n",
      "         [133.3550, 167.6796],\n",
      "         [108.1061, 168.8931],\n",
      "         [150.0552, 182.3964],\n",
      "         [113.5247, 187.0389],\n",
      "         [136.3645, 194.6821],\n",
      "         [113.1655, 197.5942],\n",
      "         [141.4727, 214.0445],\n",
      "         [119.6509, 211.8750],\n",
      "         [146.6602, 248.8539],\n",
      "         [ 95.2302, 236.4623],\n",
      "         [177.6554, 282.1860],\n",
      "         [101.9546, 270.6512]]])\n",
      "xyn: tensor([[[0.3188, 0.4157],\n",
      "         [0.3258, 0.4067],\n",
      "         [0.3096, 0.4065],\n",
      "         [0.3360, 0.4165],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3704, 0.4658],\n",
      "         [0.3003, 0.4691],\n",
      "         [0.4168, 0.5067],\n",
      "         [0.3153, 0.5196],\n",
      "         [0.3788, 0.5408],\n",
      "         [0.3143, 0.5489],\n",
      "         [0.3930, 0.5946],\n",
      "         [0.3324, 0.5885],\n",
      "         [0.4074, 0.6913],\n",
      "         [0.2645, 0.6568],\n",
      "         [0.4935, 0.7838],\n",
      "         [0.2832, 0.7518]]])\n",
      "\n",
      "0: 640x640 2 persons, 116.0ms\n",
      "Speed: 1.5ms preprocess, 116.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 90, 145, 201],\n",
      "        [ 89, 144, 200],\n",
      "        [ 87, 141, 197],\n",
      "        ...,\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[ 90, 145, 201],\n",
      "        [ 89, 144, 200],\n",
      "        [ 87, 141, 197],\n",
      "        ...,\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       [[ 89, 144, 200],\n",
      "        [ 89, 144, 200],\n",
      "        [ 88, 143, 198],\n",
      "        ...,\n",
      "        [125, 157, 192],\n",
      "        [124, 155, 190],\n",
      "        [124, 155, 190]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4660358428955078, 'inference': 115.97204208374023, 'postprocess': 0.4107952117919922}]\n",
      "Bounding Box: tensor([[135.5000, 220.0000, 117.0000, 168.0000]])\n",
      "Bounding Box: tensor([[131.0000, 173.5000,  62.0000,  73.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9694, 0.9082, 0.8843, 0.6238, 0.5972, 0.9934, 0.9908, 0.9894, 0.9817, 0.9839, 0.9767, 0.9989, 0.9986, 0.9959, 0.9936, 0.9735, 0.9654]])\n",
      "data: tensor([[[122.4182, 148.1524,   0.9694],\n",
      "         [125.0988, 144.8750,   0.9082],\n",
      "         [119.6552, 145.3815,   0.8843],\n",
      "         [129.1100, 147.8514,   0.6238],\n",
      "         [114.2603, 148.9528,   0.5972],\n",
      "         [140.2608, 166.2928,   0.9934],\n",
      "         [109.7965, 168.3857,   0.9908],\n",
      "         [156.8915, 184.1548,   0.9894],\n",
      "         [108.9035, 186.5740,   0.9817],\n",
      "         [142.5009, 193.4117,   0.9839],\n",
      "         [104.6787, 198.4848,   0.9767],\n",
      "         [143.9099, 211.8298,   0.9989],\n",
      "         [118.3600, 211.2648,   0.9986],\n",
      "         [157.2090, 243.2778,   0.9959],\n",
      "         [ 95.0854, 240.2172,   0.9936],\n",
      "         [176.6423, 279.9042,   0.9735],\n",
      "         [ 91.8682, 269.9394,   0.9654]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[122.4182, 148.1524],\n",
      "         [125.0988, 144.8750],\n",
      "         [119.6552, 145.3815],\n",
      "         [129.1100, 147.8514],\n",
      "         [114.2603, 148.9528],\n",
      "         [140.2608, 166.2928],\n",
      "         [109.7965, 168.3857],\n",
      "         [156.8915, 184.1548],\n",
      "         [108.9035, 186.5740],\n",
      "         [142.5009, 193.4117],\n",
      "         [104.6787, 198.4848],\n",
      "         [143.9099, 211.8298],\n",
      "         [118.3600, 211.2648],\n",
      "         [157.2090, 243.2778],\n",
      "         [ 95.0854, 240.2172],\n",
      "         [176.6423, 279.9042],\n",
      "         [ 91.8682, 269.9394]]])\n",
      "xyn: tensor([[[0.3401, 0.4115],\n",
      "         [0.3475, 0.4024],\n",
      "         [0.3324, 0.4038],\n",
      "         [0.3586, 0.4107],\n",
      "         [0.3174, 0.4138],\n",
      "         [0.3896, 0.4619],\n",
      "         [0.3050, 0.4677],\n",
      "         [0.4358, 0.5115],\n",
      "         [0.3025, 0.5183],\n",
      "         [0.3958, 0.5373],\n",
      "         [0.2908, 0.5513],\n",
      "         [0.3997, 0.5884],\n",
      "         [0.3288, 0.5868],\n",
      "         [0.4367, 0.6758],\n",
      "         [0.2641, 0.6673],\n",
      "         [0.4907, 0.7775],\n",
      "         [0.2552, 0.7498]]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9692, 0.9477, 0.9515, 0.6925, 0.7205, 0.9885, 0.9860, 0.9681, 0.9509, 0.9424, 0.9197, 0.9217, 0.9164, 0.2196, 0.2010, 0.0233, 0.0201]])\n",
      "data: tensor([[[1.2265e+02, 1.4752e+02, 9.6916e-01],\n",
      "         [1.2522e+02, 1.4477e+02, 9.4771e-01],\n",
      "         [1.1974e+02, 1.4503e+02, 9.5148e-01],\n",
      "         [1.2881e+02, 1.4684e+02, 6.9248e-01],\n",
      "         [1.1550e+02, 1.4814e+02, 7.2045e-01],\n",
      "         [1.3735e+02, 1.6396e+02, 9.8851e-01],\n",
      "         [1.1138e+02, 1.6683e+02, 9.8604e-01],\n",
      "         [1.5635e+02, 1.8286e+02, 9.6810e-01],\n",
      "         [1.0713e+02, 1.8617e+02, 9.5092e-01],\n",
      "         [1.4046e+02, 1.9462e+02, 9.4243e-01],\n",
      "         [1.0664e+02, 1.9803e+02, 9.1968e-01],\n",
      "         [1.3574e+02, 2.0786e+02, 9.2170e-01],\n",
      "         [1.1789e+02, 2.0853e+02, 9.1641e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1955e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0104e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3350e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0131e-02]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[122.6522, 147.5228],\n",
      "         [125.2197, 144.7737],\n",
      "         [119.7386, 145.0315],\n",
      "         [128.8142, 146.8420],\n",
      "         [115.4951, 148.1351],\n",
      "         [137.3484, 163.9594],\n",
      "         [111.3839, 166.8323],\n",
      "         [156.3508, 182.8603],\n",
      "         [107.1337, 186.1703],\n",
      "         [140.4576, 194.6194],\n",
      "         [106.6366, 198.0250],\n",
      "         [135.7419, 207.8638],\n",
      "         [117.8872, 208.5309],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000]]])\n",
      "xyn: tensor([[[0.3407, 0.4098],\n",
      "         [0.3478, 0.4021],\n",
      "         [0.3326, 0.4029],\n",
      "         [0.3578, 0.4079],\n",
      "         [0.3208, 0.4115],\n",
      "         [0.3815, 0.4554],\n",
      "         [0.3094, 0.4634],\n",
      "         [0.4343, 0.5079],\n",
      "         [0.2976, 0.5171],\n",
      "         [0.3902, 0.5406],\n",
      "         [0.2962, 0.5501],\n",
      "         [0.3771, 0.5774],\n",
      "         [0.3275, 0.5793],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "\n",
      "0: 640x640 1 person, 112.4ms\n",
      "Speed: 1.5ms preprocess, 112.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 96, 150, 202],\n",
      "        [ 98, 152, 204],\n",
      "        [104, 158, 208],\n",
      "        ...,\n",
      "        [120, 152, 187],\n",
      "        [118, 150, 185],\n",
      "        [119, 151, 186]],\n",
      "\n",
      "       [[ 96, 150, 202],\n",
      "        [100, 153, 205],\n",
      "        [104, 158, 208],\n",
      "        ...,\n",
      "        [119, 151, 186],\n",
      "        [117, 148, 183],\n",
      "        [118, 150, 185]],\n",
      "\n",
      "       [[ 95, 148, 201],\n",
      "        [100, 153, 205],\n",
      "        [104, 158, 208],\n",
      "        ...,\n",
      "        [118, 150, 185],\n",
      "        [116, 147, 182],\n",
      "        [117, 148, 183]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4641284942626953, 'inference': 112.40720748901367, 'postprocess': 0.39196014404296875}]\n",
      "Bounding Box: tensor([[132.5000, 220.0000, 113.0000, 166.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9725, 0.8877, 0.8924, 0.5481, 0.6243, 0.9946, 0.9964, 0.9914, 0.9940, 0.9882, 0.9909, 0.9996, 0.9996, 0.9982, 0.9981, 0.9863, 0.9854]])\n",
      "data: tensor([[[129.1290, 154.0214,   0.9725],\n",
      "         [131.5645, 150.4500,   0.8877],\n",
      "         [126.4935, 151.2089,   0.8924],\n",
      "         [135.6443, 150.2834,   0.5481],\n",
      "         [121.2062, 152.1018,   0.6243],\n",
      "         [144.5107, 165.5587,   0.9946],\n",
      "         [115.6917, 168.8243,   0.9964],\n",
      "         [159.3156, 183.6906,   0.9914],\n",
      "         [103.9686, 189.1921,   0.9940],\n",
      "         [148.9215, 192.2432,   0.9882],\n",
      "         [ 91.8877, 202.9147,   0.9909],\n",
      "         [145.4651, 213.1683,   0.9996],\n",
      "         [122.3931, 213.2538,   0.9996],\n",
      "         [156.7442, 243.4223,   0.9982],\n",
      "         [102.1372, 242.4338,   0.9981],\n",
      "         [173.7689, 280.6638,   0.9863],\n",
      "         [ 93.0693, 274.6338,   0.9854]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[129.1290, 154.0214],\n",
      "         [131.5645, 150.4500],\n",
      "         [126.4935, 151.2089],\n",
      "         [135.6443, 150.2834],\n",
      "         [121.2062, 152.1018],\n",
      "         [144.5107, 165.5587],\n",
      "         [115.6917, 168.8243],\n",
      "         [159.3156, 183.6906],\n",
      "         [103.9686, 189.1921],\n",
      "         [148.9215, 192.2432],\n",
      "         [ 91.8877, 202.9147],\n",
      "         [145.4651, 213.1683],\n",
      "         [122.3931, 213.2538],\n",
      "         [156.7442, 243.4223],\n",
      "         [102.1372, 242.4338],\n",
      "         [173.7689, 280.6638],\n",
      "         [ 93.0693, 274.6338]]])\n",
      "xyn: tensor([[[0.3587, 0.4278],\n",
      "         [0.3655, 0.4179],\n",
      "         [0.3514, 0.4200],\n",
      "         [0.3768, 0.4175],\n",
      "         [0.3367, 0.4225],\n",
      "         [0.4014, 0.4599],\n",
      "         [0.3214, 0.4690],\n",
      "         [0.4425, 0.5103],\n",
      "         [0.2888, 0.5255],\n",
      "         [0.4137, 0.5340],\n",
      "         [0.2552, 0.5637],\n",
      "         [0.4041, 0.5921],\n",
      "         [0.3400, 0.5924],\n",
      "         [0.4354, 0.6762],\n",
      "         [0.2837, 0.6734],\n",
      "         [0.4827, 0.7796],\n",
      "         [0.2585, 0.7629]]])\n",
      "\n",
      "0: 640x640 1 person, 112.1ms\n",
      "Speed: 1.3ms preprocess, 112.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[113, 167, 219],\n",
      "        [113, 167, 219],\n",
      "        [112, 166, 218],\n",
      "        ...,\n",
      "        [111, 147, 193],\n",
      "        [111, 147, 193],\n",
      "        [108, 144, 189]],\n",
      "\n",
      "       [[113, 167, 219],\n",
      "        [113, 167, 219],\n",
      "        [112, 166, 218],\n",
      "        ...,\n",
      "        [110, 146, 192],\n",
      "        [110, 146, 192],\n",
      "        [108, 144, 189]],\n",
      "\n",
      "       [[113, 167, 219],\n",
      "        [113, 167, 219],\n",
      "        [113, 167, 219],\n",
      "        ...,\n",
      "        [108, 146, 192],\n",
      "        [108, 145, 194],\n",
      "        [105, 143, 192]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.25885009765625, 'inference': 112.08987236022949, 'postprocess': 0.6289482116699219}]\n",
      "Bounding Box: tensor([[130.0000, 218.5000, 112.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9192, 0.7145, 0.7085, 0.3623, 0.3993, 0.9892, 0.9936, 0.9891, 0.9930, 0.9864, 0.9901, 0.9995, 0.9996, 0.9979, 0.9978, 0.9848, 0.9840]])\n",
      "data: tensor([[[134.8066, 152.6393,   0.9192],\n",
      "         [137.8355, 149.7570,   0.7145],\n",
      "         [132.8389, 150.3589,   0.7085],\n",
      "         [  0.0000,   0.0000,   0.3623],\n",
      "         [  0.0000,   0.0000,   0.3993],\n",
      "         [149.0380, 166.0189,   0.9892],\n",
      "         [123.5216, 166.8380,   0.9936],\n",
      "         [159.7830, 186.9568,   0.9891],\n",
      "         [104.1225, 186.0367,   0.9930],\n",
      "         [152.4386, 196.6969,   0.9864],\n",
      "         [ 85.3995, 204.7683,   0.9901],\n",
      "         [147.4982, 211.3700,   0.9995],\n",
      "         [126.9263, 211.2065,   0.9996],\n",
      "         [156.2158, 238.6471,   0.9979],\n",
      "         [112.3908, 241.6538,   0.9978],\n",
      "         [171.0081, 274.0950,   0.9848],\n",
      "         [ 89.8129, 269.9369,   0.9840]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[134.8066, 152.6393],\n",
      "         [137.8355, 149.7570],\n",
      "         [132.8389, 150.3589],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [149.0380, 166.0189],\n",
      "         [123.5216, 166.8380],\n",
      "         [159.7830, 186.9568],\n",
      "         [104.1225, 186.0367],\n",
      "         [152.4386, 196.6969],\n",
      "         [ 85.3995, 204.7683],\n",
      "         [147.4982, 211.3700],\n",
      "         [126.9263, 211.2065],\n",
      "         [156.2158, 238.6471],\n",
      "         [112.3908, 241.6538],\n",
      "         [171.0081, 274.0950],\n",
      "         [ 89.8129, 269.9369]]])\n",
      "xyn: tensor([[[0.3745, 0.4240],\n",
      "         [0.3829, 0.4160],\n",
      "         [0.3690, 0.4177],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4140, 0.4612],\n",
      "         [0.3431, 0.4634],\n",
      "         [0.4438, 0.5193],\n",
      "         [0.2892, 0.5168],\n",
      "         [0.4234, 0.5464],\n",
      "         [0.2372, 0.5688],\n",
      "         [0.4097, 0.5871],\n",
      "         [0.3526, 0.5867],\n",
      "         [0.4339, 0.6629],\n",
      "         [0.3122, 0.6713],\n",
      "         [0.4750, 0.7614],\n",
      "         [0.2495, 0.7498]]])\n",
      "\n",
      "0: 640x640 1 person, 116.6ms\n",
      "Speed: 1.3ms preprocess, 116.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[112, 166, 218],\n",
      "        [113, 167, 219],\n",
      "        [113, 167, 217],\n",
      "        ...,\n",
      "        [104, 141, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[113, 167, 219],\n",
      "        [115, 168, 221],\n",
      "        [113, 167, 217],\n",
      "        ...,\n",
      "        [104, 141, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[113, 167, 219],\n",
      "        [115, 168, 221],\n",
      "        [113, 167, 217],\n",
      "        ...,\n",
      "        [ 97, 145, 193],\n",
      "        [ 98, 144, 193],\n",
      "        [ 98, 144, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 84,  89,  37],\n",
      "        [ 89,  94,  41],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 86,  90,  38],\n",
      "        [ 88,  93,  40],\n",
      "        [ 83,  88,  35]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 89,  94,  41],\n",
      "        [ 90,  95,  42]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2700557708740234, 'inference': 116.56713485717773, 'postprocess': 0.40602684020996094}]\n",
      "Bounding Box: tensor([[129.5000, 216.5000, 105.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8740, 0.4632, 0.8310, 0.1570, 0.7480, 0.9364, 0.9968, 0.8243, 0.9953, 0.7788, 0.9861, 0.9975, 0.9994, 0.9912, 0.9975, 0.9660, 0.9852]])\n",
      "data: tensor([[[1.4315e+02, 1.5197e+02, 8.7396e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6322e-01],\n",
      "         [1.4129e+02, 1.4955e+02, 8.3105e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5702e-01],\n",
      "         [1.3490e+02, 1.5051e+02, 7.4800e-01],\n",
      "         [1.4590e+02, 1.6531e+02, 9.3641e-01],\n",
      "         [1.2725e+02, 1.6705e+02, 9.9675e-01],\n",
      "         [1.5614e+02, 1.8519e+02, 8.2425e-01],\n",
      "         [1.0890e+02, 1.8508e+02, 9.9533e-01],\n",
      "         [1.5521e+02, 1.9086e+02, 7.7877e-01],\n",
      "         [8.9733e+01, 1.9897e+02, 9.8607e-01],\n",
      "         [1.4603e+02, 2.1261e+02, 9.9755e-01],\n",
      "         [1.2979e+02, 2.1375e+02, 9.9942e-01],\n",
      "         [1.5801e+02, 2.4253e+02, 9.9124e-01],\n",
      "         [1.1593e+02, 2.4982e+02, 9.9746e-01],\n",
      "         [1.6792e+02, 2.7613e+02, 9.6597e-01],\n",
      "         [8.9456e+01, 2.6630e+02, 9.8519e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[143.1530, 151.9702],\n",
      "         [  0.0000,   0.0000],\n",
      "         [141.2858, 149.5541],\n",
      "         [  0.0000,   0.0000],\n",
      "         [134.8961, 150.5099],\n",
      "         [145.9016, 165.3088],\n",
      "         [127.2536, 167.0500],\n",
      "         [156.1423, 185.1898],\n",
      "         [108.8974, 185.0782],\n",
      "         [155.2112, 190.8583],\n",
      "         [ 89.7333, 198.9664],\n",
      "         [146.0310, 212.6114],\n",
      "         [129.7874, 213.7468],\n",
      "         [158.0106, 242.5309],\n",
      "         [115.9323, 249.8172],\n",
      "         [167.9187, 276.1313],\n",
      "         [ 89.4563, 266.2954]]])\n",
      "xyn: tensor([[[0.3976, 0.4221],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3925, 0.4154],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3747, 0.4181],\n",
      "         [0.4053, 0.4592],\n",
      "         [0.3535, 0.4640],\n",
      "         [0.4337, 0.5144],\n",
      "         [0.3025, 0.5141],\n",
      "         [0.4311, 0.5302],\n",
      "         [0.2493, 0.5527],\n",
      "         [0.4056, 0.5906],\n",
      "         [0.3605, 0.5937],\n",
      "         [0.4389, 0.6737],\n",
      "         [0.3220, 0.6939],\n",
      "         [0.4664, 0.7670],\n",
      "         [0.2485, 0.7397]]])\n",
      "\n",
      "0: 640x640 1 person, 140.6ms\n",
      "Speed: 1.4ms preprocess, 140.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[117, 165, 214],\n",
      "        [109, 157, 205],\n",
      "        [109, 151, 195],\n",
      "        ...,\n",
      "        [104, 141, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[117, 165, 214],\n",
      "        [109, 157, 205],\n",
      "        [109, 151, 195],\n",
      "        ...,\n",
      "        [104, 141, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[117, 165, 214],\n",
      "        [109, 157, 205],\n",
      "        [109, 151, 195],\n",
      "        ...,\n",
      "        [ 97, 145, 193],\n",
      "        [ 98, 144, 193],\n",
      "        [ 98, 144, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 86,  90,  38]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 89,  94,  41],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 87,  91,  39],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3768672943115234, 'inference': 140.60306549072266, 'postprocess': 0.4150867462158203}]\n",
      "Bounding Box: tensor([[130.0000, 219.5000,  98.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1276, 0.0469, 0.0958, 0.1919, 0.4886, 0.8409, 0.9759, 0.5181, 0.9252, 0.2039, 0.6608, 0.9953, 0.9983, 0.9889, 0.9952, 0.9769, 0.9873]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.2761e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6900e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 9.5790e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9191e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8858e-01],\n",
      "         [1.3787e+02, 1.6962e+02, 8.4093e-01],\n",
      "         [1.3923e+02, 1.7166e+02, 9.7592e-01],\n",
      "         [1.3963e+02, 1.8152e+02, 5.1806e-01],\n",
      "         [1.3528e+02, 1.8085e+02, 9.2515e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0385e-01],\n",
      "         [1.2049e+02, 1.8012e+02, 6.6080e-01],\n",
      "         [1.4080e+02, 2.1536e+02, 9.9532e-01],\n",
      "         [1.3514e+02, 2.1607e+02, 9.9831e-01],\n",
      "         [1.5962e+02, 2.4653e+02, 9.8886e-01],\n",
      "         [1.2083e+02, 2.5080e+02, 9.9520e-01],\n",
      "         [1.6575e+02, 2.8195e+02, 9.7691e-01],\n",
      "         [9.1424e+01, 2.6337e+02, 9.8734e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [137.8722, 169.6237],\n",
      "         [139.2291, 171.6597],\n",
      "         [139.6257, 181.5191],\n",
      "         [135.2757, 180.8525],\n",
      "         [  0.0000,   0.0000],\n",
      "         [120.4890, 180.1205],\n",
      "         [140.7964, 215.3603],\n",
      "         [135.1360, 216.0704],\n",
      "         [159.6207, 246.5279],\n",
      "         [120.8313, 250.7958],\n",
      "         [165.7502, 281.9478],\n",
      "         [ 91.4236, 263.3741]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3830, 0.4712],\n",
      "         [0.3867, 0.4768],\n",
      "         [0.3878, 0.5042],\n",
      "         [0.3758, 0.5024],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3347, 0.5003],\n",
      "         [0.3911, 0.5982],\n",
      "         [0.3754, 0.6002],\n",
      "         [0.4434, 0.6848],\n",
      "         [0.3356, 0.6967],\n",
      "         [0.4604, 0.7832],\n",
      "         [0.2540, 0.7316]]])\n",
      "\n",
      "0: 640x640 1 person, 117.6ms\n",
      "Speed: 1.2ms preprocess, 117.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[147, 180, 217],\n",
      "        [136, 168, 205],\n",
      "        [123, 154, 189],\n",
      "        ...,\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[147, 180, 217],\n",
      "        [136, 168, 205],\n",
      "        [123, 154, 189],\n",
      "        ...,\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193],\n",
      "        [105, 140, 193]],\n",
      "\n",
      "       [[147, 180, 217],\n",
      "        [136, 168, 205],\n",
      "        [123, 154, 189],\n",
      "        ...,\n",
      "        [ 98, 144, 193],\n",
      "        [ 98, 144, 193],\n",
      "        [ 98, 144, 193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 84,  89,  37],\n",
      "        [ 88,  93,  40],\n",
      "        [ 83,  88,  35]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 88,  93,  40],\n",
      "        [ 83,  88,  35]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 88,  93,  40],\n",
      "        [ 89,  94,  41]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2431144714355469, 'inference': 117.55585670471191, 'postprocess': 0.3998279571533203}]\n",
      "Bounding Box: tensor([[131.0000, 214.5000,  98.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0916, 0.0090, 0.1173, 0.0500, 0.7924, 0.8534, 0.9967, 0.5340, 0.9942, 0.2671, 0.9528, 0.9978, 0.9997, 0.9902, 0.9982, 0.9700, 0.9901]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 9.1603e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 9.0193e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1730e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0014e-02],\n",
      "         [1.4624e+02, 1.4921e+02, 7.9244e-01],\n",
      "         [1.2977e+02, 1.6040e+02, 8.5336e-01],\n",
      "         [1.4001e+02, 1.6323e+02, 9.9667e-01],\n",
      "         [1.2694e+02, 1.8775e+02, 5.3400e-01],\n",
      "         [1.3887e+02, 1.8459e+02, 9.9423e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6711e-01],\n",
      "         [1.4048e+02, 2.0856e+02, 9.5281e-01],\n",
      "         [1.3475e+02, 2.0924e+02, 9.9784e-01],\n",
      "         [1.3454e+02, 2.1217e+02, 9.9967e-01],\n",
      "         [1.5627e+02, 2.3701e+02, 9.9017e-01],\n",
      "         [1.2271e+02, 2.5079e+02, 9.9819e-01],\n",
      "         [1.6353e+02, 2.7766e+02, 9.6997e-01],\n",
      "         [8.8705e+01, 2.6055e+02, 9.9009e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [146.2435, 149.2126],\n",
      "         [129.7655, 160.4018],\n",
      "         [140.0136, 163.2292],\n",
      "         [126.9414, 187.7513],\n",
      "         [138.8684, 184.5950],\n",
      "         [  0.0000,   0.0000],\n",
      "         [140.4799, 208.5551],\n",
      "         [134.7467, 209.2350],\n",
      "         [134.5359, 212.1736],\n",
      "         [156.2699, 237.0100],\n",
      "         [122.7051, 250.7868],\n",
      "         [163.5315, 277.6610],\n",
      "         [ 88.7052, 260.5462]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4062, 0.4145],\n",
      "         [0.3605, 0.4456],\n",
      "         [0.3889, 0.4534],\n",
      "         [0.3526, 0.5215],\n",
      "         [0.3857, 0.5128],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3902, 0.5793],\n",
      "         [0.3743, 0.5812],\n",
      "         [0.3737, 0.5894],\n",
      "         [0.4341, 0.6584],\n",
      "         [0.3408, 0.6966],\n",
      "         [0.4543, 0.7713],\n",
      "         [0.2464, 0.7237]]])\n",
      "\n",
      "0: 640x640 1 person, 115.7ms\n",
      "Speed: 1.3ms preprocess, 115.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[107, 141, 187],\n",
      "        [123, 158, 203],\n",
      "        [109, 147, 193],\n",
      "        ...,\n",
      "        [ 89, 139, 186],\n",
      "        [ 90, 140, 187],\n",
      "        [ 90, 140, 187]],\n",
      "\n",
      "       [[107, 141, 187],\n",
      "        [123, 158, 203],\n",
      "        [109, 147, 193],\n",
      "        ...,\n",
      "        [ 86, 136, 182],\n",
      "        [ 86, 136, 182],\n",
      "        [ 87, 137, 183]],\n",
      "\n",
      "       [[104, 140, 186],\n",
      "        [120, 157, 202],\n",
      "        [109, 147, 193],\n",
      "        ...,\n",
      "        [ 79, 131, 179],\n",
      "        [ 80, 132, 180],\n",
      "        [ 80, 132, 180]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 90,  95,  42],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 89,  94,  41],\n",
      "        [ 87,  91,  39]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2729167938232422, 'inference': 115.74101448059082, 'postprocess': 0.42819976806640625}]\n",
      "Bounding Box: tensor([[150.5000, 206.5000, 143.0000, 171.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4261, 0.0519, 0.4509, 0.0620, 0.8605, 0.9488, 0.9940, 0.7218, 0.9916, 0.5719, 0.9635, 0.9962, 0.9989, 0.9862, 0.9966, 0.9467, 0.9764]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.2609e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.1895e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5088e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.1969e-02],\n",
      "         [1.3870e+02, 1.4392e+02, 8.6047e-01],\n",
      "         [1.4041e+02, 1.6132e+02, 9.4882e-01],\n",
      "         [1.3397e+02, 1.5900e+02, 9.9401e-01],\n",
      "         [1.6781e+02, 1.4403e+02, 7.2180e-01],\n",
      "         [1.6680e+02, 1.4385e+02, 9.9161e-01],\n",
      "         [1.9848e+02, 1.2878e+02, 5.7190e-01],\n",
      "         [1.9707e+02, 1.2473e+02, 9.6355e-01],\n",
      "         [1.3064e+02, 2.1343e+02, 9.9617e-01],\n",
      "         [1.2778e+02, 2.1422e+02, 9.9889e-01],\n",
      "         [1.4099e+02, 2.4302e+02, 9.8624e-01],\n",
      "         [1.5143e+02, 2.4516e+02, 9.9661e-01],\n",
      "         [1.1151e+02, 2.6184e+02, 9.4670e-01],\n",
      "         [1.5072e+02, 2.8085e+02, 9.7639e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [138.6999, 143.9162],\n",
      "         [140.4064, 161.3177],\n",
      "         [133.9743, 158.9992],\n",
      "         [167.8105, 144.0311],\n",
      "         [166.8013, 143.8478],\n",
      "         [198.4770, 128.7752],\n",
      "         [197.0712, 124.7310],\n",
      "         [130.6396, 213.4306],\n",
      "         [127.7759, 214.2191],\n",
      "         [140.9928, 243.0224],\n",
      "         [151.4273, 245.1574],\n",
      "         [111.5119, 261.8364],\n",
      "         [150.7170, 280.8499]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3853, 0.3998],\n",
      "         [0.3900, 0.4481],\n",
      "         [0.3722, 0.4417],\n",
      "         [0.4661, 0.4001],\n",
      "         [0.4633, 0.3996],\n",
      "         [0.5513, 0.3577],\n",
      "         [0.5474, 0.3465],\n",
      "         [0.3629, 0.5929],\n",
      "         [0.3549, 0.5951],\n",
      "         [0.3916, 0.6751],\n",
      "         [0.4206, 0.6810],\n",
      "         [0.3098, 0.7273],\n",
      "         [0.4187, 0.7801]]])\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 1.5ms preprocess, 113.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[101, 148, 196],\n",
      "        [101, 148, 196],\n",
      "        [ 96, 148, 196],\n",
      "        ...,\n",
      "        [ 80, 132, 180],\n",
      "        [ 80, 132, 180],\n",
      "        [ 80, 132, 180]],\n",
      "\n",
      "       [[101, 148, 196],\n",
      "        [101, 148, 196],\n",
      "        [ 97, 150, 197],\n",
      "        ...,\n",
      "        [ 77, 130, 178],\n",
      "        [ 77, 130, 178],\n",
      "        [ 77, 130, 178]],\n",
      "\n",
      "       [[101, 148, 196],\n",
      "        [101, 148, 196],\n",
      "        [ 98, 151, 198],\n",
      "        ...,\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 83,  88,  35],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.4770030975341797, 'inference': 113.29007148742676, 'postprocess': 0.3859996795654297}]\n",
      "Bounding Box: tensor([[139.0000, 205.5000, 126.0000, 169.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4221, 0.0816, 0.4439, 0.1274, 0.8475, 0.9416, 0.9943, 0.7355, 0.9903, 0.5019, 0.9320, 0.9985, 0.9995, 0.9939, 0.9981, 0.9810, 0.9900]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.2212e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.1627e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4390e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2736e-01],\n",
      "         [1.3914e+02, 1.4402e+02, 8.4746e-01],\n",
      "         [1.3300e+02, 1.5654e+02, 9.4165e-01],\n",
      "         [1.3853e+02, 1.5734e+02, 9.9428e-01],\n",
      "         [1.5648e+02, 1.6081e+02, 7.3550e-01],\n",
      "         [1.6298e+02, 1.4570e+02, 9.9028e-01],\n",
      "         [1.6822e+02, 1.6470e+02, 5.0192e-01],\n",
      "         [1.9036e+02, 1.2876e+02, 9.3202e-01],\n",
      "         [1.2758e+02, 2.0821e+02, 9.9847e-01],\n",
      "         [1.2354e+02, 2.1058e+02, 9.9954e-01],\n",
      "         [1.5256e+02, 2.3881e+02, 9.9393e-01],\n",
      "         [1.2247e+02, 2.5251e+02, 9.9809e-01],\n",
      "         [1.5505e+02, 2.7304e+02, 9.8103e-01],\n",
      "         [8.0259e+01, 2.6649e+02, 9.9005e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [139.1404, 144.0245],\n",
      "         [133.0045, 156.5391],\n",
      "         [138.5271, 157.3408],\n",
      "         [156.4777, 160.8144],\n",
      "         [162.9800, 145.6993],\n",
      "         [168.2217, 164.6994],\n",
      "         [190.3647, 128.7555],\n",
      "         [127.5806, 208.2071],\n",
      "         [123.5392, 210.5781],\n",
      "         [152.5565, 238.8052],\n",
      "         [122.4678, 252.5079],\n",
      "         [155.0515, 273.0360],\n",
      "         [ 80.2587, 266.4930]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3865, 0.4001],\n",
      "         [0.3695, 0.4348],\n",
      "         [0.3848, 0.4371],\n",
      "         [0.4347, 0.4467],\n",
      "         [0.4527, 0.4047],\n",
      "         [0.4673, 0.4575],\n",
      "         [0.5288, 0.3577],\n",
      "         [0.3544, 0.5784],\n",
      "         [0.3432, 0.5849],\n",
      "         [0.4238, 0.6633],\n",
      "         [0.3402, 0.7014],\n",
      "         [0.4307, 0.7584],\n",
      "         [0.2229, 0.7403]]])\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.3ms preprocess, 114.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 98, 148, 196],\n",
      "        [ 98, 148, 196],\n",
      "        [ 98, 146, 194],\n",
      "        ...,\n",
      "        [ 79, 133, 180],\n",
      "        [ 79, 133, 180],\n",
      "        [ 79, 133, 180]],\n",
      "\n",
      "       [[ 98, 148, 196],\n",
      "        [ 98, 148, 196],\n",
      "        [100, 147, 195],\n",
      "        ...,\n",
      "        [ 77, 132, 179],\n",
      "        [ 77, 132, 179],\n",
      "        [ 77, 132, 179]],\n",
      "\n",
      "       [[100, 150, 197],\n",
      "        [100, 150, 197],\n",
      "        [102, 147, 196],\n",
      "        ...,\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 83,  88,  35],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2750625610351562, 'inference': 114.85505104064941, 'postprocess': 0.4050731658935547}]\n",
      "Bounding Box: tensor([[136., 207., 138., 170.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4679, 0.0501, 0.6416, 0.0510, 0.9296, 0.8727, 0.9985, 0.4331, 0.9980, 0.2617, 0.9796, 0.9985, 0.9999, 0.9949, 0.9995, 0.9875, 0.9970]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.6793e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0056e-02],\n",
      "         [1.4430e+02, 1.4172e+02, 6.4158e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0970e-02],\n",
      "         [1.3683e+02, 1.4676e+02, 9.2957e-01],\n",
      "         [1.3218e+02, 1.5909e+02, 8.7265e-01],\n",
      "         [1.3561e+02, 1.5794e+02, 9.9847e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3306e-01],\n",
      "         [1.6255e+02, 1.4529e+02, 9.9801e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6168e-01],\n",
      "         [1.8489e+02, 1.3022e+02, 9.7960e-01],\n",
      "         [1.2717e+02, 2.0913e+02, 9.9850e-01],\n",
      "         [1.2130e+02, 2.1134e+02, 9.9985e-01],\n",
      "         [1.5117e+02, 2.3630e+02, 9.9492e-01],\n",
      "         [1.1835e+02, 2.5478e+02, 9.9949e-01],\n",
      "         [1.5031e+02, 2.7172e+02, 9.8749e-01],\n",
      "         [7.8255e+01, 2.6757e+02, 9.9699e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [144.3041, 141.7160],\n",
      "         [  0.0000,   0.0000],\n",
      "         [136.8286, 146.7583],\n",
      "         [132.1776, 159.0864],\n",
      "         [135.6114, 157.9404],\n",
      "         [  0.0000,   0.0000],\n",
      "         [162.5465, 145.2914],\n",
      "         [  0.0000,   0.0000],\n",
      "         [184.8905, 130.2157],\n",
      "         [127.1686, 209.1297],\n",
      "         [121.2986, 211.3365],\n",
      "         [151.1658, 236.3028],\n",
      "         [118.3480, 254.7822],\n",
      "         [150.3071, 271.7191],\n",
      "         [ 78.2547, 267.5743]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4008, 0.3937],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3801, 0.4077],\n",
      "         [0.3672, 0.4419],\n",
      "         [0.3767, 0.4387],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4515, 0.4036],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.5136, 0.3617],\n",
      "         [0.3532, 0.5809],\n",
      "         [0.3369, 0.5870],\n",
      "         [0.4199, 0.6564],\n",
      "         [0.3287, 0.7077],\n",
      "         [0.4175, 0.7548],\n",
      "         [0.2174, 0.7433]]])\n",
      "\n",
      "0: 640x640 1 person, 119.0ms\n",
      "Speed: 1.2ms preprocess, 119.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[100, 146, 198],\n",
      "        [101, 147, 200],\n",
      "        [100, 148, 202],\n",
      "        ...,\n",
      "        [ 79, 133, 180],\n",
      "        [ 79, 133, 180],\n",
      "        [ 79, 133, 180]],\n",
      "\n",
      "       [[100, 146, 198],\n",
      "        [101, 147, 200],\n",
      "        [ 98, 147, 201],\n",
      "        ...,\n",
      "        [ 77, 132, 179],\n",
      "        [ 77, 132, 179],\n",
      "        [ 77, 132, 179]],\n",
      "\n",
      "       [[104, 148, 202],\n",
      "        [103, 147, 201],\n",
      "        [102, 148, 203],\n",
      "        ...,\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181],\n",
      "        [ 77, 131, 181]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 83,  88,  35],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2180805206298828, 'inference': 118.98326873779297, 'postprocess': 0.39887428283691406}]\n",
      "Bounding Box: tensor([[118.5000, 216.0000, 117.0000, 170.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3497, 0.0465, 0.5321, 0.0714, 0.9166, 0.8175, 0.9969, 0.2791, 0.9939, 0.1191, 0.9329, 0.9969, 0.9997, 0.9938, 0.9993, 0.9892, 0.9972]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.4970e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6547e-02],\n",
      "         [1.4343e+02, 1.4446e+02, 5.3208e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.1365e-02],\n",
      "         [1.3712e+02, 1.4868e+02, 9.1661e-01],\n",
      "         [1.2838e+02, 1.6147e+02, 8.1753e-01],\n",
      "         [1.3983e+02, 1.5787e+02, 9.9692e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7912e-01],\n",
      "         [1.6074e+02, 1.4654e+02, 9.9389e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1912e-01],\n",
      "         [1.7160e+02, 1.3637e+02, 9.3291e-01],\n",
      "         [1.2518e+02, 2.1028e+02, 9.9690e-01],\n",
      "         [1.2475e+02, 2.1088e+02, 9.9967e-01],\n",
      "         [1.4767e+02, 2.3975e+02, 9.9377e-01],\n",
      "         [1.1951e+02, 2.5477e+02, 9.9931e-01],\n",
      "         [1.4828e+02, 2.7712e+02, 9.8924e-01],\n",
      "         [7.2363e+01, 2.7276e+02, 9.9725e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [143.4335, 144.4632],\n",
      "         [  0.0000,   0.0000],\n",
      "         [137.1215, 148.6770],\n",
      "         [128.3805, 161.4746],\n",
      "         [139.8310, 157.8717],\n",
      "         [  0.0000,   0.0000],\n",
      "         [160.7418, 146.5443],\n",
      "         [  0.0000,   0.0000],\n",
      "         [171.6040, 136.3705],\n",
      "         [125.1785, 210.2817],\n",
      "         [124.7471, 210.8828],\n",
      "         [147.6694, 239.7511],\n",
      "         [119.5106, 254.7750],\n",
      "         [148.2774, 277.1198],\n",
      "         [ 72.3634, 272.7550]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3984, 0.4013],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3809, 0.4130],\n",
      "         [0.3566, 0.4485],\n",
      "         [0.3884, 0.4385],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4465, 0.4071],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4767, 0.3788],\n",
      "         [0.3477, 0.5841],\n",
      "         [0.3465, 0.5858],\n",
      "         [0.4102, 0.6660],\n",
      "         [0.3320, 0.7077],\n",
      "         [0.4119, 0.7698],\n",
      "         [0.2010, 0.7577]]])\n",
      "\n",
      "0: 640x640 1 person, 113.0ms\n",
      "Speed: 1.2ms preprocess, 113.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[103, 147, 201],\n",
      "        [102, 146, 200],\n",
      "        [101, 146, 196],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[104, 148, 202],\n",
      "        [103, 147, 201],\n",
      "        [103, 148, 198],\n",
      "        ...,\n",
      "        [ 59, 125, 180],\n",
      "        [ 59, 125, 180],\n",
      "        [ 59, 125, 180]],\n",
      "\n",
      "       [[110, 151, 205],\n",
      "        [109, 150, 204],\n",
      "        [110, 151, 202],\n",
      "        ...,\n",
      "        [ 59, 126, 176],\n",
      "        [ 59, 126, 176],\n",
      "        [ 59, 126, 176]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 83,  88,  35],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2431144714355469, 'inference': 112.98584938049316, 'postprocess': 0.39505958557128906}]\n",
      "Bounding Box: tensor([[111.0000, 221.5000, 114.0000, 171.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6012, 0.1516, 0.6552, 0.1727, 0.9272, 0.9385, 0.9961, 0.6739, 0.9917, 0.4723, 0.9512, 0.9960, 0.9990, 0.9896, 0.9973, 0.9668, 0.9848]])\n",
      "data: tensor([[[1.4620e+02, 1.4743e+02, 6.0123e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5164e-01],\n",
      "         [1.4336e+02, 1.4574e+02, 6.5520e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7270e-01],\n",
      "         [1.3654e+02, 1.4958e+02, 9.2722e-01],\n",
      "         [1.2314e+02, 1.6602e+02, 9.3851e-01],\n",
      "         [1.4217e+02, 1.5953e+02, 9.9606e-01],\n",
      "         [1.2252e+02, 1.8523e+02, 6.7390e-01],\n",
      "         [1.5889e+02, 1.5189e+02, 9.9166e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7235e-01],\n",
      "         [1.6309e+02, 1.4599e+02, 9.5121e-01],\n",
      "         [1.1825e+02, 2.0966e+02, 9.9603e-01],\n",
      "         [1.2391e+02, 2.0918e+02, 9.9905e-01],\n",
      "         [1.3534e+02, 2.4203e+02, 9.8960e-01],\n",
      "         [1.1490e+02, 2.5109e+02, 9.9734e-01],\n",
      "         [1.2711e+02, 2.7807e+02, 9.6680e-01],\n",
      "         [7.1215e+01, 2.8140e+02, 9.8484e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[146.2038, 147.4305],\n",
      "         [  0.0000,   0.0000],\n",
      "         [143.3644, 145.7403],\n",
      "         [  0.0000,   0.0000],\n",
      "         [136.5358, 149.5809],\n",
      "         [123.1368, 166.0190],\n",
      "         [142.1721, 159.5280],\n",
      "         [122.5197, 185.2344],\n",
      "         [158.8869, 151.8931],\n",
      "         [  0.0000,   0.0000],\n",
      "         [163.0905, 145.9924],\n",
      "         [118.2547, 209.6620],\n",
      "         [123.9102, 209.1837],\n",
      "         [135.3381, 242.0327],\n",
      "         [114.8981, 251.0901],\n",
      "         [127.1103, 278.0732],\n",
      "         [ 71.2152, 281.3958]]])\n",
      "xyn: tensor([[[0.4061, 0.4095],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3982, 0.4048],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3793, 0.4155],\n",
      "         [0.3420, 0.4612],\n",
      "         [0.3949, 0.4431],\n",
      "         [0.3403, 0.5145],\n",
      "         [0.4414, 0.4219],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4530, 0.4055],\n",
      "         [0.3285, 0.5824],\n",
      "         [0.3442, 0.5811],\n",
      "         [0.3759, 0.6723],\n",
      "         [0.3192, 0.6975],\n",
      "         [0.3531, 0.7724],\n",
      "         [0.1978, 0.7817]]])\n",
      "\n",
      "0: 640x640 1 person, 113.5ms\n",
      "Speed: 1.7ms preprocess, 113.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[113, 151, 202],\n",
      "        [113, 151, 202],\n",
      "        [113, 151, 200],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[117, 154, 205],\n",
      "        [116, 153, 204],\n",
      "        [116, 153, 202],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[122, 157, 207],\n",
      "        [122, 157, 207],\n",
      "        [122, 157, 203],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 87,  91,  39],\n",
      "        [ 83,  88,  35],\n",
      "        [ 82,  87,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 91,  96,  44],\n",
      "        [ 89,  94,  41],\n",
      "        [ 88,  93,  40]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 89,  94,  41],\n",
      "        [ 86,  90,  38],\n",
      "        [ 84,  89,  37]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.6970634460449219, 'inference': 113.45791816711426, 'postprocess': 0.7610321044921875}]\n",
      "Bounding Box: tensor([[107.5000, 227.5000, 107.0000, 177.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2886, 0.0866, 0.3220, 0.3586, 0.9163, 0.9386, 0.9920, 0.2889, 0.8942, 0.0858, 0.4970, 0.9857, 0.9956, 0.9839, 0.9948, 0.9678, 0.9845]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.8862e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.6611e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2203e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5860e-01],\n",
      "         [1.3778e+02, 1.5075e+02, 9.1631e-01],\n",
      "         [1.2117e+02, 1.6928e+02, 9.3856e-01],\n",
      "         [1.4154e+02, 1.6314e+02, 9.9196e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8886e-01],\n",
      "         [1.5440e+02, 1.5851e+02, 8.9422e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.5785e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9699e-01],\n",
      "         [1.1337e+02, 2.1139e+02, 9.8569e-01],\n",
      "         [1.3037e+02, 2.0898e+02, 9.9558e-01],\n",
      "         [1.1142e+02, 2.4719e+02, 9.8387e-01],\n",
      "         [1.4348e+02, 2.4132e+02, 9.9477e-01],\n",
      "         [7.8774e+01, 2.8201e+02, 9.6784e-01],\n",
      "         [1.3947e+02, 2.8311e+02, 9.8447e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [137.7796, 150.7472],\n",
      "         [121.1748, 169.2796],\n",
      "         [141.5356, 163.1427],\n",
      "         [  0.0000,   0.0000],\n",
      "         [154.4017, 158.5088],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [113.3731, 211.3897],\n",
      "         [130.3711, 208.9783],\n",
      "         [111.4241, 247.1900],\n",
      "         [143.4787, 241.3248],\n",
      "         [ 78.7738, 282.0116],\n",
      "         [139.4659, 283.1100]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3827, 0.4187],\n",
      "         [0.3366, 0.4702],\n",
      "         [0.3932, 0.4532],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4289, 0.4403],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3149, 0.5872],\n",
      "         [0.3621, 0.5805],\n",
      "         [0.3095, 0.6866],\n",
      "         [0.3986, 0.6703],\n",
      "         [0.2188, 0.7834],\n",
      "         [0.3874, 0.7864]]])\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 1.2ms preprocess, 113.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[138, 167, 212],\n",
      "        [139, 168, 214],\n",
      "        [143, 172, 214],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[139, 168, 214],\n",
      "        [140, 169, 215],\n",
      "        [144, 173, 215],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[141, 172, 214],\n",
      "        [143, 173, 215],\n",
      "        [146, 175, 215],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 86,  90,  38],\n",
      "        [ 83,  88,  35],\n",
      "        [ 84,  89,  37]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 89,  94,  41],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 86,  90,  38],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2469291687011719, 'inference': 113.57641220092773, 'postprocess': 0.42319297790527344}]\n",
      "Bounding Box: tensor([[106., 229., 100., 178.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0791, 0.0433, 0.0631, 0.6285, 0.8434, 0.9680, 0.9836, 0.3492, 0.5823, 0.0477, 0.0965, 0.9920, 0.9950, 0.9918, 0.9943, 0.9857, 0.9891]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 7.9054e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3319e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.3146e-02],\n",
      "         [1.2550e+02, 1.5239e+02, 6.2849e-01],\n",
      "         [1.3875e+02, 1.5314e+02, 8.4340e-01],\n",
      "         [1.1761e+02, 1.7041e+02, 9.6796e-01],\n",
      "         [1.4182e+02, 1.6501e+02, 9.8356e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4925e-01],\n",
      "         [1.4983e+02, 1.6610e+02, 5.8231e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7656e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 9.6505e-02],\n",
      "         [1.0903e+02, 2.1390e+02, 9.9203e-01],\n",
      "         [1.2911e+02, 2.1026e+02, 9.9504e-01],\n",
      "         [1.0602e+02, 2.5286e+02, 9.9175e-01],\n",
      "         [1.4000e+02, 2.3928e+02, 9.9425e-01],\n",
      "         [7.3967e+01, 2.8868e+02, 9.8566e-01],\n",
      "         [1.3888e+02, 2.7768e+02, 9.8909e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [125.4963, 152.3859],\n",
      "         [138.7535, 153.1425],\n",
      "         [117.6143, 170.4071],\n",
      "         [141.8195, 165.0081],\n",
      "         [  0.0000,   0.0000],\n",
      "         [149.8350, 166.0986],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [109.0311, 213.8977],\n",
      "         [129.1133, 210.2636],\n",
      "         [106.0239, 252.8566],\n",
      "         [139.9969, 239.2845],\n",
      "         [ 73.9672, 288.6813],\n",
      "         [138.8773, 277.6783]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3486, 0.4233],\n",
      "         [0.3854, 0.4254],\n",
      "         [0.3267, 0.4734],\n",
      "         [0.3939, 0.4584],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4162, 0.4614],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3029, 0.5942],\n",
      "         [0.3586, 0.5841],\n",
      "         [0.2945, 0.7024],\n",
      "         [0.3889, 0.6647],\n",
      "         [0.2055, 0.8019],\n",
      "         [0.3858, 0.7713]]])\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.3ms preprocess, 114.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[148, 178, 217],\n",
      "        [152, 181, 221],\n",
      "        [153, 182, 221],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[147, 176, 216],\n",
      "        [151, 180, 219],\n",
      "        [152, 181, 219],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[147, 176, 216],\n",
      "        [151, 180, 219],\n",
      "        [152, 181, 219],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 86,  90,  38],\n",
      "        [ 83,  88,  35],\n",
      "        [ 84,  89,  37]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 90,  95,  42],\n",
      "        [ 89,  94,  41],\n",
      "        [ 87,  91,  39]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 88,  93,  40],\n",
      "        [ 86,  90,  38],\n",
      "        [ 86,  90,  38]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2748241424560547, 'inference': 114.85815048217773, 'postprocess': 0.3991127014160156}]\n",
      "Bounding Box: tensor([[105.0000, 231.5000,  96.0000, 175.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0900, 0.0277, 0.0757, 0.2490, 0.7565, 0.9498, 0.9811, 0.7074, 0.9192, 0.2858, 0.5704, 0.9984, 0.9991, 0.9974, 0.9985, 0.9947, 0.9953]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 9.0015e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7674e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 7.5673e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4900e-01],\n",
      "         [1.3553e+02, 1.5713e+02, 7.5652e-01],\n",
      "         [1.1568e+02, 1.6973e+02, 9.4985e-01],\n",
      "         [1.3772e+02, 1.6567e+02, 9.8113e-01],\n",
      "         [1.1671e+02, 1.9610e+02, 7.0739e-01],\n",
      "         [1.4516e+02, 1.6016e+02, 9.1922e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8579e-01],\n",
      "         [1.3930e+02, 1.5808e+02, 5.7043e-01],\n",
      "         [1.1149e+02, 2.1230e+02, 9.9840e-01],\n",
      "         [1.1974e+02, 2.1479e+02, 9.9908e-01],\n",
      "         [1.3576e+02, 2.3752e+02, 9.9739e-01],\n",
      "         [1.0909e+02, 2.5940e+02, 9.9846e-01],\n",
      "         [1.3199e+02, 2.7804e+02, 9.9474e-01],\n",
      "         [7.0876e+01, 2.9559e+02, 9.9532e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [135.5269, 157.1317],\n",
      "         [115.6821, 169.7330],\n",
      "         [137.7202, 165.6732],\n",
      "         [116.7129, 196.0980],\n",
      "         [145.1633, 160.1586],\n",
      "         [  0.0000,   0.0000],\n",
      "         [139.3027, 158.0801],\n",
      "         [111.4910, 212.2974],\n",
      "         [119.7413, 214.7857],\n",
      "         [135.7571, 237.5153],\n",
      "         [109.0880, 259.3978],\n",
      "         [131.9874, 278.0376],\n",
      "         [ 70.8757, 295.5898]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3765, 0.4365],\n",
      "         [0.3213, 0.4715],\n",
      "         [0.3826, 0.4602],\n",
      "         [0.3242, 0.5447],\n",
      "         [0.4032, 0.4449],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3870, 0.4391],\n",
      "         [0.3097, 0.5897],\n",
      "         [0.3326, 0.5966],\n",
      "         [0.3771, 0.6598],\n",
      "         [0.3030, 0.7205],\n",
      "         [0.3666, 0.7723],\n",
      "         [0.1969, 0.8211]]])\n",
      "\n",
      "0: 640x640 1 person, 119.6ms\n",
      "Speed: 1.3ms preprocess, 119.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[162, 189, 225],\n",
      "        [164, 190, 226],\n",
      "        [166, 190, 225],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[161, 188, 224],\n",
      "        [162, 189, 225],\n",
      "        [165, 189, 224],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[161, 188, 224],\n",
      "        [162, 189, 225],\n",
      "        [165, 189, 224],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 80,  84,  30],\n",
      "        [ 79,  83,  28],\n",
      "        [ 76,  81,  26]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 82,  87,  32],\n",
      "        [ 80,  84,  30],\n",
      "        [ 77,  82,  27]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 82,  87,  32],\n",
      "        [ 81,  86,  31],\n",
      "        [ 79,  83,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.25885009765625, 'inference': 119.6451187133789, 'postprocess': 0.48613548278808594}]\n",
      "Bounding Box: tensor([[103.0000, 235.5000,  88.0000, 181.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2065, 0.0356, 0.2617, 0.2138, 0.9236, 0.9400, 0.9969, 0.5219, 0.9876, 0.1928, 0.8703, 0.9977, 0.9995, 0.9946, 0.9989, 0.9870, 0.9945]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.0651e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5649e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6165e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1377e-01],\n",
      "         [1.2866e+02, 1.5887e+02, 9.2364e-01],\n",
      "         [1.0981e+02, 1.7253e+02, 9.4003e-01],\n",
      "         [1.3167e+02, 1.6634e+02, 9.9691e-01],\n",
      "         [1.0892e+02, 1.9702e+02, 5.2193e-01],\n",
      "         [1.3944e+02, 1.5521e+02, 9.8761e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9277e-01],\n",
      "         [1.3404e+02, 1.5162e+02, 8.7026e-01],\n",
      "         [1.0842e+02, 2.1308e+02, 9.9773e-01],\n",
      "         [1.1657e+02, 2.1519e+02, 9.9954e-01],\n",
      "         [1.3159e+02, 2.3745e+02, 9.9462e-01],\n",
      "         [1.0687e+02, 2.6110e+02, 9.9892e-01],\n",
      "         [1.3070e+02, 2.7629e+02, 9.8703e-01],\n",
      "         [7.2798e+01, 2.9895e+02, 9.9452e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [128.6611, 158.8728],\n",
      "         [109.8059, 172.5276],\n",
      "         [131.6673, 166.3446],\n",
      "         [108.9195, 197.0233],\n",
      "         [139.4428, 155.2065],\n",
      "         [  0.0000,   0.0000],\n",
      "         [134.0377, 151.6153],\n",
      "         [108.4182, 213.0832],\n",
      "         [116.5736, 215.1942],\n",
      "         [131.5928, 237.4526],\n",
      "         [106.8715, 261.0972],\n",
      "         [130.7048, 276.2899],\n",
      "         [ 72.7984, 298.9454]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3574, 0.4413],\n",
      "         [0.3050, 0.4792],\n",
      "         [0.3657, 0.4621],\n",
      "         [0.3026, 0.5473],\n",
      "         [0.3873, 0.4311],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3723, 0.4212],\n",
      "         [0.3012, 0.5919],\n",
      "         [0.3238, 0.5978],\n",
      "         [0.3655, 0.6596],\n",
      "         [0.2969, 0.7253],\n",
      "         [0.3631, 0.7675],\n",
      "         [0.2022, 0.8304]]])\n",
      "\n",
      "0: 640x640 1 person, 129.8ms\n",
      "Speed: 1.3ms preprocess, 129.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[167, 192, 224],\n",
      "        [167, 192, 224],\n",
      "        [167, 192, 224],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[166, 190, 223],\n",
      "        [166, 190, 223],\n",
      "        [167, 192, 224],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[165, 189, 222],\n",
      "        [166, 190, 223],\n",
      "        [166, 190, 223],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 79,  83,  28],\n",
      "        [ 77,  82,  27],\n",
      "        [ 76,  81,  26]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 81,  86,  31],\n",
      "        [ 79,  83,  28],\n",
      "        [ 77,  82,  27]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 81,  86,  31],\n",
      "        [ 80,  84,  30],\n",
      "        [ 79,  83,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2700557708740234, 'inference': 129.81796264648438, 'postprocess': 0.41294097900390625}]\n",
      "Bounding Box: tensor([[102.5000, 230.5000,  75.0000, 171.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0944, 0.0348, 0.0917, 0.4564, 0.8547, 0.9531, 0.9872, 0.5049, 0.8742, 0.1085, 0.3530, 0.9965, 0.9984, 0.9940, 0.9972, 0.9886, 0.9915]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 9.4405e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4834e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 9.1714e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5641e-01],\n",
      "         [1.2729e+02, 1.5878e+02, 8.5471e-01],\n",
      "         [1.0623e+02, 1.7264e+02, 9.5310e-01],\n",
      "         [1.3098e+02, 1.6776e+02, 9.8721e-01],\n",
      "         [1.0445e+02, 1.9678e+02, 5.0491e-01],\n",
      "         [1.3596e+02, 1.6346e+02, 8.7418e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0851e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5296e-01],\n",
      "         [1.0238e+02, 2.1376e+02, 9.9648e-01],\n",
      "         [1.1360e+02, 2.1474e+02, 9.9836e-01],\n",
      "         [1.2313e+02, 2.4458e+02, 9.9404e-01],\n",
      "         [1.0576e+02, 2.6088e+02, 9.9718e-01],\n",
      "         [1.2390e+02, 2.8215e+02, 9.8855e-01],\n",
      "         [7.6294e+01, 2.9793e+02, 9.9152e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [127.2918, 158.7823],\n",
      "         [106.2323, 172.6400],\n",
      "         [130.9806, 167.7628],\n",
      "         [104.4501, 196.7811],\n",
      "         [135.9601, 163.4619],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.3833, 213.7583],\n",
      "         [113.6002, 214.7388],\n",
      "         [123.1280, 244.5829],\n",
      "         [105.7557, 260.8812],\n",
      "         [123.8998, 282.1451],\n",
      "         [ 76.2941, 297.9274]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3536, 0.4411],\n",
      "         [0.2951, 0.4796],\n",
      "         [0.3638, 0.4660],\n",
      "         [0.2901, 0.5466],\n",
      "         [0.3777, 0.4541],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2844, 0.5938],\n",
      "         [0.3156, 0.5965],\n",
      "         [0.3420, 0.6794],\n",
      "         [0.2938, 0.7247],\n",
      "         [0.3442, 0.7837],\n",
      "         [0.2119, 0.8276]]])\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[166, 192, 222],\n",
      "        [167, 193, 223],\n",
      "        [169, 195, 225],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[165, 190, 221],\n",
      "        [167, 193, 223],\n",
      "        [168, 194, 224],\n",
      "        ...,\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179],\n",
      "        [ 58, 124, 179]],\n",
      "\n",
      "       [[165, 190, 221],\n",
      "        [166, 192, 222],\n",
      "        [168, 194, 224],\n",
      "        ...,\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178],\n",
      "        [ 59, 125, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 77,  87,  33],\n",
      "        [ 77,  87,  33],\n",
      "        [ 77,  87,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  84,  31],\n",
      "        [ 75,  84,  31],\n",
      "        [ 75,  84,  31]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 73,  82,  28],\n",
      "        [ 73,  82,  28],\n",
      "        [ 73,  82,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2559890747070312, 'inference': 116.47605895996094, 'postprocess': 0.7600784301757812}]\n",
      "Bounding Box: tensor([[103.5000, 231.5000,  67.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1352, 0.0555, 0.1491, 0.5058, 0.8714, 0.9325, 0.9844, 0.3336, 0.8037, 0.0635, 0.2787, 0.9926, 0.9969, 0.9905, 0.9960, 0.9847, 0.9901]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.3523e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.5546e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4914e-01],\n",
      "         [1.1611e+02, 1.5774e+02, 5.0577e-01],\n",
      "         [1.2466e+02, 1.5964e+02, 8.7142e-01],\n",
      "         [1.0569e+02, 1.7156e+02, 9.3245e-01],\n",
      "         [1.2728e+02, 1.7089e+02, 9.8441e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3363e-01],\n",
      "         [1.2836e+02, 1.8523e+02, 8.0372e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.3510e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7865e-01],\n",
      "         [9.9503e+01, 2.1135e+02, 9.9257e-01],\n",
      "         [1.0993e+02, 2.1186e+02, 9.9688e-01],\n",
      "         [1.1960e+02, 2.4833e+02, 9.9045e-01],\n",
      "         [1.0773e+02, 2.5589e+02, 9.9599e-01],\n",
      "         [1.1961e+02, 2.8669e+02, 9.8467e-01],\n",
      "         [8.1855e+01, 2.9715e+02, 9.9011e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [116.1072, 157.7423],\n",
      "         [124.6600, 159.6424],\n",
      "         [105.6852, 171.5640],\n",
      "         [127.2787, 170.8927],\n",
      "         [  0.0000,   0.0000],\n",
      "         [128.3601, 185.2280],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 99.5034, 211.3470],\n",
      "         [109.9311, 211.8593],\n",
      "         [119.6031, 248.3268],\n",
      "         [107.7338, 255.8869],\n",
      "         [119.6107, 286.6875],\n",
      "         [ 81.8555, 297.1504]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3225, 0.4382],\n",
      "         [0.3463, 0.4435],\n",
      "         [0.2936, 0.4766],\n",
      "         [0.3536, 0.4747],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3566, 0.5145],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2764, 0.5871],\n",
      "         [0.3054, 0.5885],\n",
      "         [0.3322, 0.6898],\n",
      "         [0.2993, 0.7108],\n",
      "         [0.3323, 0.7964],\n",
      "         [0.2274, 0.8254]]])\n",
      "\n",
      "0: 640x640 1 person, 115.6ms\n",
      "Speed: 1.2ms preprocess, 115.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 228],\n",
      "        [173, 195, 228],\n",
      "        [174, 196, 229],\n",
      "        ...,\n",
      "        [ 58, 124, 176],\n",
      "        [ 58, 124, 176],\n",
      "        [ 58, 124, 176]],\n",
      "\n",
      "       [[172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        [174, 196, 229],\n",
      "        ...,\n",
      "        [ 58, 124, 176],\n",
      "        [ 58, 124, 176],\n",
      "        [ 58, 124, 176]],\n",
      "\n",
      "       [[172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        [174, 196, 229],\n",
      "        ...,\n",
      "        [ 59, 126, 176],\n",
      "        [ 59, 126, 176],\n",
      "        [ 59, 126, 176]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 77,  87,  33],\n",
      "        [ 77,  87,  33],\n",
      "        [ 76,  86,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  84,  31],\n",
      "        [ 75,  84,  31],\n",
      "        [ 74,  83,  30]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 73,  82,  28],\n",
      "        [ 73,  82,  28],\n",
      "        [ 70,  80,  26]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2276172637939453, 'inference': 115.63539505004883, 'postprocess': 0.3960132598876953}]\n",
      "Bounding Box: tensor([[111., 231.,  70., 164.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.0837, 0.0182, 0.1068, 0.2804, 0.8946, 0.9219, 0.9926, 0.3529, 0.9454, 0.0811, 0.5791, 0.9958, 0.9989, 0.9948, 0.9986, 0.9904, 0.9956]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 8.3693e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8214e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0676e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8045e-01],\n",
      "         [1.2120e+02, 1.6138e+02, 8.9462e-01],\n",
      "         [9.8902e+01, 1.7188e+02, 9.2192e-01],\n",
      "         [1.2631e+02, 1.7120e+02, 9.9260e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5290e-01],\n",
      "         [1.4360e+02, 1.8229e+02, 9.4542e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.1138e-02],\n",
      "         [1.3750e+02, 1.8172e+02, 5.7911e-01],\n",
      "         [9.4071e+01, 2.1066e+02, 9.9583e-01],\n",
      "         [1.0769e+02, 2.1138e+02, 9.9888e-01],\n",
      "         [1.1690e+02, 2.4597e+02, 9.9475e-01],\n",
      "         [1.0972e+02, 2.5294e+02, 9.9861e-01],\n",
      "         [1.1689e+02, 2.8554e+02, 9.9044e-01],\n",
      "         [8.8910e+01, 2.9399e+02, 9.9560e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [121.1996, 161.3780],\n",
      "         [ 98.9019, 171.8795],\n",
      "         [126.3148, 171.1985],\n",
      "         [  0.0000,   0.0000],\n",
      "         [143.6038, 182.2930],\n",
      "         [  0.0000,   0.0000],\n",
      "         [137.5012, 181.7162],\n",
      "         [ 94.0708, 210.6616],\n",
      "         [107.6896, 211.3847],\n",
      "         [116.8995, 245.9719],\n",
      "         [109.7191, 252.9383],\n",
      "         [116.8948, 285.5377],\n",
      "         [ 88.9105, 293.9905]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3367, 0.4483],\n",
      "         [0.2747, 0.4774],\n",
      "         [0.3509, 0.4756],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3989, 0.5064],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3819, 0.5048],\n",
      "         [0.2613, 0.5852],\n",
      "         [0.2991, 0.5872],\n",
      "         [0.3247, 0.6833],\n",
      "         [0.3048, 0.7026],\n",
      "         [0.3247, 0.7932],\n",
      "         [0.2470, 0.8166]]])\n",
      "\n",
      "0: 640x640 2 persons, 113.4ms\n",
      "Speed: 1.2ms preprocess, 113.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[166, 190, 225],\n",
      "        [166, 190, 225],\n",
      "        [167, 192, 226],\n",
      "        ...,\n",
      "        [ 60, 127, 175],\n",
      "        [ 60, 127, 175],\n",
      "        [ 60, 127, 175]],\n",
      "\n",
      "       [[166, 190, 225],\n",
      "        [166, 190, 225],\n",
      "        [167, 192, 226],\n",
      "        ...,\n",
      "        [ 60, 127, 175],\n",
      "        [ 60, 127, 175],\n",
      "        [ 60, 127, 175]],\n",
      "\n",
      "       [[167, 189, 222],\n",
      "        [168, 190, 223],\n",
      "        [168, 190, 223],\n",
      "        ...,\n",
      "        [ 60, 127, 178],\n",
      "        [ 60, 127, 178],\n",
      "        [ 60, 127, 178]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 77,  87,  33],\n",
      "        [ 77,  87,  33],\n",
      "        [ 79,  88,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 74,  83,  30],\n",
      "        [ 75,  84,  31],\n",
      "        [ 75,  84,  31]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 72,  81,  27],\n",
      "        [ 72,  81,  27],\n",
      "        [ 72,  81,  27]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2252330780029297, 'inference': 113.43812942504883, 'postprocess': 0.4298686981201172}]\n",
      "Bounding Box: tensor([[115.0000, 227.5000,  70.0000, 155.0000]])\n",
      "Bounding Box: tensor([[121.5000, 274.5000,  55.0000, 107.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2266, 0.0603, 0.1911, 0.3627, 0.8106, 0.9562, 0.9829, 0.6005, 0.8955, 0.2473, 0.5939, 0.9941, 0.9968, 0.9870, 0.9932, 0.9554, 0.9678]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2656e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.0314e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9108e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6271e-01],\n",
      "         [1.1734e+02, 1.6070e+02, 8.1061e-01],\n",
      "         [9.7349e+01, 1.7254e+02, 9.5622e-01],\n",
      "         [1.2150e+02, 1.7039e+02, 9.8287e-01],\n",
      "         [9.4911e+01, 1.9322e+02, 6.0047e-01],\n",
      "         [1.3590e+02, 1.8028e+02, 8.9553e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4726e-01],\n",
      "         [1.4625e+02, 1.8558e+02, 5.9394e-01],\n",
      "         [9.3428e+01, 2.1183e+02, 9.9407e-01],\n",
      "         [1.0567e+02, 2.1090e+02, 9.9679e-01],\n",
      "         [1.0983e+02, 2.4965e+02, 9.8704e-01],\n",
      "         [1.1177e+02, 2.5127e+02, 9.9320e-01],\n",
      "         [1.0938e+02, 2.8740e+02, 9.5540e-01],\n",
      "         [9.4225e+01, 2.8873e+02, 9.6782e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.3422, 160.6983],\n",
      "         [ 97.3487, 172.5444],\n",
      "         [121.5027, 170.3919],\n",
      "         [ 94.9113, 193.2198],\n",
      "         [135.8994, 180.2762],\n",
      "         [  0.0000,   0.0000],\n",
      "         [146.2526, 185.5763],\n",
      "         [ 93.4277, 211.8269],\n",
      "         [105.6686, 210.9016],\n",
      "         [109.8320, 249.6549],\n",
      "         [111.7693, 251.2692],\n",
      "         [109.3843, 287.4043],\n",
      "         [ 94.2255, 288.7278]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3260, 0.4464],\n",
      "         [0.2704, 0.4793],\n",
      "         [0.3375, 0.4733],\n",
      "         [0.2636, 0.5367],\n",
      "         [0.3775, 0.5008],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4063, 0.5155],\n",
      "         [0.2595, 0.5884],\n",
      "         [0.2935, 0.5858],\n",
      "         [0.3051, 0.6935],\n",
      "         [0.3105, 0.6980],\n",
      "         [0.3038, 0.7983],\n",
      "         [0.2617, 0.8020]]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2298, 0.0915, 0.1619, 0.0980, 0.2715, 0.6959, 0.7804, 0.4791, 0.6956, 0.4045, 0.5948, 0.9308, 0.9482, 0.9249, 0.9408, 0.8863, 0.8982]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2981e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.1487e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6193e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.8005e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7148e-01],\n",
      "         [1.2143e+02, 2.2438e+02, 6.9593e-01],\n",
      "         [1.2607e+02, 2.2397e+02, 7.8037e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7907e-01],\n",
      "         [1.3336e+02, 2.3465e+02, 6.9559e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0448e-01],\n",
      "         [1.3943e+02, 2.4383e+02, 5.9482e-01],\n",
      "         [1.2245e+02, 2.6153e+02, 9.3079e-01],\n",
      "         [1.2581e+02, 2.6104e+02, 9.4815e-01],\n",
      "         [1.2202e+02, 2.8937e+02, 9.2488e-01],\n",
      "         [1.2026e+02, 2.8914e+02, 9.4079e-01],\n",
      "         [1.0706e+02, 3.1395e+02, 8.8625e-01],\n",
      "         [1.0420e+02, 3.1410e+02, 8.9824e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [121.4300, 224.3817],\n",
      "         [126.0737, 223.9747],\n",
      "         [  0.0000,   0.0000],\n",
      "         [133.3618, 234.6462],\n",
      "         [  0.0000,   0.0000],\n",
      "         [139.4328, 243.8330],\n",
      "         [122.4480, 261.5293],\n",
      "         [125.8089, 261.0361],\n",
      "         [122.0155, 289.3654],\n",
      "         [120.2568, 289.1417],\n",
      "         [107.0622, 313.9473],\n",
      "         [104.1985, 314.1021]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3373, 0.6233],\n",
      "         [0.3502, 0.6222],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3704, 0.6518],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3873, 0.6773],\n",
      "         [0.3401, 0.7265],\n",
      "         [0.3495, 0.7251],\n",
      "         [0.3389, 0.8038],\n",
      "         [0.3340, 0.8032],\n",
      "         [0.2974, 0.8721],\n",
      "         [0.2894, 0.8725]]])\n",
      "\n",
      "0: 640x640 1 person, 111.5ms\n",
      "Speed: 1.3ms preprocess, 111.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[167, 192, 226],\n",
      "        [168, 193, 228],\n",
      "        [169, 194, 229],\n",
      "        ...,\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179]],\n",
      "\n",
      "       [[167, 192, 226],\n",
      "        [168, 193, 228],\n",
      "        [169, 194, 229],\n",
      "        ...,\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179]],\n",
      "\n",
      "       [[168, 190, 223],\n",
      "        [169, 192, 224],\n",
      "        [171, 193, 225],\n",
      "        ...,\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179],\n",
      "        [ 61, 127, 180]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 74,  89,  33],\n",
      "        [ 74,  89,  33],\n",
      "        [ 74,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 72,  87,  31],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 67,  82,  26],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2867450714111328, 'inference': 111.47379875183105, 'postprocess': 0.6883144378662109}]\n",
      "Bounding Box: tensor([[116.0000, 222.5000,  74.0000, 145.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1749, 0.0387, 0.1685, 0.2755, 0.8428, 0.9483, 0.9876, 0.6074, 0.9401, 0.2753, 0.7256, 0.9951, 0.9978, 0.9859, 0.9939, 0.9558, 0.9722]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.7485e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8713e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6846e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7548e-01],\n",
      "         [1.1481e+02, 1.6128e+02, 8.4280e-01],\n",
      "         [9.3964e+01, 1.7325e+02, 9.4829e-01],\n",
      "         [1.1800e+02, 1.7065e+02, 9.8758e-01],\n",
      "         [9.1987e+01, 1.9216e+02, 6.0744e-01],\n",
      "         [1.3322e+02, 1.8026e+02, 9.4008e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7531e-01],\n",
      "         [1.4774e+02, 1.8732e+02, 7.2562e-01],\n",
      "         [8.7410e+01, 2.0992e+02, 9.9512e-01],\n",
      "         [1.0086e+02, 2.0969e+02, 9.9778e-01],\n",
      "         [1.0624e+02, 2.4353e+02, 9.8593e-01],\n",
      "         [1.1479e+02, 2.4549e+02, 9.9390e-01],\n",
      "         [1.0606e+02, 2.7461e+02, 9.5584e-01],\n",
      "         [1.0116e+02, 2.8044e+02, 9.7220e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [114.8094, 161.2794],\n",
      "         [ 93.9640, 173.2550],\n",
      "         [117.9984, 170.6466],\n",
      "         [ 91.9872, 192.1592],\n",
      "         [133.2221, 180.2597],\n",
      "         [  0.0000,   0.0000],\n",
      "         [147.7436, 187.3241],\n",
      "         [ 87.4095, 209.9195],\n",
      "         [100.8579, 209.6869],\n",
      "         [106.2416, 243.5257],\n",
      "         [114.7937, 245.4859],\n",
      "         [106.0560, 274.6067],\n",
      "         [101.1622, 280.4444]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3189, 0.4480],\n",
      "         [0.2610, 0.4813],\n",
      "         [0.3278, 0.4740],\n",
      "         [0.2555, 0.5338],\n",
      "         [0.3701, 0.5007],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4104, 0.5203],\n",
      "         [0.2428, 0.5831],\n",
      "         [0.2802, 0.5825],\n",
      "         [0.2951, 0.6765],\n",
      "         [0.3189, 0.6819],\n",
      "         [0.2946, 0.7628],\n",
      "         [0.2810, 0.7790]]])\n",
      "\n",
      "0: 640x640 1 person, 128.5ms\n",
      "Speed: 1.2ms preprocess, 128.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179]],\n",
      "\n",
      "       [[171, 193, 228],\n",
      "        [172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179],\n",
      "        [ 60, 126, 179]],\n",
      "\n",
      "       [[171, 193, 225],\n",
      "        [172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        ...,\n",
      "        [ 61, 127, 182],\n",
      "        [ 61, 127, 182],\n",
      "        [ 61, 127, 182]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2238025665283203, 'inference': 128.47208976745605, 'postprocess': 0.6871223449707031}]\n",
      "Bounding Box: tensor([[116.5000, 221.0000,  79.0000, 144.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2606, 0.0459, 0.2796, 0.2374, 0.8877, 0.9432, 0.9940, 0.5588, 0.9766, 0.2889, 0.8692, 0.9965, 0.9990, 0.9933, 0.9982, 0.9836, 0.9926]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.6055e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5874e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7957e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3745e-01],\n",
      "         [1.1111e+02, 1.6139e+02, 8.8772e-01],\n",
      "         [9.1110e+01, 1.7228e+02, 9.4325e-01],\n",
      "         [1.1555e+02, 1.7040e+02, 9.9397e-01],\n",
      "         [8.9954e+01, 1.9148e+02, 5.5882e-01],\n",
      "         [1.3352e+02, 1.7862e+02, 9.7664e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8893e-01],\n",
      "         [1.5012e+02, 1.8511e+02, 8.6923e-01],\n",
      "         [8.7025e+01, 2.0913e+02, 9.9645e-01],\n",
      "         [9.9519e+01, 2.1011e+02, 9.9897e-01],\n",
      "         [1.1297e+02, 2.3650e+02, 9.9333e-01],\n",
      "         [1.1703e+02, 2.4188e+02, 9.9821e-01],\n",
      "         [1.2234e+02, 2.6961e+02, 9.8358e-01],\n",
      "         [1.0446e+02, 2.7840e+02, 9.9258e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.1079, 161.3850],\n",
      "         [ 91.1099, 172.2814],\n",
      "         [115.5502, 170.4012],\n",
      "         [ 89.9545, 191.4844],\n",
      "         [133.5152, 178.6216],\n",
      "         [  0.0000,   0.0000],\n",
      "         [150.1244, 185.1102],\n",
      "         [ 87.0249, 209.1288],\n",
      "         [ 99.5192, 210.1055],\n",
      "         [112.9726, 236.4960],\n",
      "         [117.0302, 241.8798],\n",
      "         [122.3420, 269.6078],\n",
      "         [104.4559, 278.3959]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3086, 0.4483],\n",
      "         [0.2531, 0.4786],\n",
      "         [0.3210, 0.4733],\n",
      "         [0.2499, 0.5319],\n",
      "         [0.3709, 0.4962],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4170, 0.5142],\n",
      "         [0.2417, 0.5809],\n",
      "         [0.2764, 0.5836],\n",
      "         [0.3138, 0.6569],\n",
      "         [0.3251, 0.6719],\n",
      "         [0.3398, 0.7489],\n",
      "         [0.2902, 0.7733]]])\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [107, 147, 202],\n",
      "        [108, 148, 203],\n",
      "        [108, 148, 203]],\n",
      "\n",
      "       [[171, 193, 228],\n",
      "        [172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [107, 147, 202],\n",
      "        [108, 148, 203],\n",
      "        [108, 148, 203]],\n",
      "\n",
      "       [[171, 193, 225],\n",
      "        [172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        ...,\n",
      "        [104, 148, 204],\n",
      "        [104, 148, 204],\n",
      "        [104, 148, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2671947479248047, 'inference': 116.52684211730957, 'postprocess': 0.39696693420410156}]\n",
      "Bounding Box: tensor([[117.5000, 217.5000,  85.0000, 139.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1977, 0.0239, 0.2607, 0.1394, 0.9025, 0.9080, 0.9948, 0.3863, 0.9812, 0.1831, 0.8810, 0.9937, 0.9987, 0.9881, 0.9976, 0.9722, 0.9901]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.9768e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3931e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6070e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3940e-01],\n",
      "         [1.1088e+02, 1.6071e+02, 9.0246e-01],\n",
      "         [8.9333e+01, 1.7252e+02, 9.0796e-01],\n",
      "         [1.1241e+02, 1.6976e+02, 9.9482e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8633e-01],\n",
      "         [1.3058e+02, 1.7695e+02, 9.8118e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8307e-01],\n",
      "         [1.5123e+02, 1.8248e+02, 8.8102e-01],\n",
      "         [8.4986e+01, 2.0879e+02, 9.9367e-01],\n",
      "         [9.8039e+01, 2.0876e+02, 9.9867e-01],\n",
      "         [1.0650e+02, 2.3866e+02, 9.8810e-01],\n",
      "         [1.1683e+02, 2.3870e+02, 9.9761e-01],\n",
      "         [1.0836e+02, 2.7034e+02, 9.7217e-01],\n",
      "         [1.1004e+02, 2.7051e+02, 9.9014e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [110.8824, 160.7115],\n",
      "         [ 89.3334, 172.5163],\n",
      "         [112.4109, 169.7633],\n",
      "         [  0.0000,   0.0000],\n",
      "         [130.5797, 176.9511],\n",
      "         [  0.0000,   0.0000],\n",
      "         [151.2350, 182.4752],\n",
      "         [ 84.9859, 208.7860],\n",
      "         [ 98.0387, 208.7611],\n",
      "         [106.4998, 238.6622],\n",
      "         [116.8269, 238.7033],\n",
      "         [108.3621, 270.3404],\n",
      "         [110.0402, 270.5127]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3080, 0.4464],\n",
      "         [0.2481, 0.4792],\n",
      "         [0.3123, 0.4716],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3627, 0.4915],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4201, 0.5069],\n",
      "         [0.2361, 0.5800],\n",
      "         [0.2723, 0.5799],\n",
      "         [0.2958, 0.6630],\n",
      "         [0.3245, 0.6631],\n",
      "         [0.3010, 0.7509],\n",
      "         [0.3057, 0.7514]]])\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 152, 209],\n",
      "        [118, 152, 209],\n",
      "        [118, 152, 209]],\n",
      "\n",
      "       [[171, 193, 228],\n",
      "        [172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 152, 209],\n",
      "        [118, 152, 209],\n",
      "        [118, 152, 209]],\n",
      "\n",
      "       [[171, 193, 225],\n",
      "        [172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        ...,\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2080669403076172, 'inference': 113.79599571228027, 'postprocess': 0.39124488830566406}]\n",
      "Bounding Box: tensor([[116.5000, 217.0000,  89.0000, 138.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.4359, 0.0600, 0.5303, 0.1767, 0.9400, 0.9348, 0.9975, 0.4321, 0.9904, 0.2455, 0.9401, 0.9938, 0.9989, 0.9879, 0.9979, 0.9693, 0.9909]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 4.3594e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.9973e-02],\n",
      "         [1.1103e+02, 1.5740e+02, 5.3029e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7670e-01],\n",
      "         [1.0541e+02, 1.5939e+02, 9.4001e-01],\n",
      "         [8.5678e+01, 1.7285e+02, 9.3478e-01],\n",
      "         [1.1054e+02, 1.6814e+02, 9.9746e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3214e-01],\n",
      "         [1.3357e+02, 1.7505e+02, 9.9041e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4546e-01],\n",
      "         [1.5440e+02, 1.8022e+02, 9.4014e-01],\n",
      "         [8.4162e+01, 2.0824e+02, 9.9380e-01],\n",
      "         [9.8267e+01, 2.0644e+02, 9.9890e-01],\n",
      "         [1.0341e+02, 2.4031e+02, 9.8787e-01],\n",
      "         [1.2097e+02, 2.3282e+02, 9.9789e-01],\n",
      "         [1.0618e+02, 2.7205e+02, 9.6927e-01],\n",
      "         [1.1195e+02, 2.5942e+02, 9.9091e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.0348, 157.4019],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.4137, 159.3926],\n",
      "         [ 85.6782, 172.8493],\n",
      "         [110.5354, 168.1443],\n",
      "         [  0.0000,   0.0000],\n",
      "         [133.5725, 175.0542],\n",
      "         [  0.0000,   0.0000],\n",
      "         [154.3995, 180.2228],\n",
      "         [ 84.1624, 208.2364],\n",
      "         [ 98.2670, 206.4437],\n",
      "         [103.4055, 240.3093],\n",
      "         [120.9697, 232.8189],\n",
      "         [106.1806, 272.0508],\n",
      "         [111.9544, 259.4216]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3084, 0.4372],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2928, 0.4428],\n",
      "         [0.2380, 0.4801],\n",
      "         [0.3070, 0.4671],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3710, 0.4863],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4289, 0.5006],\n",
      "         [0.2338, 0.5784],\n",
      "         [0.2730, 0.5735],\n",
      "         [0.2872, 0.6675],\n",
      "         [0.3360, 0.6467],\n",
      "         [0.2949, 0.7557],\n",
      "         [0.3110, 0.7206]]])\n",
      "\n",
      "0: 640x640 1 person, 119.0ms\n",
      "Speed: 1.3ms preprocess, 119.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209]],\n",
      "\n",
      "       [[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209]],\n",
      "\n",
      "       [[172, 194, 226],\n",
      "        [172, 194, 226],\n",
      "        [173, 195, 228],\n",
      "        ...,\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209],\n",
      "        [117, 153, 209]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2869834899902344, 'inference': 118.9577579498291, 'postprocess': 0.41222572326660156}]\n",
      "Bounding Box: tensor([[118.0000, 217.5000,  94.0000, 143.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3947, 0.0522, 0.4571, 0.1883, 0.9346, 0.9557, 0.9977, 0.5495, 0.9908, 0.3177, 0.9402, 0.9969, 0.9993, 0.9944, 0.9988, 0.9845, 0.9949]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.9472e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.2191e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5712e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8835e-01],\n",
      "         [1.0450e+02, 1.5725e+02, 9.3461e-01],\n",
      "         [8.4487e+01, 1.7028e+02, 9.5568e-01],\n",
      "         [1.0949e+02, 1.6557e+02, 9.9770e-01],\n",
      "         [8.4908e+01, 1.8844e+02, 5.4953e-01],\n",
      "         [1.3578e+02, 1.7176e+02, 9.9081e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1774e-01],\n",
      "         [1.5838e+02, 1.7653e+02, 9.4023e-01],\n",
      "         [8.2949e+01, 2.0594e+02, 9.9689e-01],\n",
      "         [9.7235e+01, 2.0404e+02, 9.9935e-01],\n",
      "         [1.0229e+02, 2.4000e+02, 9.9440e-01],\n",
      "         [1.2073e+02, 2.3139e+02, 9.9884e-01],\n",
      "         [1.0249e+02, 2.7428e+02, 9.8449e-01],\n",
      "         [1.1267e+02, 2.6011e+02, 9.9489e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.4952, 157.2457],\n",
      "         [ 84.4871, 170.2762],\n",
      "         [109.4950, 165.5662],\n",
      "         [ 84.9080, 188.4353],\n",
      "         [135.7751, 171.7552],\n",
      "         [  0.0000,   0.0000],\n",
      "         [158.3772, 176.5289],\n",
      "         [ 82.9494, 205.9356],\n",
      "         [ 97.2347, 204.0357],\n",
      "         [102.2884, 239.9993],\n",
      "         [120.7261, 231.3861],\n",
      "         [102.4898, 274.2762],\n",
      "         [112.6650, 260.1143]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2903, 0.4368],\n",
      "         [0.2347, 0.4730],\n",
      "         [0.3042, 0.4599],\n",
      "         [0.2359, 0.5234],\n",
      "         [0.3772, 0.4771],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4399, 0.4904],\n",
      "         [0.2304, 0.5720],\n",
      "         [0.2701, 0.5668],\n",
      "         [0.2841, 0.6667],\n",
      "         [0.3354, 0.6427],\n",
      "         [0.2847, 0.7619],\n",
      "         [0.3130, 0.7225]]])\n",
      "\n",
      "0: 640x640 1 person, 112.4ms\n",
      "Speed: 1.3ms preprocess, 112.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2960433959960938, 'inference': 112.36190795898438, 'postprocess': 0.4038810729980469}]\n",
      "Bounding Box: tensor([[118.0000, 219.5000,  96.0000, 149.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2353, 0.0270, 0.2746, 0.1947, 0.9277, 0.9591, 0.9978, 0.5401, 0.9896, 0.2554, 0.9154, 0.9973, 0.9994, 0.9947, 0.9989, 0.9848, 0.9950]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.3530e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6980e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7465e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9469e-01],\n",
      "         [1.0477e+02, 1.5653e+02, 9.2772e-01],\n",
      "         [8.3642e+01, 1.6932e+02, 9.5906e-01],\n",
      "         [1.0876e+02, 1.6399e+02, 9.9779e-01],\n",
      "         [8.5054e+01, 1.8914e+02, 5.4011e-01],\n",
      "         [1.3651e+02, 1.6878e+02, 9.8961e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5544e-01],\n",
      "         [1.5860e+02, 1.7243e+02, 9.1541e-01],\n",
      "         [8.2136e+01, 2.0572e+02, 9.9735e-01],\n",
      "         [9.6429e+01, 2.0330e+02, 9.9943e-01],\n",
      "         [1.0210e+02, 2.3995e+02, 9.9470e-01],\n",
      "         [1.2127e+02, 2.2931e+02, 9.9886e-01],\n",
      "         [1.0320e+02, 2.7720e+02, 9.8477e-01],\n",
      "         [1.1683e+02, 2.5647e+02, 9.9504e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.7697, 156.5264],\n",
      "         [ 83.6423, 169.3186],\n",
      "         [108.7554, 163.9913],\n",
      "         [ 85.0539, 189.1449],\n",
      "         [136.5098, 168.7837],\n",
      "         [  0.0000,   0.0000],\n",
      "         [158.6039, 172.4257],\n",
      "         [ 82.1365, 205.7242],\n",
      "         [ 96.4289, 203.3002],\n",
      "         [102.0988, 239.9458],\n",
      "         [121.2738, 229.3123],\n",
      "         [103.1977, 277.1967],\n",
      "         [116.8259, 256.4656]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2910, 0.4348],\n",
      "         [0.2323, 0.4703],\n",
      "         [0.3021, 0.4555],\n",
      "         [0.2363, 0.5254],\n",
      "         [0.3792, 0.4688],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4406, 0.4790],\n",
      "         [0.2282, 0.5715],\n",
      "         [0.2679, 0.5647],\n",
      "         [0.2836, 0.6665],\n",
      "         [0.3369, 0.6370],\n",
      "         [0.2867, 0.7700],\n",
      "         [0.3245, 0.7124]]])\n",
      "\n",
      "0: 640x640 1 person, 114.6ms\n",
      "Speed: 1.4ms preprocess, 114.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3880729675292969, 'inference': 114.60494995117188, 'postprocess': 0.7579326629638672}]\n",
      "Bounding Box: tensor([[119.0000, 219.5000,  98.0000, 153.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2275, 0.0361, 0.2251, 0.2548, 0.8909, 0.9690, 0.9958, 0.6185, 0.9733, 0.2843, 0.8187, 0.9981, 0.9993, 0.9973, 0.9990, 0.9935, 0.9971]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.2752e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6053e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2508e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5483e-01],\n",
      "         [1.0426e+02, 1.5421e+02, 8.9095e-01],\n",
      "         [8.3310e+01, 1.6794e+02, 9.6899e-01],\n",
      "         [1.0679e+02, 1.6094e+02, 9.9580e-01],\n",
      "         [8.5014e+01, 1.9079e+02, 6.1851e-01],\n",
      "         [1.3348e+02, 1.6313e+02, 9.7330e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8433e-01],\n",
      "         [1.6048e+02, 1.6844e+02, 8.1867e-01],\n",
      "         [8.2144e+01, 2.0450e+02, 9.9807e-01],\n",
      "         [9.5626e+01, 2.0111e+02, 9.9933e-01],\n",
      "         [1.0093e+02, 2.4062e+02, 9.9733e-01],\n",
      "         [1.2223e+02, 2.2611e+02, 9.9902e-01],\n",
      "         [1.0262e+02, 2.8327e+02, 9.9353e-01],\n",
      "         [1.1911e+02, 2.5051e+02, 9.9708e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.2618, 154.2141],\n",
      "         [ 83.3096, 167.9398],\n",
      "         [106.7916, 160.9372],\n",
      "         [ 85.0139, 190.7935],\n",
      "         [133.4806, 163.1328],\n",
      "         [  0.0000,   0.0000],\n",
      "         [160.4816, 168.4357],\n",
      "         [ 82.1436, 204.5013],\n",
      "         [ 95.6258, 201.1133],\n",
      "         [100.9324, 240.6180],\n",
      "         [122.2342, 226.1121],\n",
      "         [102.6229, 283.2739],\n",
      "         [119.1088, 250.5131]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2896, 0.4284],\n",
      "         [0.2314, 0.4665],\n",
      "         [0.2966, 0.4470],\n",
      "         [0.2361, 0.5300],\n",
      "         [0.3708, 0.4531],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4458, 0.4679],\n",
      "         [0.2282, 0.5681],\n",
      "         [0.2656, 0.5586],\n",
      "         [0.2804, 0.6684],\n",
      "         [0.3395, 0.6281],\n",
      "         [0.2851, 0.7869],\n",
      "         [0.3309, 0.6959]]])\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.2ms preprocess, 116.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2462139129638672, 'inference': 116.12987518310547, 'postprocess': 0.4048347473144531}]\n",
      "Bounding Box: tensor([[119.5000, 219.0000, 101.0000, 156.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.2587, 0.0249, 0.2924, 0.1580, 0.9304, 0.9688, 0.9986, 0.6344, 0.9943, 0.3564, 0.9530, 0.9989, 0.9998, 0.9982, 0.9996, 0.9942, 0.9982]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 2.5870e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4858e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9239e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5798e-01],\n",
      "         [1.0308e+02, 1.5188e+02, 9.3043e-01],\n",
      "         [8.3035e+01, 1.6605e+02, 9.6884e-01],\n",
      "         [1.0600e+02, 1.5852e+02, 9.9862e-01],\n",
      "         [8.5614e+01, 1.8468e+02, 6.3435e-01],\n",
      "         [1.3372e+02, 1.6044e+02, 9.9433e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5645e-01],\n",
      "         [1.5931e+02, 1.6319e+02, 9.5297e-01],\n",
      "         [8.1895e+01, 2.0319e+02, 9.9894e-01],\n",
      "         [9.5233e+01, 1.9919e+02, 9.9979e-01],\n",
      "         [9.8862e+01, 2.4181e+02, 9.9816e-01],\n",
      "         [1.2228e+02, 2.2492e+02, 9.9961e-01],\n",
      "         [1.0102e+02, 2.8294e+02, 9.9416e-01],\n",
      "         [1.1997e+02, 2.5329e+02, 9.9823e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [103.0842, 151.8779],\n",
      "         [ 83.0354, 166.0519],\n",
      "         [106.0011, 158.5216],\n",
      "         [ 85.6138, 184.6784],\n",
      "         [133.7247, 160.4365],\n",
      "         [  0.0000,   0.0000],\n",
      "         [159.3107, 163.1895],\n",
      "         [ 81.8952, 203.1934],\n",
      "         [ 95.2327, 199.1868],\n",
      "         [ 98.8625, 241.8075],\n",
      "         [122.2845, 224.9219],\n",
      "         [101.0215, 282.9395],\n",
      "         [119.9650, 253.2940]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2863, 0.4219],\n",
      "         [0.2307, 0.4613],\n",
      "         [0.2944, 0.4403],\n",
      "         [0.2378, 0.5130],\n",
      "         [0.3715, 0.4457],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4425, 0.4533],\n",
      "         [0.2275, 0.5644],\n",
      "         [0.2645, 0.5533],\n",
      "         [0.2746, 0.6717],\n",
      "         [0.3397, 0.6248],\n",
      "         [0.2806, 0.7859],\n",
      "         [0.3332, 0.7036]]])\n",
      "\n",
      "0: 640x640 1 person, 119.4ms\n",
      "Speed: 1.3ms preprocess, 119.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [174, 196, 231],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       [[172, 194, 229],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210],\n",
      "        [118, 154, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2500286102294922, 'inference': 119.40288543701172, 'postprocess': 0.40721893310546875}]\n",
      "Bounding Box: tensor([[120.5000, 218.0000, 103.0000, 156.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1670, 0.0133, 0.2041, 0.1185, 0.9245, 0.9580, 0.9987, 0.5556, 0.9954, 0.2838, 0.9557, 0.9989, 0.9998, 0.9982, 0.9997, 0.9944, 0.9985]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.6701e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3284e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0413e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1854e-01],\n",
      "         [1.0366e+02, 1.5079e+02, 9.2452e-01],\n",
      "         [8.2746e+01, 1.6598e+02, 9.5802e-01],\n",
      "         [1.0711e+02, 1.5691e+02, 9.9870e-01],\n",
      "         [8.7471e+01, 1.8683e+02, 5.5564e-01],\n",
      "         [1.3638e+02, 1.5751e+02, 9.9537e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8376e-01],\n",
      "         [1.6109e+02, 1.5779e+02, 9.5568e-01],\n",
      "         [8.1628e+01, 2.0263e+02, 9.9895e-01],\n",
      "         [9.5962e+01, 1.9790e+02, 9.9983e-01],\n",
      "         [9.7090e+01, 2.4157e+02, 9.9818e-01],\n",
      "         [1.2108e+02, 2.2353e+02, 9.9969e-01],\n",
      "         [1.0030e+02, 2.8223e+02, 9.9443e-01],\n",
      "         [1.1878e+02, 2.5204e+02, 9.9855e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [103.6616, 150.7933],\n",
      "         [ 82.7465, 165.9804],\n",
      "         [107.1071, 156.9101],\n",
      "         [ 87.4711, 186.8253],\n",
      "         [136.3833, 157.5148],\n",
      "         [  0.0000,   0.0000],\n",
      "         [161.0869, 157.7867],\n",
      "         [ 81.6285, 202.6289],\n",
      "         [ 95.9620, 197.9012],\n",
      "         [ 97.0905, 241.5688],\n",
      "         [121.0788, 223.5287],\n",
      "         [100.2959, 282.2258],\n",
      "         [118.7818, 252.0371]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2879, 0.4189],\n",
      "         [0.2299, 0.4611],\n",
      "         [0.2975, 0.4359],\n",
      "         [0.2430, 0.5190],\n",
      "         [0.3788, 0.4375],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4475, 0.4383],\n",
      "         [0.2267, 0.5629],\n",
      "         [0.2666, 0.5497],\n",
      "         [0.2697, 0.6710],\n",
      "         [0.3363, 0.6209],\n",
      "         [0.2786, 0.7840],\n",
      "         [0.3299, 0.7001]]])\n",
      "\n",
      "0: 640x640 1 person, 118.0ms\n",
      "Speed: 1.3ms preprocess, 118.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 196, 231],\n",
      "        [174, 196, 231],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       [[174, 196, 231],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2760162353515625, 'inference': 118.04890632629395, 'postprocess': 0.40602684020996094}]\n",
      "Bounding Box: tensor([[121.5000, 216.5000, 105.0000, 155.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1823, 0.0159, 0.2023, 0.1084, 0.8985, 0.9582, 0.9978, 0.6424, 0.9934, 0.3874, 0.9519, 0.9988, 0.9997, 0.9981, 0.9996, 0.9938, 0.9980]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.8233e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5851e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0235e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0844e-01],\n",
      "         [1.0274e+02, 1.4936e+02, 8.9852e-01],\n",
      "         [8.2816e+01, 1.6437e+02, 9.5816e-01],\n",
      "         [1.0545e+02, 1.5482e+02, 9.9776e-01],\n",
      "         [8.6977e+01, 1.8379e+02, 6.4240e-01],\n",
      "         [1.3370e+02, 1.5437e+02, 9.9345e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8742e-01],\n",
      "         [1.5950e+02, 1.5374e+02, 9.5190e-01],\n",
      "         [8.1260e+01, 2.0079e+02, 9.9879e-01],\n",
      "         [9.4354e+01, 1.9595e+02, 9.9974e-01],\n",
      "         [9.5979e+01, 2.3942e+02, 9.9809e-01],\n",
      "         [1.2039e+02, 2.2138e+02, 9.9956e-01],\n",
      "         [9.9395e+01, 2.8108e+02, 9.9377e-01],\n",
      "         [1.1698e+02, 2.5209e+02, 9.9798e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.7417, 149.3649],\n",
      "         [ 82.8158, 164.3733],\n",
      "         [105.4501, 154.8196],\n",
      "         [ 86.9771, 183.7866],\n",
      "         [133.7001, 154.3673],\n",
      "         [  0.0000,   0.0000],\n",
      "         [159.4979, 153.7360],\n",
      "         [ 81.2596, 200.7947],\n",
      "         [ 94.3536, 195.9535],\n",
      "         [ 95.9789, 239.4173],\n",
      "         [120.3890, 221.3752],\n",
      "         [ 99.3953, 281.0810],\n",
      "         [116.9752, 252.0860]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2854, 0.4149],\n",
      "         [0.2300, 0.4566],\n",
      "         [0.2929, 0.4301],\n",
      "         [0.2416, 0.5105],\n",
      "         [0.3714, 0.4288],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4430, 0.4270],\n",
      "         [0.2257, 0.5578],\n",
      "         [0.2621, 0.5443],\n",
      "         [0.2666, 0.6650],\n",
      "         [0.3344, 0.6149],\n",
      "         [0.2761, 0.7808],\n",
      "         [0.3249, 0.7002]]])\n",
      "\n",
      "0: 640x640 1 person, 116.2ms\n",
      "Speed: 1.3ms preprocess, 116.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 196, 231],\n",
      "        [174, 196, 231],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       [[173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        [173, 195, 230],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       [[173, 195, 228],\n",
      "        [173, 195, 228],\n",
      "        [173, 195, 228],\n",
      "        ...,\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210],\n",
      "        [119, 153, 210]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  91,  35],\n",
      "        [ 77,  93,  37],\n",
      "        [ 77,  93,  37]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2679100036621094, 'inference': 116.18781089782715, 'postprocess': 0.7121562957763672}]\n",
      "Bounding Box: tensor([[121.5000, 216.0000, 105.0000, 160.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.1935, 0.0207, 0.2276, 0.1374, 0.9024, 0.9470, 0.9970, 0.5072, 0.9886, 0.2480, 0.9104, 0.9979, 0.9996, 0.9972, 0.9994, 0.9925, 0.9976]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 1.9354e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0704e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2763e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3735e-01],\n",
      "         [1.0283e+02, 1.4725e+02, 9.0238e-01],\n",
      "         [8.3070e+01, 1.6308e+02, 9.4703e-01],\n",
      "         [1.0602e+02, 1.5340e+02, 9.9700e-01],\n",
      "         [8.7618e+01, 1.8308e+02, 5.0724e-01],\n",
      "         [1.3573e+02, 1.5163e+02, 9.8864e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4802e-01],\n",
      "         [1.6105e+02, 1.4898e+02, 9.1042e-01],\n",
      "         [8.1457e+01, 2.0196e+02, 9.9795e-01],\n",
      "         [9.4581e+01, 1.9718e+02, 9.9956e-01],\n",
      "         [9.5194e+01, 2.3998e+02, 9.9720e-01],\n",
      "         [1.1807e+02, 2.2273e+02, 9.9938e-01],\n",
      "         [9.6014e+01, 2.8209e+02, 9.9254e-01],\n",
      "         [1.1342e+02, 2.5474e+02, 9.9762e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.8343, 147.2474],\n",
      "         [ 83.0697, 163.0812],\n",
      "         [106.0200, 153.4013],\n",
      "         [ 87.6176, 183.0777],\n",
      "         [135.7322, 151.6257],\n",
      "         [  0.0000,   0.0000],\n",
      "         [161.0472, 148.9827],\n",
      "         [ 81.4568, 201.9629],\n",
      "         [ 94.5806, 197.1805],\n",
      "         [ 95.1939, 239.9773],\n",
      "         [118.0666, 222.7264],\n",
      "         [ 96.0138, 282.0918],\n",
      "         [113.4250, 254.7419]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2857, 0.4090],\n",
      "         [0.2307, 0.4530],\n",
      "         [0.2945, 0.4261],\n",
      "         [0.2434, 0.5085],\n",
      "         [0.3770, 0.4212],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4474, 0.4138],\n",
      "         [0.2263, 0.5610],\n",
      "         [0.2627, 0.5477],\n",
      "         [0.2644, 0.6666],\n",
      "         [0.3280, 0.6187],\n",
      "         [0.2667, 0.7836],\n",
      "         [0.3151, 0.7076]]])\n",
      "\n",
      "0: 640x640 1 person, 133.6ms\n",
      "Speed: 1.2ms preprocess, 133.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 230],\n",
      "        [174, 194, 230],\n",
      "        [174, 194, 230],\n",
      "        ...,\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201]],\n",
      "\n",
      "       [[174, 194, 230],\n",
      "        [174, 194, 230],\n",
      "        [174, 194, 230],\n",
      "        ...,\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [118, 157, 201],\n",
      "        [118, 157, 201],\n",
      "        [118, 157, 201]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 73,  88,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2290477752685547, 'inference': 133.64815711975098, 'postprocess': 0.41222572326660156}]\n",
      "Bounding Box: tensor([[122.0000, 213.5000, 104.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5473, 0.0849, 0.5720, 0.1936, 0.9340, 0.9694, 0.9981, 0.6747, 0.9936, 0.4802, 0.9625, 0.9981, 0.9996, 0.9972, 0.9993, 0.9903, 0.9967]])\n",
      "data: tensor([[[1.0776e+02, 1.4413e+02, 5.4728e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.4895e-02],\n",
      "         [1.0592e+02, 1.4170e+02, 5.7198e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9360e-01],\n",
      "         [1.0065e+02, 1.4406e+02, 9.3395e-01],\n",
      "         [8.2181e+01, 1.6162e+02, 9.6944e-01],\n",
      "         [1.0577e+02, 1.5210e+02, 9.9811e-01],\n",
      "         [8.5237e+01, 1.8187e+02, 6.7472e-01],\n",
      "         [1.3379e+02, 1.4991e+02, 9.9365e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8018e-01],\n",
      "         [1.5922e+02, 1.4600e+02, 9.6249e-01],\n",
      "         [8.2290e+01, 2.0125e+02, 9.9810e-01],\n",
      "         [9.5991e+01, 1.9660e+02, 9.9956e-01],\n",
      "         [9.4521e+01, 2.4088e+02, 9.9719e-01],\n",
      "         [1.1596e+02, 2.2416e+02, 9.9932e-01],\n",
      "         [9.6175e+01, 2.8429e+02, 9.9029e-01],\n",
      "         [1.1029e+02, 2.5783e+02, 9.9667e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[107.7579, 144.1313],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.9196, 141.7037],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.6466, 144.0608],\n",
      "         [ 82.1805, 161.6198],\n",
      "         [105.7677, 152.1022],\n",
      "         [ 85.2374, 181.8724],\n",
      "         [133.7910, 149.9094],\n",
      "         [  0.0000,   0.0000],\n",
      "         [159.2157, 146.0013],\n",
      "         [ 82.2897, 201.2544],\n",
      "         [ 95.9910, 196.6031],\n",
      "         [ 94.5215, 240.8767],\n",
      "         [115.9562, 224.1559],\n",
      "         [ 96.1746, 284.2948],\n",
      "         [110.2928, 257.8252]]])\n",
      "xyn: tensor([[[0.2993, 0.4004],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2942, 0.3936],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2796, 0.4002],\n",
      "         [0.2283, 0.4489],\n",
      "         [0.2938, 0.4225],\n",
      "         [0.2368, 0.5052],\n",
      "         [0.3716, 0.4164],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4423, 0.4056],\n",
      "         [0.2286, 0.5590],\n",
      "         [0.2666, 0.5461],\n",
      "         [0.2626, 0.6691],\n",
      "         [0.3221, 0.6227],\n",
      "         [0.2672, 0.7897],\n",
      "         [0.3064, 0.7162]]])\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201],\n",
      "        [119, 155, 201]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [118, 157, 201],\n",
      "        [118, 157, 201],\n",
      "        [118, 157, 201]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 73,  88,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2288093566894531, 'inference': 113.8150691986084, 'postprocess': 0.4000663757324219}]\n",
      "Bounding Box: tensor([[121.0000, 213.5000, 102.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5712, 0.1010, 0.5698, 0.2282, 0.9271, 0.9719, 0.9975, 0.7204, 0.9916, 0.5326, 0.9550, 0.9971, 0.9992, 0.9949, 0.9985, 0.9795, 0.9918]])\n",
      "data: tensor([[[1.0726e+02, 1.4272e+02, 5.7120e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0098e-01],\n",
      "         [1.0560e+02, 1.4043e+02, 5.6979e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2815e-01],\n",
      "         [1.0082e+02, 1.4287e+02, 9.2710e-01],\n",
      "         [8.2111e+01, 1.6042e+02, 9.7193e-01],\n",
      "         [1.0566e+02, 1.5150e+02, 9.9745e-01],\n",
      "         [8.2014e+01, 1.8289e+02, 7.2045e-01],\n",
      "         [1.3383e+02, 1.4789e+02, 9.9163e-01],\n",
      "         [9.9542e+01, 1.9690e+02, 5.3259e-01],\n",
      "         [1.5715e+02, 1.4203e+02, 9.5497e-01],\n",
      "         [8.2050e+01, 1.9991e+02, 9.9710e-01],\n",
      "         [9.5308e+01, 1.9557e+02, 9.9918e-01],\n",
      "         [9.5071e+01, 2.3965e+02, 9.9488e-01],\n",
      "         [1.1148e+02, 2.2433e+02, 9.9846e-01],\n",
      "         [9.6077e+01, 2.8419e+02, 9.7949e-01],\n",
      "         [1.0863e+02, 2.5651e+02, 9.9177e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[107.2570, 142.7166],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.6032, 140.4312],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.8167, 142.8653],\n",
      "         [ 82.1106, 160.4199],\n",
      "         [105.6560, 151.4967],\n",
      "         [ 82.0140, 182.8939],\n",
      "         [133.8317, 147.8891],\n",
      "         [ 99.5419, 196.8966],\n",
      "         [157.1523, 142.0334],\n",
      "         [ 82.0504, 199.9123],\n",
      "         [ 95.3084, 195.5739],\n",
      "         [ 95.0710, 239.6503],\n",
      "         [111.4758, 224.3300],\n",
      "         [ 96.0774, 284.1902],\n",
      "         [108.6251, 256.5126]]])\n",
      "xyn: tensor([[[0.2979, 0.3964],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2933, 0.3901],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2800, 0.3968],\n",
      "         [0.2281, 0.4456],\n",
      "         [0.2935, 0.4208],\n",
      "         [0.2278, 0.5080],\n",
      "         [0.3718, 0.4108],\n",
      "         [0.2765, 0.5469],\n",
      "         [0.4365, 0.3945],\n",
      "         [0.2279, 0.5553],\n",
      "         [0.2647, 0.5433],\n",
      "         [0.2641, 0.6657],\n",
      "         [0.3097, 0.6231],\n",
      "         [0.2669, 0.7894],\n",
      "         [0.3017, 0.7125]]])\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.3ms preprocess, 114.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [120, 154, 208],\n",
      "        [120, 154, 208],\n",
      "        [120, 154, 208]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [120, 154, 208],\n",
      "        [120, 154, 208],\n",
      "        [120, 154, 208]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [119, 154, 208],\n",
      "        [119, 154, 208],\n",
      "        [119, 154, 208]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 73,  88,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.255035400390625, 'inference': 114.19510841369629, 'postprocess': 0.3998279571533203}]\n",
      "Bounding Box: tensor([[121.5000, 213.5000, 103.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5925, 0.0947, 0.6092, 0.1651, 0.9241, 0.9660, 0.9978, 0.7169, 0.9947, 0.5723, 0.9737, 0.9982, 0.9996, 0.9972, 0.9993, 0.9889, 0.9959]])\n",
      "data: tensor([[[1.0696e+02, 1.4205e+02, 5.9250e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.4741e-02],\n",
      "         [1.0518e+02, 1.3964e+02, 6.0919e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6509e-01],\n",
      "         [9.9450e+01, 1.4215e+02, 9.2406e-01],\n",
      "         [8.1426e+01, 1.5950e+02, 9.6598e-01],\n",
      "         [1.0482e+02, 1.5104e+02, 9.9782e-01],\n",
      "         [8.1577e+01, 1.7951e+02, 7.1687e-01],\n",
      "         [1.3107e+02, 1.4641e+02, 9.9474e-01],\n",
      "         [9.7578e+01, 1.8659e+02, 5.7228e-01],\n",
      "         [1.5526e+02, 1.3914e+02, 9.7374e-01],\n",
      "         [8.2943e+01, 2.0002e+02, 9.9820e-01],\n",
      "         [9.5988e+01, 1.9672e+02, 9.9958e-01],\n",
      "         [9.3173e+01, 2.3970e+02, 9.9717e-01],\n",
      "         [1.0941e+02, 2.3056e+02, 9.9931e-01],\n",
      "         [9.5385e+01, 2.8325e+02, 9.8889e-01],\n",
      "         [1.0227e+02, 2.6717e+02, 9.9591e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[106.9602, 142.0483],\n",
      "         [  0.0000,   0.0000],\n",
      "         [105.1845, 139.6369],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 99.4499, 142.1461],\n",
      "         [ 81.4262, 159.4966],\n",
      "         [104.8234, 151.0442],\n",
      "         [ 81.5766, 179.5117],\n",
      "         [131.0653, 146.4137],\n",
      "         [ 97.5782, 186.5878],\n",
      "         [155.2557, 139.1351],\n",
      "         [ 82.9427, 200.0216],\n",
      "         [ 95.9877, 196.7224],\n",
      "         [ 93.1729, 239.6958],\n",
      "         [109.4068, 230.5626],\n",
      "         [ 95.3855, 283.2489],\n",
      "         [102.2743, 267.1680]]])\n",
      "xyn: tensor([[[0.2971, 0.3946],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2922, 0.3879],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2762, 0.3949],\n",
      "         [0.2262, 0.4430],\n",
      "         [0.2912, 0.4196],\n",
      "         [0.2266, 0.4986],\n",
      "         [0.3641, 0.4067],\n",
      "         [0.2711, 0.5183],\n",
      "         [0.4313, 0.3865],\n",
      "         [0.2304, 0.5556],\n",
      "         [0.2666, 0.5465],\n",
      "         [0.2588, 0.6658],\n",
      "         [0.3039, 0.6405],\n",
      "         [0.2650, 0.7868],\n",
      "         [0.2841, 0.7421]]])\n",
      "\n",
      "0: 640x640 1 person, 117.7ms\n",
      "Speed: 1.3ms preprocess, 117.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 165, 201],\n",
      "        [129, 165, 201],\n",
      "        [127, 164, 200]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 165, 201],\n",
      "        [129, 165, 201],\n",
      "        [127, 164, 200]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 165, 201],\n",
      "        [129, 165, 201],\n",
      "        [127, 164, 200]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  91,  35],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2657642364501953, 'inference': 117.69413948059082, 'postprocess': 0.6668567657470703}]\n",
      "Bounding Box: tensor([[120.5000, 213.0000, 101.0000, 168.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5768, 0.0965, 0.5574, 0.1591, 0.8978, 0.9700, 0.9970, 0.7992, 0.9936, 0.6784, 0.9726, 0.9986, 0.9996, 0.9977, 0.9993, 0.9905, 0.9957]])\n",
      "data: tensor([[[1.0645e+02, 1.4141e+02, 5.7675e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.6470e-02],\n",
      "         [1.0478e+02, 1.3890e+02, 5.5736e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5907e-01],\n",
      "         [9.8853e+01, 1.4146e+02, 8.9778e-01],\n",
      "         [8.2433e+01, 1.5917e+02, 9.7005e-01],\n",
      "         [1.0322e+02, 1.5058e+02, 9.9698e-01],\n",
      "         [8.3057e+01, 1.7869e+02, 7.9919e-01],\n",
      "         [1.2965e+02, 1.4525e+02, 9.9361e-01],\n",
      "         [9.9993e+01, 1.8052e+02, 6.7845e-01],\n",
      "         [1.5214e+02, 1.3728e+02, 9.7260e-01],\n",
      "         [8.3716e+01, 2.0010e+02, 9.9857e-01],\n",
      "         [9.4717e+01, 1.9733e+02, 9.9958e-01],\n",
      "         [9.3217e+01, 2.3734e+02, 9.9772e-01],\n",
      "         [1.0363e+02, 2.3299e+02, 9.9930e-01],\n",
      "         [9.6232e+01, 2.7962e+02, 9.9049e-01],\n",
      "         [9.9499e+01, 2.7049e+02, 9.9566e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[106.4463, 141.4077],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.7812, 138.9037],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 98.8528, 141.4615],\n",
      "         [ 82.4333, 159.1665],\n",
      "         [103.2243, 150.5840],\n",
      "         [ 83.0565, 178.6881],\n",
      "         [129.6493, 145.2490],\n",
      "         [ 99.9931, 180.5154],\n",
      "         [152.1362, 137.2815],\n",
      "         [ 83.7155, 200.0957],\n",
      "         [ 94.7166, 197.3344],\n",
      "         [ 93.2165, 237.3414],\n",
      "         [103.6289, 232.9890],\n",
      "         [ 96.2315, 279.6160],\n",
      "         [ 99.4985, 270.4932]]])\n",
      "xyn: tensor([[[0.2957, 0.3928],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2911, 0.3858],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2746, 0.3929],\n",
      "         [0.2290, 0.4421],\n",
      "         [0.2867, 0.4183],\n",
      "         [0.2307, 0.4964],\n",
      "         [0.3601, 0.4035],\n",
      "         [0.2778, 0.5014],\n",
      "         [0.4226, 0.3813],\n",
      "         [0.2325, 0.5558],\n",
      "         [0.2631, 0.5482],\n",
      "         [0.2589, 0.6593],\n",
      "         [0.2879, 0.6472],\n",
      "         [0.2673, 0.7767],\n",
      "         [0.2764, 0.7514]]])\n",
      "\n",
      "0: 640x640 1 person, 113.2ms\n",
      "Speed: 1.2ms preprocess, 113.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [127, 159, 205],\n",
      "        [127, 159, 205],\n",
      "        [127, 159, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  91,  35],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.1911392211914062, 'inference': 113.21330070495605, 'postprocess': 0.4029273986816406}]\n",
      "Bounding Box: tensor([[117.5000, 211.5000,  97.0000, 169.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6095, 0.1453, 0.5978, 0.2302, 0.8906, 0.9600, 0.9947, 0.6959, 0.9854, 0.5250, 0.9380, 0.9971, 0.9991, 0.9960, 0.9987, 0.9870, 0.9936]])\n",
      "data: tensor([[[1.0485e+02, 1.4052e+02, 6.0950e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4533e-01],\n",
      "         [1.0295e+02, 1.3810e+02, 5.9782e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3021e-01],\n",
      "         [9.7823e+01, 1.4115e+02, 8.9058e-01],\n",
      "         [8.1115e+01, 1.5830e+02, 9.5999e-01],\n",
      "         [1.0462e+02, 1.5055e+02, 9.9471e-01],\n",
      "         [7.9061e+01, 1.7764e+02, 6.9591e-01],\n",
      "         [1.2898e+02, 1.4345e+02, 9.8537e-01],\n",
      "         [9.4659e+01, 1.8119e+02, 5.2503e-01],\n",
      "         [1.4995e+02, 1.3456e+02, 9.3801e-01],\n",
      "         [8.2235e+01, 2.0051e+02, 9.9707e-01],\n",
      "         [9.4606e+01, 1.9857e+02, 9.9908e-01],\n",
      "         [9.1929e+01, 2.3553e+02, 9.9600e-01],\n",
      "         [1.0186e+02, 2.3601e+02, 9.9873e-01],\n",
      "         [9.6137e+01, 2.7598e+02, 9.8700e-01],\n",
      "         [9.4319e+01, 2.7494e+02, 9.9365e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[104.8531, 140.5243],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.9470, 138.1012],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 97.8231, 141.1503],\n",
      "         [ 81.1148, 158.3049],\n",
      "         [104.6227, 150.5486],\n",
      "         [ 79.0606, 177.6366],\n",
      "         [128.9848, 143.4538],\n",
      "         [ 94.6587, 181.1931],\n",
      "         [149.9464, 134.5620],\n",
      "         [ 82.2348, 200.5142],\n",
      "         [ 94.6064, 198.5748],\n",
      "         [ 91.9285, 235.5332],\n",
      "         [101.8599, 236.0066],\n",
      "         [ 96.1369, 275.9841],\n",
      "         [ 94.3188, 274.9409]]])\n",
      "xyn: tensor([[[0.2913, 0.3903],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2860, 0.3836],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2717, 0.3921],\n",
      "         [0.2253, 0.4397],\n",
      "         [0.2906, 0.4182],\n",
      "         [0.2196, 0.4934],\n",
      "         [0.3583, 0.3985],\n",
      "         [0.2629, 0.5033],\n",
      "         [0.4165, 0.3738],\n",
      "         [0.2284, 0.5570],\n",
      "         [0.2628, 0.5516],\n",
      "         [0.2554, 0.6543],\n",
      "         [0.2829, 0.6556],\n",
      "         [0.2670, 0.7666],\n",
      "         [0.2620, 0.7637]]])\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 1.2ms preprocess, 113.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205],\n",
      "        [129, 158, 205]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [127, 159, 205],\n",
      "        [127, 159, 205],\n",
      "        [127, 159, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2319087982177734, 'inference': 113.55781555175781, 'postprocess': 0.4038810729980469}]\n",
      "Bounding Box: tensor([[112., 210.,  90., 166.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5537, 0.1146, 0.5914, 0.2030, 0.9057, 0.9351, 0.9936, 0.5490, 0.9824, 0.3606, 0.9171, 0.9914, 0.9977, 0.9886, 0.9971, 0.9684, 0.9861]])\n",
      "data: tensor([[[1.0459e+02, 1.4027e+02, 5.5369e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1460e-01],\n",
      "         [1.0271e+02, 1.3777e+02, 5.9139e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0299e-01],\n",
      "         [9.7812e+01, 1.4135e+02, 9.0568e-01],\n",
      "         [8.0567e+01, 1.5758e+02, 9.3510e-01],\n",
      "         [1.0370e+02, 1.5155e+02, 9.9360e-01],\n",
      "         [7.9729e+01, 1.7724e+02, 5.4900e-01],\n",
      "         [1.2818e+02, 1.4514e+02, 9.8245e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6058e-01],\n",
      "         [1.4558e+02, 1.3283e+02, 9.1710e-01],\n",
      "         [8.0158e+01, 2.0045e+02, 9.9142e-01],\n",
      "         [9.2180e+01, 1.9970e+02, 9.9771e-01],\n",
      "         [9.3320e+01, 2.3268e+02, 9.8865e-01],\n",
      "         [9.9790e+01, 2.3698e+02, 9.9706e-01],\n",
      "         [9.7134e+01, 2.7131e+02, 9.6843e-01],\n",
      "         [9.3869e+01, 2.7727e+02, 9.8611e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[104.5940, 140.2721],\n",
      "         [  0.0000,   0.0000],\n",
      "         [102.7094, 137.7684],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 97.8116, 141.3515],\n",
      "         [ 80.5670, 157.5836],\n",
      "         [103.7012, 151.5500],\n",
      "         [ 79.7289, 177.2416],\n",
      "         [128.1767, 145.1420],\n",
      "         [  0.0000,   0.0000],\n",
      "         [145.5845, 132.8275],\n",
      "         [ 80.1579, 200.4529],\n",
      "         [ 92.1796, 199.7039],\n",
      "         [ 93.3199, 232.6813],\n",
      "         [ 99.7899, 236.9810],\n",
      "         [ 97.1338, 271.3120],\n",
      "         [ 93.8695, 277.2650]]])\n",
      "xyn: tensor([[[0.2905, 0.3896],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2853, 0.3827],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2717, 0.3926],\n",
      "         [0.2238, 0.4377],\n",
      "         [0.2881, 0.4210],\n",
      "         [0.2215, 0.4923],\n",
      "         [0.3560, 0.4032],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4044, 0.3690],\n",
      "         [0.2227, 0.5568],\n",
      "         [0.2561, 0.5547],\n",
      "         [0.2592, 0.6463],\n",
      "         [0.2772, 0.6583],\n",
      "         [0.2698, 0.7536],\n",
      "         [0.2607, 0.7702]]])\n",
      "\n",
      "0: 640x640 1 person, 117.3ms\n",
      "Speed: 1.2ms preprocess, 117.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202],\n",
      "        [132, 162, 202]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2049674987792969, 'inference': 117.3238754272461, 'postprocess': 0.39768218994140625}]\n",
      "Bounding Box: tensor([[113., 207.,  92., 168.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5540, 0.1022, 0.5886, 0.1987, 0.9325, 0.9594, 0.9979, 0.6817, 0.9946, 0.5111, 0.9688, 0.9982, 0.9996, 0.9971, 0.9994, 0.9914, 0.9967]])\n",
      "data: tensor([[[1.0445e+02, 1.4008e+02, 5.5404e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0215e-01],\n",
      "         [1.0198e+02, 1.3751e+02, 5.8857e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9869e-01],\n",
      "         [9.6803e+01, 1.4124e+02, 9.3247e-01],\n",
      "         [8.0651e+01, 1.5773e+02, 9.5942e-01],\n",
      "         [1.0309e+02, 1.5122e+02, 9.9789e-01],\n",
      "         [7.9267e+01, 1.7569e+02, 6.8166e-01],\n",
      "         [1.2599e+02, 1.4308e+02, 9.9461e-01],\n",
      "         [9.3324e+01, 1.7578e+02, 5.1108e-01],\n",
      "         [1.4472e+02, 1.3196e+02, 9.6875e-01],\n",
      "         [8.0393e+01, 2.0026e+02, 9.9820e-01],\n",
      "         [9.2487e+01, 1.9977e+02, 9.9963e-01],\n",
      "         [9.1464e+01, 2.3133e+02, 9.9708e-01],\n",
      "         [9.9454e+01, 2.3599e+02, 9.9938e-01],\n",
      "         [9.7006e+01, 2.6938e+02, 9.9139e-01],\n",
      "         [9.2361e+01, 2.7497e+02, 9.9674e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[104.4461, 140.0836],\n",
      "         [  0.0000,   0.0000],\n",
      "         [101.9814, 137.5095],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 96.8032, 141.2380],\n",
      "         [ 80.6508, 157.7311],\n",
      "         [103.0925, 151.2198],\n",
      "         [ 79.2671, 175.6934],\n",
      "         [125.9925, 143.0820],\n",
      "         [ 93.3244, 175.7836],\n",
      "         [144.7168, 131.9602],\n",
      "         [ 80.3926, 200.2562],\n",
      "         [ 92.4874, 199.7670],\n",
      "         [ 91.4640, 231.3315],\n",
      "         [ 99.4545, 235.9855],\n",
      "         [ 97.0065, 269.3799],\n",
      "         [ 92.3608, 274.9739]]])\n",
      "xyn: tensor([[[0.2901, 0.3891],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2833, 0.3820],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2689, 0.3923],\n",
      "         [0.2240, 0.4381],\n",
      "         [0.2864, 0.4201],\n",
      "         [0.2202, 0.4880],\n",
      "         [0.3500, 0.3975],\n",
      "         [0.2592, 0.4883],\n",
      "         [0.4020, 0.3666],\n",
      "         [0.2233, 0.5563],\n",
      "         [0.2569, 0.5549],\n",
      "         [0.2541, 0.6426],\n",
      "         [0.2763, 0.6555],\n",
      "         [0.2695, 0.7483],\n",
      "         [0.2566, 0.7638]]])\n",
      "\n",
      "0: 640x640 1 person, 113.1ms\n",
      "Speed: 1.4ms preprocess, 113.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 69,  84,  28]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 63,  79,  23]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3811588287353516, 'inference': 113.13390731811523, 'postprocess': 0.39386749267578125}]\n",
      "Bounding Box: tensor([[113.0000, 206.5000,  94.0000, 169.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.3213, 0.0480, 0.3454, 0.2099, 0.9182, 0.9593, 0.9976, 0.6475, 0.9924, 0.3969, 0.9437, 0.9983, 0.9996, 0.9969, 0.9993, 0.9912, 0.9965]])\n",
      "data: tensor([[[0.0000e+00, 0.0000e+00, 3.2128e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8037e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4539e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0995e-01],\n",
      "         [9.6992e+01, 1.4116e+02, 9.1815e-01],\n",
      "         [7.9395e+01, 1.5765e+02, 9.5929e-01],\n",
      "         [1.0263e+02, 1.5100e+02, 9.9758e-01],\n",
      "         [7.7893e+01, 1.7545e+02, 6.4750e-01],\n",
      "         [1.2540e+02, 1.4180e+02, 9.9240e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9685e-01],\n",
      "         [1.4498e+02, 1.3099e+02, 9.4375e-01],\n",
      "         [7.9448e+01, 2.0121e+02, 9.9834e-01],\n",
      "         [9.2160e+01, 2.0071e+02, 9.9964e-01],\n",
      "         [8.8079e+01, 2.3082e+02, 9.9690e-01],\n",
      "         [9.6903e+01, 2.3629e+02, 9.9931e-01],\n",
      "         [9.3725e+01, 2.6721e+02, 9.9116e-01],\n",
      "         [9.1475e+01, 2.7455e+02, 9.9650e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 96.9921, 141.1594],\n",
      "         [ 79.3946, 157.6544],\n",
      "         [102.6335, 151.0001],\n",
      "         [ 77.8933, 175.4466],\n",
      "         [125.4012, 141.7968],\n",
      "         [  0.0000,   0.0000],\n",
      "         [144.9834, 130.9913],\n",
      "         [ 79.4484, 201.2128],\n",
      "         [ 92.1597, 200.7136],\n",
      "         [ 88.0794, 230.8189],\n",
      "         [ 96.9033, 236.2889],\n",
      "         [ 93.7245, 267.2052],\n",
      "         [ 91.4754, 274.5529]]])\n",
      "xyn: tensor([[[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2694, 0.3921],\n",
      "         [0.2205, 0.4379],\n",
      "         [0.2851, 0.4194],\n",
      "         [0.2164, 0.4874],\n",
      "         [0.3483, 0.3939],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4027, 0.3639],\n",
      "         [0.2207, 0.5589],\n",
      "         [0.2560, 0.5575],\n",
      "         [0.2447, 0.6412],\n",
      "         [0.2692, 0.6564],\n",
      "         [0.2603, 0.7422],\n",
      "         [0.2541, 0.7626]]])\n",
      "\n",
      "0: 640x640 1 person, 129.2ms\n",
      "Speed: 1.3ms preprocess, 129.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        [174, 194, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3480186462402344, 'inference': 129.2271614074707, 'postprocess': 0.4169940948486328}]\n",
      "Bounding Box: tensor([[111.5000, 206.5000,  91.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6073, 0.1058, 0.7366, 0.1267, 0.9453, 0.8841, 0.9978, 0.3006, 0.9940, 0.2068, 0.9613, 0.9955, 0.9995, 0.9950, 0.9994, 0.9905, 0.9977]])\n",
      "data: tensor([[[1.0257e+02, 1.4034e+02, 6.0735e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0577e-01],\n",
      "         [1.0035e+02, 1.3752e+02, 7.3659e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2670e-01],\n",
      "         [9.3465e+01, 1.4016e+02, 9.4531e-01],\n",
      "         [7.7086e+01, 1.5644e+02, 8.8408e-01],\n",
      "         [9.7557e+01, 1.5145e+02, 9.9779e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0056e-01],\n",
      "         [1.2132e+02, 1.4365e+02, 9.9405e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0682e-01],\n",
      "         [1.4067e+02, 1.3150e+02, 9.6130e-01],\n",
      "         [7.8300e+01, 2.0124e+02, 9.9546e-01],\n",
      "         [9.0061e+01, 2.0072e+02, 9.9948e-01],\n",
      "         [8.6739e+01, 2.3430e+02, 9.9505e-01],\n",
      "         [9.7106e+01, 2.3527e+02, 9.9941e-01],\n",
      "         [9.0451e+01, 2.7338e+02, 9.9055e-01],\n",
      "         [9.2684e+01, 2.7146e+02, 9.9771e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[102.5677, 140.3439],\n",
      "         [  0.0000,   0.0000],\n",
      "         [100.3450, 137.5235],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 93.4646, 140.1577],\n",
      "         [ 77.0861, 156.4369],\n",
      "         [ 97.5572, 151.4477],\n",
      "         [  0.0000,   0.0000],\n",
      "         [121.3219, 143.6495],\n",
      "         [  0.0000,   0.0000],\n",
      "         [140.6723, 131.4973],\n",
      "         [ 78.2998, 201.2396],\n",
      "         [ 90.0612, 200.7183],\n",
      "         [ 86.7392, 234.3007],\n",
      "         [ 97.1063, 235.2746],\n",
      "         [ 90.4507, 273.3781],\n",
      "         [ 92.6836, 271.4648]]])\n",
      "xyn: tensor([[[0.2849, 0.3898],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2787, 0.3820],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2596, 0.3893],\n",
      "         [0.2141, 0.4345],\n",
      "         [0.2710, 0.4207],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3370, 0.3990],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3908, 0.3653],\n",
      "         [0.2175, 0.5590],\n",
      "         [0.2502, 0.5576],\n",
      "         [0.2409, 0.6508],\n",
      "         [0.2697, 0.6535],\n",
      "         [0.2513, 0.7594],\n",
      "         [0.2575, 0.7541]]])\n",
      "\n",
      "0: 640x640 1 person, 112.5ms\n",
      "Speed: 1.3ms preprocess, 112.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[175, 194, 226],\n",
      "        [175, 194, 226],\n",
      "        [175, 194, 226],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2888908386230469, 'inference': 112.48183250427246, 'postprocess': 0.408172607421875}]\n",
      "Bounding Box: tensor([[109.5000, 205.0000,  89.0000, 166.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5749, 0.1006, 0.6860, 0.1282, 0.9444, 0.8998, 0.9979, 0.4036, 0.9948, 0.2938, 0.9679, 0.9957, 0.9994, 0.9908, 0.9987, 0.9746, 0.9932]])\n",
      "data: tensor([[[9.9923e+01, 1.3937e+02, 5.7488e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0061e-01],\n",
      "         [9.7792e+01, 1.3697e+02, 6.8597e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2822e-01],\n",
      "         [9.1675e+01, 1.3972e+02, 9.4435e-01],\n",
      "         [7.5161e+01, 1.5501e+02, 8.9983e-01],\n",
      "         [9.5801e+01, 1.5089e+02, 9.9790e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.0362e-01],\n",
      "         [1.1814e+02, 1.4371e+02, 9.9477e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9384e-01],\n",
      "         [1.3763e+02, 1.3002e+02, 9.6792e-01],\n",
      "         [7.6596e+01, 1.9998e+02, 9.9572e-01],\n",
      "         [8.8750e+01, 1.9969e+02, 9.9944e-01],\n",
      "         [8.4209e+01, 2.3397e+02, 9.9078e-01],\n",
      "         [9.5450e+01, 2.3428e+02, 9.9870e-01],\n",
      "         [8.7443e+01, 2.7167e+02, 9.7464e-01],\n",
      "         [9.1440e+01, 2.6760e+02, 9.9317e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 99.9235, 139.3657],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 97.7921, 136.9710],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 91.6752, 139.7164],\n",
      "         [ 75.1612, 155.0115],\n",
      "         [ 95.8015, 150.8908],\n",
      "         [  0.0000,   0.0000],\n",
      "         [118.1432, 143.7071],\n",
      "         [  0.0000,   0.0000],\n",
      "         [137.6311, 130.0246],\n",
      "         [ 76.5956, 199.9814],\n",
      "         [ 88.7505, 199.6889],\n",
      "         [ 84.2088, 233.9727],\n",
      "         [ 95.4499, 234.2769],\n",
      "         [ 87.4431, 271.6656],\n",
      "         [ 91.4400, 267.6013]]])\n",
      "xyn: tensor([[[0.2776, 0.3871],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2716, 0.3805],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2547, 0.3881],\n",
      "         [0.2088, 0.4306],\n",
      "         [0.2661, 0.4191],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3282, 0.3992],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3823, 0.3612],\n",
      "         [0.2128, 0.5555],\n",
      "         [0.2465, 0.5547],\n",
      "         [0.2339, 0.6499],\n",
      "         [0.2651, 0.6508],\n",
      "         [0.2429, 0.7546],\n",
      "         [0.2540, 0.7433]]])\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 1.3ms preprocess, 117.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        [175, 193, 228],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[175, 194, 226],\n",
      "        [175, 194, 226],\n",
      "        [175, 194, 226],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3020038604736328, 'inference': 117.89917945861816, 'postprocess': 0.4048347473144531}]\n",
      "Bounding Box: tensor([[109.0000, 204.5000,  88.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5205, 0.0876, 0.6127, 0.1647, 0.9473, 0.9360, 0.9982, 0.5067, 0.9947, 0.3352, 0.9627, 0.9970, 0.9995, 0.9949, 0.9992, 0.9862, 0.9958]])\n",
      "data: tensor([[[9.6928e+01, 1.3674e+02, 5.2053e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.7598e-02],\n",
      "         [9.4583e+01, 1.3437e+02, 6.1267e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6474e-01],\n",
      "         [8.8925e+01, 1.3829e+02, 9.4725e-01],\n",
      "         [7.2204e+01, 1.5544e+02, 9.3596e-01],\n",
      "         [9.3925e+01, 1.4937e+02, 9.9821e-01],\n",
      "         [7.3952e+01, 1.6835e+02, 5.0666e-01],\n",
      "         [1.1686e+02, 1.4074e+02, 9.9473e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.3520e-01],\n",
      "         [1.3858e+02, 1.2955e+02, 9.6272e-01],\n",
      "         [7.5081e+01, 1.9901e+02, 9.9698e-01],\n",
      "         [8.7841e+01, 1.9812e+02, 9.9953e-01],\n",
      "         [8.2564e+01, 2.3374e+02, 9.9492e-01],\n",
      "         [9.5346e+01, 2.3319e+02, 9.9916e-01],\n",
      "         [8.4467e+01, 2.7249e+02, 9.8622e-01],\n",
      "         [8.9512e+01, 2.6867e+02, 9.9585e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 96.9280, 136.7383],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 94.5834, 134.3749],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 88.9252, 138.2871],\n",
      "         [ 72.2040, 155.4403],\n",
      "         [ 93.9252, 149.3719],\n",
      "         [ 73.9521, 168.3498],\n",
      "         [116.8605, 140.7449],\n",
      "         [  0.0000,   0.0000],\n",
      "         [138.5793, 129.5530],\n",
      "         [ 75.0814, 199.0146],\n",
      "         [ 87.8414, 198.1170],\n",
      "         [ 82.5643, 233.7403],\n",
      "         [ 95.3458, 233.1908],\n",
      "         [ 84.4667, 272.4946],\n",
      "         [ 89.5124, 268.6715]]])\n",
      "xyn: tensor([[[0.2692, 0.3798],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2627, 0.3733],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2470, 0.3841],\n",
      "         [0.2006, 0.4318],\n",
      "         [0.2609, 0.4149],\n",
      "         [0.2054, 0.4676],\n",
      "         [0.3246, 0.3910],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3849, 0.3599],\n",
      "         [0.2086, 0.5528],\n",
      "         [0.2440, 0.5503],\n",
      "         [0.2293, 0.6493],\n",
      "         [0.2648, 0.6478],\n",
      "         [0.2346, 0.7569],\n",
      "         [0.2486, 0.7463]]])\n",
      "\n",
      "0: 640x640 1 person, 112.0ms\n",
      "Speed: 1.3ms preprocess, 112.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198],\n",
      "        [132, 164, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 74,  89,  33],\n",
      "        [ 74,  89,  33],\n",
      "        [ 74,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27],\n",
      "        [ 67,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 63,  79,  23],\n",
      "        [ 62,  77,  22],\n",
      "        [ 60,  75,  19]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3310909271240234, 'inference': 112.03575134277344, 'postprocess': 0.39505958557128906}]\n",
      "Bounding Box: tensor([[108.5000, 202.0000,  91.0000, 164.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.5586, 0.0689, 0.6882, 0.1110, 0.9532, 0.9137, 0.9988, 0.3572, 0.9971, 0.2529, 0.9802, 0.9972, 0.9997, 0.9944, 0.9994, 0.9835, 0.9965]])\n",
      "data: tensor([[[9.5801e+01, 1.3606e+02, 5.5864e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.8922e-02],\n",
      "         [9.3723e+01, 1.3360e+02, 6.8823e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1100e-01],\n",
      "         [8.7401e+01, 1.3673e+02, 9.5324e-01],\n",
      "         [7.0228e+01, 1.5462e+02, 9.1366e-01],\n",
      "         [9.3276e+01, 1.4827e+02, 9.9884e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5716e-01],\n",
      "         [1.1743e+02, 1.3849e+02, 9.9709e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.5295e-01],\n",
      "         [1.3641e+02, 1.2676e+02, 9.8021e-01],\n",
      "         [7.4150e+01, 1.9967e+02, 9.9716e-01],\n",
      "         [8.7964e+01, 1.9844e+02, 9.9972e-01],\n",
      "         [8.0234e+01, 2.3403e+02, 9.9435e-01],\n",
      "         [9.3703e+01, 2.3421e+02, 9.9943e-01],\n",
      "         [7.9838e+01, 2.7144e+02, 9.8353e-01],\n",
      "         [9.0381e+01, 2.6974e+02, 9.9652e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 95.8013, 136.0633],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 93.7230, 133.5964],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 87.4007, 136.7298],\n",
      "         [ 70.2280, 154.6166],\n",
      "         [ 93.2756, 148.2696],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.4262, 138.4909],\n",
      "         [  0.0000,   0.0000],\n",
      "         [136.4121, 126.7580],\n",
      "         [ 74.1496, 199.6692],\n",
      "         [ 87.9641, 198.4409],\n",
      "         [ 80.2342, 234.0283],\n",
      "         [ 93.7026, 234.2079],\n",
      "         [ 79.8384, 271.4437],\n",
      "         [ 90.3815, 269.7368]]])\n",
      "xyn: tensor([[[0.2661, 0.3780],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2603, 0.3711],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2428, 0.3798],\n",
      "         [0.1951, 0.4295],\n",
      "         [0.2591, 0.4119],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3262, 0.3847],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3789, 0.3521],\n",
      "         [0.2060, 0.5546],\n",
      "         [0.2443, 0.5512],\n",
      "         [0.2229, 0.6501],\n",
      "         [0.2603, 0.6506],\n",
      "         [0.2218, 0.7540],\n",
      "         [0.2511, 0.7493]]])\n",
      "\n",
      "0: 640x640 1 person, 119.8ms\n",
      "Speed: 1.3ms preprocess, 119.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        [173, 192, 222],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 74,  89,  33],\n",
      "        [ 73,  88,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 70,  86,  30],\n",
      "        [ 68,  83,  27],\n",
      "        [ 67,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 67,  82,  26],\n",
      "        [ 65,  80,  24],\n",
      "        [ 62,  77,  22]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3048648834228516, 'inference': 119.77720260620117, 'postprocess': 0.6909370422363281}]\n",
      "Bounding Box: tensor([[106.5000, 201.0000,  91.0000, 164.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7132, 0.1479, 0.8145, 0.1166, 0.9544, 0.8919, 0.9980, 0.3597, 0.9954, 0.3015, 0.9759, 0.9933, 0.9992, 0.9881, 0.9985, 0.9681, 0.9919]])\n",
      "data: tensor([[[9.4620e+01, 1.3559e+02, 7.1321e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4794e-01],\n",
      "         [9.1896e+01, 1.3289e+02, 8.1454e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1660e-01],\n",
      "         [8.4242e+01, 1.3545e+02, 9.5445e-01],\n",
      "         [6.9376e+01, 1.5322e+02, 8.9190e-01],\n",
      "         [9.0482e+01, 1.4707e+02, 9.9801e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5970e-01],\n",
      "         [1.1459e+02, 1.3850e+02, 9.9542e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0153e-01],\n",
      "         [1.3388e+02, 1.2677e+02, 9.7594e-01],\n",
      "         [7.1749e+01, 1.9582e+02, 9.9326e-01],\n",
      "         [8.4933e+01, 1.9513e+02, 9.9920e-01],\n",
      "         [7.7903e+01, 2.2939e+02, 9.8814e-01],\n",
      "         [9.1931e+01, 2.3119e+02, 9.9854e-01],\n",
      "         [7.6644e+01, 2.6694e+02, 9.6807e-01],\n",
      "         [8.6327e+01, 2.6642e+02, 9.9195e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 94.6203, 135.5908],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 91.8955, 132.8946],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 84.2424, 135.4534],\n",
      "         [ 69.3763, 153.2229],\n",
      "         [ 90.4822, 147.0726],\n",
      "         [  0.0000,   0.0000],\n",
      "         [114.5853, 138.5038],\n",
      "         [  0.0000,   0.0000],\n",
      "         [133.8811, 126.7744],\n",
      "         [ 71.7491, 195.8221],\n",
      "         [ 84.9325, 195.1323],\n",
      "         [ 77.9035, 229.3858],\n",
      "         [ 91.9313, 231.1929],\n",
      "         [ 76.6443, 266.9355],\n",
      "         [ 86.3273, 266.4183]]])\n",
      "xyn: tensor([[[0.2628, 0.3766],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2553, 0.3692],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2340, 0.3763],\n",
      "         [0.1927, 0.4256],\n",
      "         [0.2513, 0.4085],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3183, 0.3847],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3719, 0.3522],\n",
      "         [0.1993, 0.5440],\n",
      "         [0.2359, 0.5420],\n",
      "         [0.2164, 0.6372],\n",
      "         [0.2554, 0.6422],\n",
      "         [0.2129, 0.7415],\n",
      "         [0.2398, 0.7401]]])\n",
      "\n",
      "0: 640x640 1 person, 116.2ms\n",
      "Speed: 1.3ms preprocess, 116.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 74,  89,  33],\n",
      "        [ 73,  88,  32],\n",
      "        [ 74,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  84,  28],\n",
      "        [ 67,  82,  26],\n",
      "        [ 67,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 63,  79,  23],\n",
      "        [ 61,  76,  20]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2509822845458984, 'inference': 116.18494987487793, 'postprocess': 0.396728515625}]\n",
      "Bounding Box: tensor([[105.0000, 199.5000,  92.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6864, 0.1307, 0.7714, 0.1383, 0.9558, 0.9368, 0.9987, 0.4854, 0.9965, 0.3831, 0.9798, 0.9968, 0.9996, 0.9939, 0.9992, 0.9815, 0.9950]])\n",
      "data: tensor([[[9.2537e+01, 1.3387e+02, 6.8635e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3067e-01],\n",
      "         [8.9792e+01, 1.3141e+02, 7.7143e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3834e-01],\n",
      "         [8.2745e+01, 1.3437e+02, 9.5578e-01],\n",
      "         [6.8945e+01, 1.5376e+02, 9.3677e-01],\n",
      "         [8.8271e+01, 1.4631e+02, 9.9866e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8537e-01],\n",
      "         [1.1367e+02, 1.3668e+02, 9.9651e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8308e-01],\n",
      "         [1.3336e+02, 1.2409e+02, 9.7981e-01],\n",
      "         [7.0722e+01, 1.9647e+02, 9.9684e-01],\n",
      "         [8.2828e+01, 1.9550e+02, 9.9959e-01],\n",
      "         [7.7247e+01, 2.3041e+02, 9.9393e-01],\n",
      "         [9.0649e+01, 2.3307e+02, 9.9918e-01],\n",
      "         [6.9060e+01, 2.6727e+02, 9.8149e-01],\n",
      "         [8.7577e+01, 2.6899e+02, 9.9501e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 92.5368, 133.8692],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 89.7919, 131.4068],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 82.7451, 134.3710],\n",
      "         [ 68.9452, 153.7554],\n",
      "         [ 88.2706, 146.3094],\n",
      "         [  0.0000,   0.0000],\n",
      "         [113.6719, 136.6836],\n",
      "         [  0.0000,   0.0000],\n",
      "         [133.3594, 124.0880],\n",
      "         [ 70.7223, 196.4658],\n",
      "         [ 82.8285, 195.5003],\n",
      "         [ 77.2470, 230.4057],\n",
      "         [ 90.6493, 233.0663],\n",
      "         [ 69.0598, 267.2710],\n",
      "         [ 87.5773, 268.9908]]])\n",
      "xyn: tensor([[[0.2570, 0.3719],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2494, 0.3650],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2298, 0.3733],\n",
      "         [0.1915, 0.4271],\n",
      "         [0.2452, 0.4064],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3158, 0.3797],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3704, 0.3447],\n",
      "         [0.1965, 0.5457],\n",
      "         [0.2301, 0.5431],\n",
      "         [0.2146, 0.6400],\n",
      "         [0.2518, 0.6474],\n",
      "         [0.1918, 0.7424],\n",
      "         [0.2433, 0.7472]]])\n",
      "\n",
      "0: 640x640 1 person, 113.4ms\n",
      "Speed: 1.3ms preprocess, 113.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       [[173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        [173, 192, 219],\n",
      "        ...,\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198],\n",
      "        [131, 165, 198]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  32],\n",
      "        [ 75,  90,  32],\n",
      "        [ 75,  90,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  27],\n",
      "        [ 70,  87,  28],\n",
      "        [ 69,  86,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 69,  86,  27],\n",
      "        [ 69,  86,  27],\n",
      "        [ 69,  86,  27]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2691020965576172, 'inference': 113.35611343383789, 'postprocess': 0.6511211395263672}]\n",
      "Bounding Box: tensor([[102.5000, 198.5000,  93.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7080, 0.1201, 0.7878, 0.1161, 0.9485, 0.9336, 0.9983, 0.4697, 0.9962, 0.3733, 0.9802, 0.9947, 0.9993, 0.9919, 0.9989, 0.9748, 0.9933]])\n",
      "data: tensor([[[9.0256e+01, 1.3284e+02, 7.0802e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2007e-01],\n",
      "         [8.7853e+01, 1.3041e+02, 7.8775e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1609e-01],\n",
      "         [8.0003e+01, 1.3342e+02, 9.4854e-01],\n",
      "         [6.5807e+01, 1.5323e+02, 9.3357e-01],\n",
      "         [8.5212e+01, 1.4535e+02, 9.9830e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6973e-01],\n",
      "         [1.1139e+02, 1.3535e+02, 9.9616e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7330e-01],\n",
      "         [1.3112e+02, 1.2429e+02, 9.8019e-01],\n",
      "         [6.9026e+01, 1.9605e+02, 9.9474e-01],\n",
      "         [8.0661e+01, 1.9499e+02, 9.9929e-01],\n",
      "         [7.4367e+01, 2.2988e+02, 9.9193e-01],\n",
      "         [8.9083e+01, 2.3204e+02, 9.9890e-01],\n",
      "         [6.5308e+01, 2.6581e+02, 9.7475e-01],\n",
      "         [8.6798e+01, 2.6721e+02, 9.9327e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 90.2564, 132.8379],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 87.8531, 130.4062],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 80.0031, 133.4221],\n",
      "         [ 65.8072, 153.2342],\n",
      "         [ 85.2118, 145.3521],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.3891, 135.3463],\n",
      "         [  0.0000,   0.0000],\n",
      "         [131.1201, 124.2886],\n",
      "         [ 69.0261, 196.0484],\n",
      "         [ 80.6605, 194.9915],\n",
      "         [ 74.3674, 229.8767],\n",
      "         [ 89.0826, 232.0421],\n",
      "         [ 65.3083, 265.8069],\n",
      "         [ 86.7983, 267.2137]]])\n",
      "xyn: tensor([[[0.2507, 0.3690],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2440, 0.3622],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2222, 0.3706],\n",
      "         [0.1828, 0.4257],\n",
      "         [0.2367, 0.4038],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3094, 0.3760],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3642, 0.3452],\n",
      "         [0.1917, 0.5446],\n",
      "         [0.2241, 0.5416],\n",
      "         [0.2066, 0.6385],\n",
      "         [0.2475, 0.6446],\n",
      "         [0.1814, 0.7384],\n",
      "         [0.2411, 0.7423]]])\n",
      "\n",
      "0: 640x640 1 person, 112.2ms\n",
      "Speed: 1.2ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  32],\n",
      "        [ 75,  90,  32],\n",
      "        [ 75,  90,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  27],\n",
      "        [ 70,  87,  28],\n",
      "        [ 69,  86,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 69,  86,  27],\n",
      "        [ 69,  86,  27],\n",
      "        [ 69,  86,  27]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2469291687011719, 'inference': 112.19978332519531, 'postprocess': 0.4048347473144531}]\n",
      "Bounding Box: tensor([[ 99.5000, 197.5000,  91.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6894, 0.1334, 0.6843, 0.1887, 0.9324, 0.9761, 0.9983, 0.7733, 0.9957, 0.6469, 0.9802, 0.9977, 0.9995, 0.9954, 0.9989, 0.9803, 0.9921]])\n",
      "data: tensor([[[8.8050e+01, 1.3201e+02, 6.8938e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3335e-01],\n",
      "         [8.5771e+01, 1.2974e+02, 6.8428e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8869e-01],\n",
      "         [7.9250e+01, 1.3284e+02, 9.3237e-01],\n",
      "         [6.4655e+01, 1.5278e+02, 9.7612e-01],\n",
      "         [8.5202e+01, 1.4477e+02, 9.9829e-01],\n",
      "         [5.8157e+01, 1.7686e+02, 7.7330e-01],\n",
      "         [1.0915e+02, 1.3544e+02, 9.9567e-01],\n",
      "         [7.0142e+01, 1.8803e+02, 6.4689e-01],\n",
      "         [1.2793e+02, 1.2352e+02, 9.8023e-01],\n",
      "         [6.7493e+01, 1.9591e+02, 9.9768e-01],\n",
      "         [7.9807e+01, 1.9485e+02, 9.9946e-01],\n",
      "         [7.1746e+01, 2.2807e+02, 9.9538e-01],\n",
      "         [8.7844e+01, 2.3122e+02, 9.9889e-01],\n",
      "         [6.2566e+01, 2.6406e+02, 9.8026e-01],\n",
      "         [8.5880e+01, 2.6742e+02, 9.9210e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 88.0501, 132.0064],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 85.7706, 129.7399],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.2500, 132.8442],\n",
      "         [ 64.6551, 152.7816],\n",
      "         [ 85.2025, 144.7673],\n",
      "         [ 58.1574, 176.8634],\n",
      "         [109.1483, 135.4360],\n",
      "         [ 70.1419, 188.0335],\n",
      "         [127.9298, 123.5175],\n",
      "         [ 67.4933, 195.9051],\n",
      "         [ 79.8068, 194.8528],\n",
      "         [ 71.7461, 228.0671],\n",
      "         [ 87.8437, 231.2165],\n",
      "         [ 62.5656, 264.0604],\n",
      "         [ 85.8804, 267.4187]]])\n",
      "xyn: tensor([[[0.2446, 0.3667],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2383, 0.3604],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2201, 0.3690],\n",
      "         [0.1796, 0.4244],\n",
      "         [0.2367, 0.4021],\n",
      "         [0.1615, 0.4913],\n",
      "         [0.3032, 0.3762],\n",
      "         [0.1948, 0.5223],\n",
      "         [0.3554, 0.3431],\n",
      "         [0.1875, 0.5442],\n",
      "         [0.2217, 0.5413],\n",
      "         [0.1993, 0.6335],\n",
      "         [0.2440, 0.6423],\n",
      "         [0.1738, 0.7335],\n",
      "         [0.2386, 0.7428]]])\n",
      "\n",
      "0: 640x640 1 person, 114.8ms\n",
      "Speed: 1.3ms preprocess, 114.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 193, 222],\n",
      "        [172, 193, 222],\n",
      "        [172, 193, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  93,  34],\n",
      "        [ 74,  90,  32],\n",
      "        [ 74,  90,  32]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 74,  94,  34],\n",
      "        [ 75,  95,  35],\n",
      "        [ 63,  83,  24]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  81,  22],\n",
      "        [ 73,  93,  33],\n",
      "        [ 75,  95,  35]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3360977172851562, 'inference': 114.75610733032227, 'postprocess': 0.6668567657470703}]\n",
      "Bounding Box: tensor([[ 96., 196.,  96., 166.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6066, 0.0956, 0.6424, 0.1728, 0.9503, 0.9726, 0.9989, 0.7178, 0.9970, 0.5668, 0.9820, 0.9984, 0.9997, 0.9971, 0.9995, 0.9893, 0.9964]])\n",
      "data: tensor([[[8.7216e+01, 1.3212e+02, 6.0657e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.5595e-02],\n",
      "         [8.4182e+01, 1.2945e+02, 6.4238e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7282e-01],\n",
      "         [7.6916e+01, 1.3287e+02, 9.5026e-01],\n",
      "         [6.3190e+01, 1.5318e+02, 9.7256e-01],\n",
      "         [8.2669e+01, 1.4467e+02, 9.9892e-01],\n",
      "         [5.7444e+01, 1.7551e+02, 7.1777e-01],\n",
      "         [1.0707e+02, 1.3434e+02, 9.9704e-01],\n",
      "         [7.1413e+01, 1.8076e+02, 5.6676e-01],\n",
      "         [1.2624e+02, 1.2074e+02, 9.8201e-01],\n",
      "         [6.4443e+01, 1.9432e+02, 9.9841e-01],\n",
      "         [7.6865e+01, 1.9316e+02, 9.9971e-01],\n",
      "         [6.8675e+01, 2.2834e+02, 9.9709e-01],\n",
      "         [8.5180e+01, 2.3038e+02, 9.9945e-01],\n",
      "         [5.7597e+01, 2.6328e+02, 9.8934e-01],\n",
      "         [8.3709e+01, 2.6905e+02, 9.9636e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 87.2165, 132.1231],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 84.1820, 129.4533],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 76.9162, 132.8717],\n",
      "         [ 63.1899, 153.1761],\n",
      "         [ 82.6689, 144.6709],\n",
      "         [ 57.4437, 175.5143],\n",
      "         [107.0725, 134.3361],\n",
      "         [ 71.4132, 180.7642],\n",
      "         [126.2388, 120.7375],\n",
      "         [ 64.4431, 194.3240],\n",
      "         [ 76.8649, 193.1626],\n",
      "         [ 68.6747, 228.3422],\n",
      "         [ 85.1802, 230.3797],\n",
      "         [ 57.5969, 263.2774],\n",
      "         [ 83.7090, 269.0450]]])\n",
      "xyn: tensor([[[0.2423, 0.3670],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2338, 0.3596],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2137, 0.3691],\n",
      "         [0.1755, 0.4255],\n",
      "         [0.2296, 0.4019],\n",
      "         [0.1596, 0.4875],\n",
      "         [0.2974, 0.3732],\n",
      "         [0.1984, 0.5021],\n",
      "         [0.3507, 0.3354],\n",
      "         [0.1790, 0.5398],\n",
      "         [0.2135, 0.5366],\n",
      "         [0.1908, 0.6343],\n",
      "         [0.2366, 0.6399],\n",
      "         [0.1600, 0.7313],\n",
      "         [0.2325, 0.7473]]])\n",
      "\n",
      "0: 640x640 1 person, 128.9ms\n",
      "Speed: 1.2ms preprocess, 128.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  90,  34],\n",
      "        [ 75,  90,  34],\n",
      "        [ 74,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  84,  28],\n",
      "        [ 68,  83,  27],\n",
      "        [ 66,  81,  25]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 66,  81,  25],\n",
      "        [ 62,  77,  22],\n",
      "        [ 60,  75,  19]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2269020080566406, 'inference': 128.9350986480713, 'postprocess': 0.5409717559814453}]\n",
      "Bounding Box: tensor([[ 93., 196.,  98., 168.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.6942, 0.1079, 0.7668, 0.1180, 0.9562, 0.9525, 0.9992, 0.5511, 0.9981, 0.4706, 0.9896, 0.9981, 0.9998, 0.9964, 0.9996, 0.9874, 0.9969]])\n",
      "data: tensor([[[8.4984e+01, 1.3198e+02, 6.9416e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0790e-01],\n",
      "         [8.2125e+01, 1.2953e+02, 7.6683e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1804e-01],\n",
      "         [7.4689e+01, 1.3273e+02, 9.5623e-01],\n",
      "         [6.1270e+01, 1.5319e+02, 9.5248e-01],\n",
      "         [8.0417e+01, 1.4503e+02, 9.9916e-01],\n",
      "         [5.9031e+01, 1.7252e+02, 5.5113e-01],\n",
      "         [1.0363e+02, 1.3437e+02, 9.9813e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.7059e-01],\n",
      "         [1.2402e+02, 1.1919e+02, 9.8965e-01],\n",
      "         [6.2667e+01, 1.9406e+02, 9.9810e-01],\n",
      "         [7.5668e+01, 1.9297e+02, 9.9978e-01],\n",
      "         [6.5429e+01, 2.2794e+02, 9.9643e-01],\n",
      "         [8.5289e+01, 2.3054e+02, 9.9958e-01],\n",
      "         [5.3894e+01, 2.6310e+02, 9.8739e-01],\n",
      "         [8.3531e+01, 2.6932e+02, 9.9690e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 84.9838, 131.9771],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 82.1248, 129.5268],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.6886, 132.7258],\n",
      "         [ 61.2696, 153.1926],\n",
      "         [ 80.4171, 145.0286],\n",
      "         [ 59.0309, 172.5175],\n",
      "         [103.6337, 134.3665],\n",
      "         [  0.0000,   0.0000],\n",
      "         [124.0211, 119.1874],\n",
      "         [ 62.6674, 194.0564],\n",
      "         [ 75.6682, 192.9679],\n",
      "         [ 65.4285, 227.9372],\n",
      "         [ 85.2886, 230.5369],\n",
      "         [ 53.8940, 263.0981],\n",
      "         [ 83.5315, 269.3173]]])\n",
      "xyn: tensor([[[0.2361, 0.3666],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2281, 0.3598],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2075, 0.3687],\n",
      "         [0.1702, 0.4255],\n",
      "         [0.2234, 0.4029],\n",
      "         [0.1640, 0.4792],\n",
      "         [0.2879, 0.3732],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3445, 0.3311],\n",
      "         [0.1741, 0.5390],\n",
      "         [0.2102, 0.5360],\n",
      "         [0.1817, 0.6332],\n",
      "         [0.2369, 0.6404],\n",
      "         [0.1497, 0.7308],\n",
      "         [0.2320, 0.7481]]])\n",
      "\n",
      "0: 640x640 1 person, 112.8ms\n",
      "Speed: 1.2ms preprocess, 112.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 193, 222],\n",
      "        [172, 193, 222],\n",
      "        [172, 193, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 73,  90,  30],\n",
      "        [ 77,  95,  34],\n",
      "        [ 72,  89,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 68,  89,  27],\n",
      "        [ 72,  93,  31],\n",
      "        [ 62,  83,  22]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  97,  35],\n",
      "        [ 74,  95,  33],\n",
      "        [ 69,  90,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.233816146850586, 'inference': 112.78223991394043, 'postprocess': 0.45108795166015625}]\n",
      "Bounding Box: tensor([[ 92.5000, 196.0000,  97.0000, 168.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8046, 0.1634, 0.8409, 0.1074, 0.9506, 0.9579, 0.9992, 0.6584, 0.9987, 0.6447, 0.9946, 0.9983, 0.9998, 0.9968, 0.9996, 0.9875, 0.9968]])\n",
      "data: tensor([[[8.1701e+01, 1.3249e+02, 8.0463e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6343e-01],\n",
      "         [7.8824e+01, 1.3020e+02, 8.4095e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0743e-01],\n",
      "         [7.0803e+01, 1.3290e+02, 9.5061e-01],\n",
      "         [5.8619e+01, 1.5313e+02, 9.5789e-01],\n",
      "         [7.7096e+01, 1.4455e+02, 9.9918e-01],\n",
      "         [5.3117e+01, 1.7635e+02, 6.5839e-01],\n",
      "         [9.9401e+01, 1.3498e+02, 9.9865e-01],\n",
      "         [6.6986e+01, 1.8282e+02, 6.4471e-01],\n",
      "         [1.1992e+02, 1.2067e+02, 9.9457e-01],\n",
      "         [6.1688e+01, 1.9400e+02, 9.9829e-01],\n",
      "         [7.4150e+01, 1.9279e+02, 9.9979e-01],\n",
      "         [6.2007e+01, 2.2694e+02, 9.9681e-01],\n",
      "         [8.3869e+01, 2.2984e+02, 9.9960e-01],\n",
      "         [5.3567e+01, 2.6311e+02, 9.8752e-01],\n",
      "         [8.3562e+01, 2.6853e+02, 9.9680e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 81.7013, 132.4905],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.8236, 130.2029],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 70.8033, 132.9049],\n",
      "         [ 58.6186, 153.1314],\n",
      "         [ 77.0961, 144.5488],\n",
      "         [ 53.1174, 176.3525],\n",
      "         [ 99.4006, 134.9760],\n",
      "         [ 66.9861, 182.8228],\n",
      "         [119.9161, 120.6739],\n",
      "         [ 61.6883, 194.0001],\n",
      "         [ 74.1499, 192.7872],\n",
      "         [ 62.0070, 226.9388],\n",
      "         [ 83.8694, 229.8398],\n",
      "         [ 53.5667, 263.1107],\n",
      "         [ 83.5618, 268.5301]]])\n",
      "xyn: tensor([[[0.2269, 0.3680],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2190, 0.3617],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1967, 0.3692],\n",
      "         [0.1628, 0.4254],\n",
      "         [0.2142, 0.4015],\n",
      "         [0.1475, 0.4899],\n",
      "         [0.2761, 0.3749],\n",
      "         [0.1861, 0.5078],\n",
      "         [0.3331, 0.3352],\n",
      "         [0.1714, 0.5389],\n",
      "         [0.2060, 0.5355],\n",
      "         [0.1722, 0.6304],\n",
      "         [0.2330, 0.6384],\n",
      "         [0.1488, 0.7309],\n",
      "         [0.2321, 0.7459]]])\n",
      "\n",
      "0: 640x640 1 person, 113.1ms\n",
      "Speed: 1.3ms preprocess, 113.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        [171, 194, 222],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 73,  90,  30],\n",
      "        [ 77,  95,  34],\n",
      "        [ 72,  89,  28]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 68,  89,  27],\n",
      "        [ 72,  93,  31],\n",
      "        [ 62,  83,  22]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  97,  35],\n",
      "        [ 74,  95,  33],\n",
      "        [ 69,  90,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3089179992675781, 'inference': 113.07597160339355, 'postprocess': 0.41294097900390625}]\n",
      "Bounding Box: tensor([[ 91.0000, 195.5000,  96.0000, 169.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8492, 0.2396, 0.9041, 0.0992, 0.9581, 0.9086, 0.9987, 0.4375, 0.9977, 0.4605, 0.9906, 0.9958, 0.9996, 0.9943, 0.9994, 0.9843, 0.9963]])\n",
      "data: tensor([[[8.0014e+01, 1.3271e+02, 8.4921e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3955e-01],\n",
      "         [7.7013e+01, 1.3041e+02, 9.0411e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.9222e-02],\n",
      "         [6.9292e+01, 1.3295e+02, 9.5809e-01],\n",
      "         [5.6793e+01, 1.5229e+02, 9.0862e-01],\n",
      "         [7.4357e+01, 1.4446e+02, 9.9874e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3747e-01],\n",
      "         [9.6470e+01, 1.3548e+02, 9.9772e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6045e-01],\n",
      "         [1.1682e+02, 1.2041e+02, 9.9058e-01],\n",
      "         [5.6983e+01, 1.9419e+02, 9.9583e-01],\n",
      "         [6.9335e+01, 1.9301e+02, 9.9957e-01],\n",
      "         [5.9422e+01, 2.2717e+02, 9.9428e-01],\n",
      "         [8.2121e+01, 2.2973e+02, 9.9939e-01],\n",
      "         [5.2380e+01, 2.6385e+02, 9.8430e-01],\n",
      "         [8.2514e+01, 2.6870e+02, 9.9634e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 80.0136, 132.7115],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 77.0128, 130.4081],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.2920, 132.9467],\n",
      "         [ 56.7928, 152.2903],\n",
      "         [ 74.3573, 144.4622],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 96.4700, 135.4819],\n",
      "         [  0.0000,   0.0000],\n",
      "         [116.8222, 120.4137],\n",
      "         [ 56.9833, 194.1947],\n",
      "         [ 69.3347, 193.0136],\n",
      "         [ 59.4224, 227.1739],\n",
      "         [ 82.1211, 229.7272],\n",
      "         [ 52.3803, 263.8459],\n",
      "         [ 82.5138, 268.7039]]])\n",
      "xyn: tensor([[[0.2223, 0.3686],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2139, 0.3622],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1925, 0.3693],\n",
      "         [0.1578, 0.4230],\n",
      "         [0.2065, 0.4013],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2680, 0.3763],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3245, 0.3345],\n",
      "         [0.1583, 0.5394],\n",
      "         [0.1926, 0.5361],\n",
      "         [0.1651, 0.6310],\n",
      "         [0.2281, 0.6381],\n",
      "         [0.1455, 0.7329],\n",
      "         [0.2292, 0.7464]]])\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.3ms preprocess, 116.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 216],\n",
      "        [172, 194, 216],\n",
      "        [172, 194, 218],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  93,  32],\n",
      "        [ 83, 101,  40],\n",
      "        [ 49,  67,   6]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  84,  23],\n",
      "        [ 76,  97,  35],\n",
      "        [ 41,  62,   1]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 72,  93,  31],\n",
      "        [ 77,  98,  37],\n",
      "        [ 45,  66,   4]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2919902801513672, 'inference': 116.40000343322754, 'postprocess': 0.3917217254638672}]\n",
      "Bounding Box: tensor([[ 90.5000, 195.5000,  97.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8014, 0.1841, 0.8608, 0.1043, 0.9585, 0.9366, 0.9989, 0.5232, 0.9976, 0.4973, 0.9892, 0.9971, 0.9997, 0.9952, 0.9994, 0.9846, 0.9961]])\n",
      "data: tensor([[[7.7769e+01, 1.3280e+02, 8.0141e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8409e-01],\n",
      "         [7.4791e+01, 1.3028e+02, 8.6084e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0430e-01],\n",
      "         [6.7565e+01, 1.3307e+02, 9.5847e-01],\n",
      "         [5.5275e+01, 1.5311e+02, 9.3656e-01],\n",
      "         [7.2597e+01, 1.4455e+02, 9.9886e-01],\n",
      "         [6.1367e+01, 1.6713e+02, 5.2322e-01],\n",
      "         [9.5827e+01, 1.3435e+02, 9.9765e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.9733e-01],\n",
      "         [1.1535e+02, 1.1963e+02, 9.8917e-01],\n",
      "         [5.4254e+01, 1.9486e+02, 9.9708e-01],\n",
      "         [6.7141e+01, 1.9329e+02, 9.9965e-01],\n",
      "         [5.7876e+01, 2.2807e+02, 9.9519e-01],\n",
      "         [8.1828e+01, 2.2971e+02, 9.9942e-01],\n",
      "         [5.1228e+01, 2.6361e+02, 9.8460e-01],\n",
      "         [8.3287e+01, 2.6805e+02, 9.9609e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 77.7687, 132.7982],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.7908, 130.2763],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.5645, 133.0750],\n",
      "         [ 55.2747, 153.1067],\n",
      "         [ 72.5967, 144.5541],\n",
      "         [ 61.3666, 167.1346],\n",
      "         [ 95.8271, 134.3535],\n",
      "         [  0.0000,   0.0000],\n",
      "         [115.3496, 119.6308],\n",
      "         [ 54.2544, 194.8648],\n",
      "         [ 67.1413, 193.2872],\n",
      "         [ 57.8765, 228.0666],\n",
      "         [ 81.8276, 229.7063],\n",
      "         [ 51.2280, 263.6143],\n",
      "         [ 83.2872, 268.0491]]])\n",
      "xyn: tensor([[[0.2160, 0.3689],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2078, 0.3619],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1877, 0.3697],\n",
      "         [0.1535, 0.4253],\n",
      "         [0.2017, 0.4015],\n",
      "         [0.1705, 0.4643],\n",
      "         [0.2662, 0.3732],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3204, 0.3323],\n",
      "         [0.1507, 0.5413],\n",
      "         [0.1865, 0.5369],\n",
      "         [0.1608, 0.6335],\n",
      "         [0.2273, 0.6381],\n",
      "         [0.1423, 0.7323],\n",
      "         [0.2314, 0.7446]]])\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 1.2ms preprocess, 116.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  94,  33],\n",
      "        [ 77,  95,  34],\n",
      "        [ 73,  90,  30]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 67,  88,  26],\n",
      "        [ 69,  90,  28],\n",
      "        [ 63,  84,  23]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  96,  34],\n",
      "        [ 70,  91,  30],\n",
      "        [ 69,  90,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.249074935913086, 'inference': 116.9121265411377, 'postprocess': 0.39768218994140625}]\n",
      "Bounding Box: tensor([[ 87., 196.,  92., 168.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7940, 0.1780, 0.8786, 0.0962, 0.9602, 0.8916, 0.9989, 0.3288, 0.9976, 0.3166, 0.9876, 0.9960, 0.9996, 0.9944, 0.9995, 0.9865, 0.9973]])\n",
      "data: tensor([[[7.5063e+01, 1.3238e+02, 7.9399e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7803e-01],\n",
      "         [7.2029e+01, 1.2994e+02, 8.7861e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.6183e-02],\n",
      "         [6.5006e+01, 1.3261e+02, 9.6018e-01],\n",
      "         [5.3470e+01, 1.5088e+02, 8.9161e-01],\n",
      "         [6.9934e+01, 1.4457e+02, 9.9885e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2882e-01],\n",
      "         [9.3200e+01, 1.3393e+02, 9.9761e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1664e-01],\n",
      "         [1.1383e+02, 1.1977e+02, 9.8757e-01],\n",
      "         [5.2932e+01, 1.9437e+02, 9.9598e-01],\n",
      "         [6.4916e+01, 1.9347e+02, 9.9965e-01],\n",
      "         [5.6624e+01, 2.2832e+02, 9.9444e-01],\n",
      "         [7.8741e+01, 2.2929e+02, 9.9951e-01],\n",
      "         [5.0401e+01, 2.6461e+02, 9.8652e-01],\n",
      "         [8.0425e+01, 2.6776e+02, 9.9732e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 75.0635, 132.3844],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 72.0286, 129.9403],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 65.0057, 132.6139],\n",
      "         [ 53.4702, 150.8774],\n",
      "         [ 69.9343, 144.5661],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 93.2002, 133.9290],\n",
      "         [  0.0000,   0.0000],\n",
      "         [113.8267, 119.7669],\n",
      "         [ 52.9320, 194.3654],\n",
      "         [ 64.9157, 193.4713],\n",
      "         [ 56.6235, 228.3193],\n",
      "         [ 78.7415, 229.2899],\n",
      "         [ 50.4006, 264.6056],\n",
      "         [ 80.4246, 267.7640]]])\n",
      "xyn: tensor([[[0.2085, 0.3677],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2001, 0.3609],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1806, 0.3684],\n",
      "         [0.1485, 0.4191],\n",
      "         [0.1943, 0.4016],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2589, 0.3720],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3162, 0.3327],\n",
      "         [0.1470, 0.5399],\n",
      "         [0.1803, 0.5374],\n",
      "         [0.1573, 0.6342],\n",
      "         [0.2187, 0.6369],\n",
      "         [0.1400, 0.7350],\n",
      "         [0.2234, 0.7438]]])\n",
      "\n",
      "0: 640x640 1 person, 121.0ms\n",
      "Speed: 1.2ms preprocess, 121.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 77,  95,  34],\n",
      "        [ 76,  94,  33],\n",
      "        [ 73,  90,  30]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 68,  89,  27],\n",
      "        [ 68,  89,  27],\n",
      "        [ 63,  84,  23]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 76,  97,  35],\n",
      "        [ 69,  90,  28],\n",
      "        [ 69,  90,  28]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2371540069580078, 'inference': 120.98002433776855, 'postprocess': 0.4248619079589844}]\n",
      "Bounding Box: tensor([[ 85., 196.,  92., 166.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.7474, 0.1572, 0.8437, 0.1162, 0.9579, 0.9006, 0.9988, 0.3145, 0.9971, 0.2698, 0.9828, 0.9964, 0.9997, 0.9955, 0.9996, 0.9900, 0.9980]])\n",
      "data: tensor([[[7.2760e+01, 1.3285e+02, 7.4743e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5718e-01],\n",
      "         [6.9990e+01, 1.3022e+02, 8.4366e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1622e-01],\n",
      "         [6.3170e+01, 1.3257e+02, 9.5792e-01],\n",
      "         [5.1253e+01, 1.5179e+02, 9.0059e-01],\n",
      "         [6.7237e+01, 1.4397e+02, 9.9883e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.1454e-01],\n",
      "         [9.1044e+01, 1.3499e+02, 9.9707e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6982e-01],\n",
      "         [1.1189e+02, 1.2108e+02, 9.8284e-01],\n",
      "         [5.1014e+01, 1.9317e+02, 9.9639e-01],\n",
      "         [6.2102e+01, 1.9182e+02, 9.9967e-01],\n",
      "         [5.5673e+01, 2.2689e+02, 9.9552e-01],\n",
      "         [7.4114e+01, 2.2787e+02, 9.9958e-01],\n",
      "         [4.9316e+01, 2.6352e+02, 9.8998e-01],\n",
      "         [7.7322e+01, 2.6674e+02, 9.9795e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 72.7598, 132.8534],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 69.9901, 130.2180],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.1701, 132.5722],\n",
      "         [ 51.2526, 151.7934],\n",
      "         [ 67.2372, 143.9728],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 91.0445, 134.9917],\n",
      "         [  0.0000,   0.0000],\n",
      "         [111.8907, 121.0843],\n",
      "         [ 51.0142, 193.1706],\n",
      "         [ 62.1015, 191.8172],\n",
      "         [ 55.6729, 226.8912],\n",
      "         [ 74.1137, 227.8664],\n",
      "         [ 49.3159, 263.5197],\n",
      "         [ 77.3216, 266.7449]]])\n",
      "xyn: tensor([[[0.2021, 0.3690],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1944, 0.3617],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1755, 0.3683],\n",
      "         [0.1424, 0.4216],\n",
      "         [0.1868, 0.3999],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2529, 0.3750],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3108, 0.3363],\n",
      "         [0.1417, 0.5366],\n",
      "         [0.1725, 0.5328],\n",
      "         [0.1546, 0.6303],\n",
      "         [0.2059, 0.6330],\n",
      "         [0.1370, 0.7320],\n",
      "         [0.2148, 0.7410]]])\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 1.2ms preprocess, 116.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       [[172, 194, 218],\n",
      "        [172, 194, 218],\n",
      "        [172, 193, 219],\n",
      "        ...,\n",
      "        [141, 172, 204],\n",
      "        [141, 172, 204],\n",
      "        [143, 173, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  91,  35],\n",
      "        [ 74,  90,  34],\n",
      "        [ 73,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  30],\n",
      "        [ 67,  83,  27],\n",
      "        [ 66,  82,  26]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  80,  24],\n",
      "        [ 61,  77,  22],\n",
      "        [ 58,  74,  18]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.194000244140625, 'inference': 116.92380905151367, 'postprocess': 0.39386749267578125}]\n",
      "Bounding Box: tensor([[ 83.0000, 196.5000,  92.0000, 167.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8094, 0.1684, 0.8929, 0.0795, 0.9535, 0.8740, 0.9982, 0.2869, 0.9966, 0.2694, 0.9843, 0.9919, 0.9992, 0.9918, 0.9992, 0.9806, 0.9961]])\n",
      "data: tensor([[[7.1094e+01, 1.3186e+02, 8.0945e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6843e-01],\n",
      "         [6.8462e+01, 1.2930e+02, 8.9290e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.9466e-02],\n",
      "         [6.0791e+01, 1.3249e+02, 9.5349e-01],\n",
      "         [5.0923e+01, 1.4960e+02, 8.7397e-01],\n",
      "         [6.3073e+01, 1.4554e+02, 9.9821e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8687e-01],\n",
      "         [8.8968e+01, 1.3593e+02, 9.9661e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6942e-01],\n",
      "         [1.0966e+02, 1.2181e+02, 9.8429e-01],\n",
      "         [4.8632e+01, 1.9307e+02, 9.9187e-01],\n",
      "         [5.6891e+01, 1.9338e+02, 9.9922e-01],\n",
      "         [5.4328e+01, 2.2709e+02, 9.9184e-01],\n",
      "         [7.0850e+01, 2.2984e+02, 9.9924e-01],\n",
      "         [4.8161e+01, 2.6370e+02, 9.8059e-01],\n",
      "         [7.7408e+01, 2.6848e+02, 9.9607e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 71.0940, 131.8622],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 68.4618, 129.3038],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 60.7915, 132.4938],\n",
      "         [ 50.9226, 149.5953],\n",
      "         [ 63.0731, 145.5372],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 88.9676, 135.9273],\n",
      "         [  0.0000,   0.0000],\n",
      "         [109.6618, 121.8057],\n",
      "         [ 48.6315, 193.0714],\n",
      "         [ 56.8911, 193.3798],\n",
      "         [ 54.3282, 227.0911],\n",
      "         [ 70.8497, 229.8430],\n",
      "         [ 48.1608, 263.7035],\n",
      "         [ 77.4080, 268.4757]]])\n",
      "xyn: tensor([[[0.1975, 0.3663],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1902, 0.3592],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1689, 0.3680],\n",
      "         [0.1415, 0.4155],\n",
      "         [0.1752, 0.4043],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2471, 0.3776],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3046, 0.3383],\n",
      "         [0.1351, 0.5363],\n",
      "         [0.1580, 0.5372],\n",
      "         [0.1509, 0.6308],\n",
      "         [0.1968, 0.6385],\n",
      "         [0.1338, 0.7325],\n",
      "         [0.2150, 0.7458]]])\n",
      "\n",
      "0: 640x640 1 person, 119.3ms\n",
      "Speed: 1.3ms preprocess, 119.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  91,  35],\n",
      "        [ 74,  90,  34],\n",
      "        [ 73,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  30],\n",
      "        [ 67,  83,  27],\n",
      "        [ 66,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 63,  80,  24],\n",
      "        [ 61,  77,  22],\n",
      "        [ 58,  74,  18]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2617111206054688, 'inference': 119.28009986877441, 'postprocess': 0.4019737243652344}]\n",
      "Bounding Box: tensor([[ 80.0000, 196.5000,  90.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8176, 0.2006, 0.9086, 0.0890, 0.9617, 0.8416, 0.9982, 0.2188, 0.9962, 0.2150, 0.9805, 0.9915, 0.9993, 0.9927, 0.9994, 0.9872, 0.9975]])\n",
      "data: tensor([[[7.0763e+01, 1.3238e+02, 8.1756e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0065e-01],\n",
      "         [6.7411e+01, 1.2939e+02, 9.0863e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.9008e-02],\n",
      "         [5.8310e+01, 1.3203e+02, 9.6166e-01],\n",
      "         [4.8373e+01, 1.5046e+02, 8.4158e-01],\n",
      "         [6.1190e+01, 1.4481e+02, 9.9822e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1883e-01],\n",
      "         [8.9532e+01, 1.3702e+02, 9.9624e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1498e-01],\n",
      "         [1.1009e+02, 1.2253e+02, 9.8046e-01],\n",
      "         [4.6091e+01, 1.9298e+02, 9.9149e-01],\n",
      "         [5.4426e+01, 1.9261e+02, 9.9927e-01],\n",
      "         [5.2779e+01, 2.2786e+02, 9.9275e-01],\n",
      "         [6.5983e+01, 2.2939e+02, 9.9938e-01],\n",
      "         [4.8104e+01, 2.6382e+02, 9.8720e-01],\n",
      "         [7.2284e+01, 2.6649e+02, 9.9752e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 70.7635, 132.3784],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 67.4115, 129.3925],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 58.3099, 132.0310],\n",
      "         [ 48.3730, 150.4636],\n",
      "         [ 61.1897, 144.8122],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 89.5322, 137.0205],\n",
      "         [  0.0000,   0.0000],\n",
      "         [110.0949, 122.5293],\n",
      "         [ 46.0912, 192.9837],\n",
      "         [ 54.4263, 192.6060],\n",
      "         [ 52.7792, 227.8557],\n",
      "         [ 65.9827, 229.3875],\n",
      "         [ 48.1035, 263.8188],\n",
      "         [ 72.2839, 266.4917]]])\n",
      "xyn: tensor([[[0.1966, 0.3677],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1873, 0.3594],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1620, 0.3668],\n",
      "         [0.1344, 0.4180],\n",
      "         [0.1700, 0.4023],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2487, 0.3806],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3058, 0.3404],\n",
      "         [0.1280, 0.5361],\n",
      "         [0.1512, 0.5350],\n",
      "         [0.1466, 0.6329],\n",
      "         [0.1833, 0.6372],\n",
      "         [0.1336, 0.7328],\n",
      "         [0.2008, 0.7403]]])\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.2ms preprocess, 114.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  91,  35],\n",
      "        [ 74,  90,  34],\n",
      "        [ 73,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  30],\n",
      "        [ 67,  83,  27],\n",
      "        [ 66,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 63,  80,  24],\n",
      "        [ 61,  77,  22],\n",
      "        [ 58,  74,  18]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2269020080566406, 'inference': 114.45975303649902, 'postprocess': 0.3859996795654297}]\n",
      "Bounding Box: tensor([[ 78.0000, 197.5000,  90.0000, 165.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8305, 0.2128, 0.8858, 0.1145, 0.9550, 0.9202, 0.9986, 0.4593, 0.9973, 0.4520, 0.9887, 0.9961, 0.9996, 0.9946, 0.9994, 0.9850, 0.9963]])\n",
      "data: tensor([[[6.7793e+01, 1.3245e+02, 8.3046e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1280e-01],\n",
      "         [6.4883e+01, 1.2972e+02, 8.8581e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1448e-01],\n",
      "         [5.6821e+01, 1.3215e+02, 9.5500e-01],\n",
      "         [4.4812e+01, 1.5112e+02, 9.2015e-01],\n",
      "         [6.1003e+01, 1.4458e+02, 9.9857e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5930e-01],\n",
      "         [8.8731e+01, 1.3840e+02, 9.9733e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5195e-01],\n",
      "         [1.0799e+02, 1.2291e+02, 9.8866e-01],\n",
      "         [4.3734e+01, 1.9335e+02, 9.9612e-01],\n",
      "         [5.3540e+01, 1.9269e+02, 9.9955e-01],\n",
      "         [5.1382e+01, 2.2770e+02, 9.9464e-01],\n",
      "         [6.3811e+01, 2.3049e+02, 9.9937e-01],\n",
      "         [4.7440e+01, 2.6475e+02, 9.8503e-01],\n",
      "         [7.0115e+01, 2.6868e+02, 9.9635e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 67.7928, 132.4514],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 64.8830, 129.7230],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 56.8214, 132.1503],\n",
      "         [ 44.8124, 151.1170],\n",
      "         [ 61.0034, 144.5814],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 88.7310, 138.3987],\n",
      "         [  0.0000,   0.0000],\n",
      "         [107.9893, 122.9057],\n",
      "         [ 43.7343, 193.3500],\n",
      "         [ 53.5396, 192.6915],\n",
      "         [ 51.3819, 227.6976],\n",
      "         [ 63.8114, 230.4886],\n",
      "         [ 47.4398, 264.7535],\n",
      "         [ 70.1150, 268.6811]]])\n",
      "xyn: tensor([[[0.1883, 0.3679],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1802, 0.3603],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1578, 0.3671],\n",
      "         [0.1245, 0.4198],\n",
      "         [0.1695, 0.4016],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2465, 0.3844],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3000, 0.3414],\n",
      "         [0.1215, 0.5371],\n",
      "         [0.1487, 0.5353],\n",
      "         [0.1427, 0.6325],\n",
      "         [0.1773, 0.6402],\n",
      "         [0.1318, 0.7354],\n",
      "         [0.1948, 0.7463]]])\n",
      "\n",
      "0: 640x640 1 person, 135.3ms\n",
      "Speed: 1.2ms preprocess, 135.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  91,  35],\n",
      "        [ 74,  90,  34],\n",
      "        [ 73,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  30],\n",
      "        [ 67,  83,  27],\n",
      "        [ 66,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 63,  80,  24],\n",
      "        [ 61,  77,  22],\n",
      "        [ 58,  74,  18]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.238107681274414, 'inference': 135.2522373199463, 'postprocess': 0.40602684020996094}]\n",
      "Bounding Box: tensor([[ 72.0000, 198.5000,  80.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8776, 0.2921, 0.9270, 0.1069, 0.9544, 0.8877, 0.9977, 0.3805, 0.9959, 0.3982, 0.9846, 0.9918, 0.9990, 0.9928, 0.9991, 0.9845, 0.9961]])\n",
      "data: tensor([[[6.6545e+01, 1.3277e+02, 8.7757e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9212e-01],\n",
      "         [6.3242e+01, 1.2983e+02, 9.2697e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0686e-01],\n",
      "         [5.4318e+01, 1.3215e+02, 9.5439e-01],\n",
      "         [4.3128e+01, 1.5027e+02, 8.8771e-01],\n",
      "         [5.8814e+01, 1.4484e+02, 9.9765e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.8050e-01],\n",
      "         [8.6574e+01, 1.4102e+02, 9.9587e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9820e-01],\n",
      "         [1.0489e+02, 1.2702e+02, 9.8459e-01],\n",
      "         [4.0409e+01, 1.9268e+02, 9.9176e-01],\n",
      "         [4.9636e+01, 1.9240e+02, 9.9899e-01],\n",
      "         [5.0442e+01, 2.2710e+02, 9.9275e-01],\n",
      "         [6.2931e+01, 2.2926e+02, 9.9912e-01],\n",
      "         [4.6470e+01, 2.6337e+02, 9.8452e-01],\n",
      "         [6.4422e+01, 2.6746e+02, 9.9613e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 66.5448, 132.7711],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 63.2422, 129.8259],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 54.3178, 132.1474],\n",
      "         [ 43.1277, 150.2724],\n",
      "         [ 58.8140, 144.8369],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 86.5737, 141.0216],\n",
      "         [  0.0000,   0.0000],\n",
      "         [104.8877, 127.0239],\n",
      "         [ 40.4090, 192.6773],\n",
      "         [ 49.6363, 192.4048],\n",
      "         [ 50.4420, 227.1041],\n",
      "         [ 62.9310, 229.2571],\n",
      "         [ 46.4704, 263.3658],\n",
      "         [ 64.4216, 267.4564]]])\n",
      "xyn: tensor([[[0.1848, 0.3688],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1757, 0.3606],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1509, 0.3671],\n",
      "         [0.1198, 0.4174],\n",
      "         [0.1634, 0.4023],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2405, 0.3917],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2914, 0.3528],\n",
      "         [0.1122, 0.5352],\n",
      "         [0.1379, 0.5345],\n",
      "         [0.1401, 0.6308],\n",
      "         [0.1748, 0.6368],\n",
      "         [0.1291, 0.7316],\n",
      "         [0.1789, 0.7429]]])\n",
      "\n",
      "0: 640x640 1 person, 119.3ms\n",
      "Speed: 1.3ms preprocess, 119.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 75,  91,  35],\n",
      "        [ 74,  90,  34],\n",
      "        [ 73,  89,  33]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 69,  86,  30],\n",
      "        [ 67,  83,  27],\n",
      "        [ 66,  82,  26]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 63,  80,  24],\n",
      "        [ 61,  77,  22],\n",
      "        [ 58,  74,  18]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3172626495361328, 'inference': 119.26698684692383, 'postprocess': 0.43201446533203125}]\n",
      "Bounding Box: tensor([[ 68.5000, 200.0000,  75.0000, 164.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8995, 0.2730, 0.9234, 0.0907, 0.9541, 0.9480, 0.9988, 0.6861, 0.9986, 0.7282, 0.9957, 0.9969, 0.9996, 0.9945, 0.9993, 0.9759, 0.9934]])\n",
      "data: tensor([[[6.3910e+01, 1.3243e+02, 8.9951e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7299e-01],\n",
      "         [6.1065e+01, 1.2973e+02, 9.2336e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.0681e-02],\n",
      "         [5.3561e+01, 1.3183e+02, 9.5408e-01],\n",
      "         [4.2269e+01, 1.4948e+02, 9.4804e-01],\n",
      "         [5.8472e+01, 1.4490e+02, 9.9879e-01],\n",
      "         [4.7723e+01, 1.6756e+02, 6.8606e-01],\n",
      "         [8.2507e+01, 1.4688e+02, 9.9857e-01],\n",
      "         [7.8937e+01, 1.6089e+02, 7.2819e-01],\n",
      "         [9.8000e+01, 1.3064e+02, 9.9567e-01],\n",
      "         [3.9223e+01, 1.9236e+02, 9.9693e-01],\n",
      "         [4.9562e+01, 1.9203e+02, 9.9957e-01],\n",
      "         [4.9388e+01, 2.2738e+02, 9.9453e-01],\n",
      "         [6.2091e+01, 2.2893e+02, 9.9926e-01],\n",
      "         [4.5791e+01, 2.6435e+02, 9.7587e-01],\n",
      "         [6.0471e+01, 2.7030e+02, 9.9338e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 63.9101, 132.4322],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 61.0652, 129.7289],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 53.5607, 131.8328],\n",
      "         [ 42.2689, 149.4751],\n",
      "         [ 58.4715, 144.8953],\n",
      "         [ 47.7234, 167.5601],\n",
      "         [ 82.5068, 146.8844],\n",
      "         [ 78.9371, 160.8908],\n",
      "         [ 98.0000, 130.6445],\n",
      "         [ 39.2226, 192.3601],\n",
      "         [ 49.5624, 192.0283],\n",
      "         [ 49.3875, 227.3825],\n",
      "         [ 62.0906, 228.9327],\n",
      "         [ 45.7908, 264.3461],\n",
      "         [ 60.4710, 270.2989]]])\n",
      "xyn: tensor([[[0.1775, 0.3679],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1696, 0.3604],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1488, 0.3662],\n",
      "         [0.1174, 0.4152],\n",
      "         [0.1624, 0.4025],\n",
      "         [0.1326, 0.4654],\n",
      "         [0.2292, 0.4080],\n",
      "         [0.2193, 0.4469],\n",
      "         [0.2722, 0.3629],\n",
      "         [0.1090, 0.5343],\n",
      "         [0.1377, 0.5334],\n",
      "         [0.1372, 0.6316],\n",
      "         [0.1725, 0.6359],\n",
      "         [0.1272, 0.7343],\n",
      "         [0.1680, 0.7508]]])\n",
      "\n",
      "0: 640x640 1 person, 117.2ms\n",
      "Speed: 1.3ms preprocess, 117.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3082027435302734, 'inference': 117.19799041748047, 'postprocess': 0.4410743713378906}]\n",
      "Bounding Box: tensor([[ 64.5000, 199.5000,  71.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9103, 0.3045, 0.9452, 0.0780, 0.9558, 0.8938, 0.9984, 0.4438, 0.9979, 0.5215, 0.9937, 0.9929, 0.9992, 0.9899, 0.9990, 0.9667, 0.9925]])\n",
      "data: tensor([[[6.3530e+01, 1.3228e+02, 9.1031e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0453e-01],\n",
      "         [6.0533e+01, 1.2957e+02, 9.4517e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.7964e-02],\n",
      "         [5.2398e+01, 1.3184e+02, 9.5582e-01],\n",
      "         [4.1338e+01, 1.4888e+02, 8.9381e-01],\n",
      "         [5.7810e+01, 1.4622e+02, 9.9838e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4378e-01],\n",
      "         [7.9763e+01, 1.5036e+02, 9.9791e-01],\n",
      "         [7.9679e+01, 1.6085e+02, 5.2152e-01],\n",
      "         [9.2351e+01, 1.3552e+02, 9.9366e-01],\n",
      "         [3.7693e+01, 1.9258e+02, 9.9288e-01],\n",
      "         [4.8285e+01, 1.9267e+02, 9.9924e-01],\n",
      "         [4.7471e+01, 2.2764e+02, 9.8986e-01],\n",
      "         [6.0091e+01, 2.2910e+02, 9.9895e-01],\n",
      "         [4.6049e+01, 2.6396e+02, 9.6668e-01],\n",
      "         [5.5709e+01, 2.6845e+02, 9.9247e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 63.5299, 132.2838],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 60.5334, 129.5715],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 52.3985, 131.8436],\n",
      "         [ 41.3375, 148.8846],\n",
      "         [ 57.8096, 146.2159],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 79.7634, 150.3620],\n",
      "         [ 79.6788, 160.8480],\n",
      "         [ 92.3513, 135.5229],\n",
      "         [ 37.6933, 192.5764],\n",
      "         [ 48.2848, 192.6722],\n",
      "         [ 47.4709, 227.6447],\n",
      "         [ 60.0912, 229.1022],\n",
      "         [ 46.0493, 263.9588],\n",
      "         [ 55.7089, 268.4500]]])\n",
      "xyn: tensor([[[0.1765, 0.3675],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1681, 0.3599],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1456, 0.3662],\n",
      "         [0.1148, 0.4136],\n",
      "         [0.1606, 0.4062],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2216, 0.4177],\n",
      "         [0.2213, 0.4468],\n",
      "         [0.2565, 0.3765],\n",
      "         [0.1047, 0.5349],\n",
      "         [0.1341, 0.5352],\n",
      "         [0.1319, 0.6323],\n",
      "         [0.1669, 0.6364],\n",
      "         [0.1279, 0.7332],\n",
      "         [0.1547, 0.7457]]])\n",
      "\n",
      "0: 640x640 1 person, 116.0ms\n",
      "Speed: 1.3ms preprocess, 116.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2631416320800781, 'inference': 116.04619026184082, 'postprocess': 0.4050731658935547}]\n",
      "Bounding Box: tensor([[ 62.0000, 198.5000,  68.0000, 163.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9345, 0.3553, 0.9593, 0.0664, 0.9537, 0.8957, 0.9985, 0.4858, 0.9983, 0.5985, 0.9956, 0.9934, 0.9993, 0.9905, 0.9990, 0.9665, 0.9925]])\n",
      "data: tensor([[[6.2112e+01, 1.3215e+02, 9.3450e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5535e-01],\n",
      "         [5.9112e+01, 1.2937e+02, 9.5935e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.6402e-02],\n",
      "         [4.9937e+01, 1.3166e+02, 9.5372e-01],\n",
      "         [4.0536e+01, 1.4845e+02, 8.9569e-01],\n",
      "         [5.4313e+01, 1.4769e+02, 9.9851e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.8584e-01],\n",
      "         [7.4602e+01, 1.5709e+02, 9.9828e-01],\n",
      "         [7.9903e+01, 1.6020e+02, 5.9854e-01],\n",
      "         [8.7856e+01, 1.4431e+02, 9.9558e-01],\n",
      "         [3.7227e+01, 1.9227e+02, 9.9342e-01],\n",
      "         [4.6377e+01, 1.9277e+02, 9.9930e-01],\n",
      "         [4.5143e+01, 2.2845e+02, 9.9048e-01],\n",
      "         [5.9068e+01, 2.2917e+02, 9.9901e-01],\n",
      "         [4.5956e+01, 2.6419e+02, 9.6646e-01],\n",
      "         [5.2654e+01, 2.6683e+02, 9.9245e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 62.1123, 132.1511],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 59.1123, 129.3723],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 49.9365, 131.6582],\n",
      "         [ 40.5358, 148.4477],\n",
      "         [ 54.3126, 147.6927],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.6021, 157.0923],\n",
      "         [ 79.9028, 160.1980],\n",
      "         [ 87.8564, 144.3122],\n",
      "         [ 37.2267, 192.2733],\n",
      "         [ 46.3775, 192.7726],\n",
      "         [ 45.1429, 228.4458],\n",
      "         [ 59.0678, 229.1671],\n",
      "         [ 45.9561, 264.1881],\n",
      "         [ 52.6538, 266.8297]]])\n",
      "xyn: tensor([[[0.1725, 0.3671],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1642, 0.3594],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1387, 0.3657],\n",
      "         [0.1126, 0.4124],\n",
      "         [0.1509, 0.4103],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2072, 0.4364],\n",
      "         [0.2220, 0.4450],\n",
      "         [0.2440, 0.4009],\n",
      "         [0.1034, 0.5341],\n",
      "         [0.1288, 0.5355],\n",
      "         [0.1254, 0.6346],\n",
      "         [0.1641, 0.6366],\n",
      "         [0.1277, 0.7339],\n",
      "         [0.1463, 0.7412]]])\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.3ms preprocess, 114.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2691020965576172, 'inference': 114.05205726623535, 'postprocess': 0.4012584686279297}]\n",
      "Bounding Box: tensor([[ 59.5000, 201.5000,  67.0000, 169.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9303, 0.4574, 0.9587, 0.1113, 0.9550, 0.8753, 0.9978, 0.3700, 0.9957, 0.4324, 0.9875, 0.9901, 0.9988, 0.9894, 0.9987, 0.9718, 0.9930]])\n",
      "data: tensor([[[5.9963e+01, 1.3209e+02, 9.3029e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5738e-01],\n",
      "         [5.6851e+01, 1.2916e+02, 9.5871e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1129e-01],\n",
      "         [4.8558e+01, 1.3096e+02, 9.5504e-01],\n",
      "         [3.8244e+01, 1.4824e+02, 8.7533e-01],\n",
      "         [5.3031e+01, 1.4772e+02, 9.9776e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7005e-01],\n",
      "         [6.6933e+01, 1.6300e+02, 9.9570e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3243e-01],\n",
      "         [8.1629e+01, 1.5462e+02, 9.8752e-01],\n",
      "         [3.5638e+01, 1.9056e+02, 9.9006e-01],\n",
      "         [4.5662e+01, 1.9126e+02, 9.9880e-01],\n",
      "         [4.4192e+01, 2.2813e+02, 9.8935e-01],\n",
      "         [5.6438e+01, 2.2908e+02, 9.9873e-01],\n",
      "         [4.3504e+01, 2.6770e+02, 9.7177e-01],\n",
      "         [4.5820e+01, 2.7109e+02, 9.9295e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 59.9634, 132.0907],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 56.8508, 129.1595],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 48.5582, 130.9552],\n",
      "         [ 38.2438, 148.2427],\n",
      "         [ 53.0312, 147.7154],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.9328, 163.0047],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 81.6290, 154.6174],\n",
      "         [ 35.6376, 190.5640],\n",
      "         [ 45.6623, 191.2616],\n",
      "         [ 44.1922, 228.1320],\n",
      "         [ 56.4377, 229.0847],\n",
      "         [ 43.5043, 267.7005],\n",
      "         [ 45.8202, 271.0887]]])\n",
      "xyn: tensor([[[0.1666, 0.3669],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1579, 0.3588],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1349, 0.3638],\n",
      "         [0.1062, 0.4118],\n",
      "         [0.1473, 0.4103],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1859, 0.4528],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2267, 0.4295],\n",
      "         [0.0990, 0.5293],\n",
      "         [0.1268, 0.5313],\n",
      "         [0.1228, 0.6337],\n",
      "         [0.1568, 0.6363],\n",
      "         [0.1208, 0.7436],\n",
      "         [0.1273, 0.7530]]])\n",
      "\n",
      "0: 640x640 1 person, 130.2ms\n",
      "Speed: 1.3ms preprocess, 130.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 217],\n",
      "        [173, 195, 217],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2640953063964844, 'inference': 130.19299507141113, 'postprocess': 0.675201416015625}]\n",
      "Bounding Box: tensor([[ 58., 202.,  64., 170.]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9176, 0.3782, 0.9591, 0.0842, 0.9585, 0.8225, 0.9978, 0.2846, 0.9966, 0.3500, 0.9892, 0.9820, 0.9982, 0.9825, 0.9983, 0.9567, 0.9906]])\n",
      "data: tensor([[[5.8507e+01, 1.3175e+02, 9.1765e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7818e-01],\n",
      "         [5.5606e+01, 1.2880e+02, 9.5908e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 8.4160e-02],\n",
      "         [4.7224e+01, 1.3066e+02, 9.5848e-01],\n",
      "         [3.7076e+01, 1.4761e+02, 8.2253e-01],\n",
      "         [4.9309e+01, 1.4781e+02, 9.9779e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8462e-01],\n",
      "         [5.8945e+01, 1.6792e+02, 9.9663e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5003e-01],\n",
      "         [7.8859e+01, 1.6273e+02, 9.8918e-01],\n",
      "         [3.5407e+01, 1.9003e+02, 9.8198e-01],\n",
      "         [4.3330e+01, 1.9117e+02, 9.9823e-01],\n",
      "         [4.4436e+01, 2.2878e+02, 9.8251e-01],\n",
      "         [5.4032e+01, 2.3019e+02, 9.9831e-01],\n",
      "         [4.3972e+01, 2.6926e+02, 9.5669e-01],\n",
      "         [4.0476e+01, 2.7255e+02, 9.9056e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 58.5074, 131.7541],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 55.6063, 128.8029],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 47.2244, 130.6589],\n",
      "         [ 37.0762, 147.6097],\n",
      "         [ 49.3090, 147.8092],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 58.9449, 167.9236],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 78.8586, 162.7327],\n",
      "         [ 35.4073, 190.0347],\n",
      "         [ 43.3301, 191.1745],\n",
      "         [ 44.4357, 228.7788],\n",
      "         [ 54.0323, 230.1945],\n",
      "         [ 43.9716, 269.2619],\n",
      "         [ 40.4763, 272.5524]]])\n",
      "xyn: tensor([[[0.1625, 0.3660],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1545, 0.3578],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1312, 0.3629],\n",
      "         [0.1030, 0.4100],\n",
      "         [0.1370, 0.4106],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1637, 0.4665],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2191, 0.4520],\n",
      "         [0.0984, 0.5279],\n",
      "         [0.1204, 0.5310],\n",
      "         [0.1234, 0.6355],\n",
      "         [0.1501, 0.6394],\n",
      "         [0.1221, 0.7479],\n",
      "         [0.1124, 0.7571]]])\n",
      "\n",
      "0: 640x640 1 person, 121.7ms\n",
      "Speed: 1.3ms preprocess, 121.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2791156768798828, 'inference': 121.6897964477539, 'postprocess': 0.8339881896972656}]\n",
      "Bounding Box: tensor([[ 53.5000, 200.0000,  59.0000, 168.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9045, 0.3203, 0.9606, 0.0685, 0.9545, 0.7039, 0.9969, 0.1243, 0.9949, 0.1629, 0.9835, 0.9530, 0.9963, 0.9525, 0.9965, 0.8996, 0.9813]])\n",
      "data: tensor([[[5.7294e+01, 1.3142e+02, 9.0445e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2030e-01],\n",
      "         [5.4402e+01, 1.2856e+02, 9.6063e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.8529e-02],\n",
      "         [4.5450e+01, 1.3074e+02, 9.5452e-01],\n",
      "         [3.8303e+01, 1.4755e+02, 7.0391e-01],\n",
      "         [4.6097e+01, 1.4830e+02, 9.9692e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.2427e-01],\n",
      "         [5.3946e+01, 1.7029e+02, 9.9495e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.6290e-01],\n",
      "         [7.4743e+01, 1.7137e+02, 9.8353e-01],\n",
      "         [3.6128e+01, 1.9070e+02, 9.5301e-01],\n",
      "         [4.0694e+01, 1.9197e+02, 9.9632e-01],\n",
      "         [4.6510e+01, 2.2760e+02, 9.5251e-01],\n",
      "         [5.1694e+01, 2.3016e+02, 9.9654e-01],\n",
      "         [4.2163e+01, 2.6323e+02, 8.9960e-01],\n",
      "         [3.7907e+01, 2.7045e+02, 9.8132e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 57.2941, 131.4193],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 54.4018, 128.5638],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 45.4501, 130.7392],\n",
      "         [ 38.3028, 147.5502],\n",
      "         [ 46.0968, 148.2987],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 53.9456, 170.2889],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 74.7430, 171.3732],\n",
      "         [ 36.1283, 190.7048],\n",
      "         [ 40.6940, 191.9694],\n",
      "         [ 46.5096, 227.5950],\n",
      "         [ 51.6943, 230.1551],\n",
      "         [ 42.1633, 263.2305],\n",
      "         [ 37.9070, 270.4458]]])\n",
      "xyn: tensor([[[0.1592, 0.3651],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1511, 0.3571],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1263, 0.3632],\n",
      "         [0.1064, 0.4099],\n",
      "         [0.1280, 0.4119],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1498, 0.4730],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2076, 0.4760],\n",
      "         [0.1004, 0.5297],\n",
      "         [0.1130, 0.5332],\n",
      "         [0.1292, 0.6322],\n",
      "         [0.1436, 0.6393],\n",
      "         [0.1171, 0.7312],\n",
      "         [0.1053, 0.7512]]])\n",
      "\n",
      "0: 640x640 1 person, 135.6ms\n",
      "Speed: 1.3ms preprocess, 135.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.2900829315185547, 'inference': 135.6189250946045, 'postprocess': 0.42700767517089844}]\n",
      "Bounding Box: tensor([[ 50.5000, 198.0000,  55.0000, 164.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8472, 0.1727, 0.9274, 0.0430, 0.9568, 0.8012, 0.9992, 0.2832, 0.9992, 0.3530, 0.9967, 0.9875, 0.9993, 0.9723, 0.9984, 0.9141, 0.9868]])\n",
      "data: tensor([[[5.5497e+01, 1.3131e+02, 8.4716e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7275e-01],\n",
      "         [5.2421e+01, 1.2864e+02, 9.2738e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3048e-02],\n",
      "         [4.3687e+01, 1.3136e+02, 9.5682e-01],\n",
      "         [4.4702e+01, 1.4807e+02, 8.0115e-01],\n",
      "         [3.9606e+01, 1.4969e+02, 9.9917e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8317e-01],\n",
      "         [4.3748e+01, 1.7615e+02, 9.9917e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5302e-01],\n",
      "         [6.6356e+01, 1.8173e+02, 9.9666e-01],\n",
      "         [4.2787e+01, 1.9191e+02, 9.8751e-01],\n",
      "         [3.9162e+01, 1.9338e+02, 9.9929e-01],\n",
      "         [4.6907e+01, 2.2745e+02, 9.7232e-01],\n",
      "         [4.7124e+01, 2.2983e+02, 9.9843e-01],\n",
      "         [4.2035e+01, 2.6145e+02, 9.1411e-01],\n",
      "         [3.3504e+01, 2.6885e+02, 9.8679e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 55.4973, 131.3133],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 52.4215, 128.6368],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 43.6865, 131.3616],\n",
      "         [ 44.7015, 148.0668],\n",
      "         [ 39.6062, 149.6883],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 43.7478, 176.1452],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.3558, 181.7349],\n",
      "         [ 42.7870, 191.9124],\n",
      "         [ 39.1616, 193.3804],\n",
      "         [ 46.9068, 227.4486],\n",
      "         [ 47.1243, 229.8281],\n",
      "         [ 42.0353, 261.4519],\n",
      "         [ 33.5042, 268.8492]]])\n",
      "xyn: tensor([[[0.1542, 0.3648],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1456, 0.3573],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1214, 0.3649],\n",
      "         [0.1242, 0.4113],\n",
      "         [0.1100, 0.4158],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1215, 0.4893],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1843, 0.5048],\n",
      "         [0.1189, 0.5331],\n",
      "         [0.1088, 0.5372],\n",
      "         [0.1303, 0.6318],\n",
      "         [0.1309, 0.6384],\n",
      "         [0.1168, 0.7263],\n",
      "         [0.0931, 0.7468]]])\n",
      "\n",
      "0: 640x640 1 person, 150.8ms\n",
      "Speed: 1.4ms preprocess, 150.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detection results: [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       [[173, 195, 219],\n",
      "        [173, 195, 219],\n",
      "        [173, 194, 221],\n",
      "        ...,\n",
      "        [144, 171, 204],\n",
      "        [145, 172, 205],\n",
      "        [144, 171, 204]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 61,  84,  22],\n",
      "        [ 53,  76,  13],\n",
      "        [ 56,  80,  17]],\n",
      "\n",
      "       [[162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        [162, 150, 129],\n",
      "        ...,\n",
      "        [ 63,  88,  25],\n",
      "        [ 62,  87,  24],\n",
      "        [ 66,  90,  27]],\n",
      "\n",
      "       [[161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        [161, 148, 127],\n",
      "        ...,\n",
      "        [ 80, 104,  41],\n",
      "        [ 80, 104,  41],\n",
      "        [ 84, 109,  46]]], shape=(360, 360, 3), dtype=uint8)\n",
      "orig_shape: (360, 360)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 1.3759136199951172, 'inference': 150.803804397583, 'postprocess': 0.7417201995849609}]\n",
      "Bounding Box: tensor([[ 49.5000, 198.0000,  55.0000, 164.0000]])\n",
      "Keypoints: ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.8510, 0.1775, 0.9276, 0.0437, 0.9561, 0.8111, 0.9992, 0.3028, 0.9992, 0.3755, 0.9968, 0.9884, 0.9993, 0.9740, 0.9985, 0.9172, 0.9870]])\n",
      "data: tensor([[[5.5447e+01, 1.3136e+02, 8.5102e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7755e-01],\n",
      "         [5.2383e+01, 1.2866e+02, 9.2759e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3736e-02],\n",
      "         [4.3660e+01, 1.3134e+02, 9.5609e-01],\n",
      "         [4.4839e+01, 1.4817e+02, 8.1113e-01],\n",
      "         [3.9583e+01, 1.4973e+02, 9.9918e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0281e-01],\n",
      "         [4.3766e+01, 1.7618e+02, 9.9918e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7554e-01],\n",
      "         [6.6390e+01, 1.8178e+02, 9.9678e-01],\n",
      "         [4.2941e+01, 1.9204e+02, 9.8839e-01],\n",
      "         [3.9237e+01, 1.9348e+02, 9.9932e-01],\n",
      "         [4.6877e+01, 2.2764e+02, 9.7395e-01],\n",
      "         [4.7128e+01, 2.3001e+02, 9.9848e-01],\n",
      "         [4.2062e+01, 2.6165e+02, 9.1719e-01],\n",
      "         [3.3350e+01, 2.6907e+02, 9.8702e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (360, 360)\n",
      "shape: torch.Size([1, 17, 3])\n",
      "xy: tensor([[[ 55.4473, 131.3553],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 52.3826, 128.6555],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 43.6602, 131.3383],\n",
      "         [ 44.8395, 148.1694],\n",
      "         [ 39.5828, 149.7308],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 43.7658, 176.1792],\n",
      "         [  0.0000,   0.0000],\n",
      "         [ 66.3904, 181.7800],\n",
      "         [ 42.9406, 192.0373],\n",
      "         [ 39.2372, 193.4762],\n",
      "         [ 46.8774, 227.6415],\n",
      "         [ 47.1280, 230.0059],\n",
      "         [ 42.0624, 261.6528],\n",
      "         [ 33.3504, 269.0701]]])\n",
      "xyn: tensor([[[0.1540, 0.3649],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1455, 0.3574],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1213, 0.3648],\n",
      "         [0.1246, 0.4116],\n",
      "         [0.1100, 0.4159],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1216, 0.4894],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1844, 0.5049],\n",
      "         [0.1193, 0.5334],\n",
      "         [0.1090, 0.5374],\n",
      "         [0.1302, 0.6323],\n",
      "         [0.1309, 0.6389],\n",
      "         [0.1168, 0.7268],\n",
      "         [0.0926, 0.7474]]])\n",
      "Extracted features: [[     68.174      139.72     0.48794      9888.6         nan]]\n",
      "Features shape before scaling: (1, 5)\n",
      "Error processing video: X has 5 features, but MinMaxScaler is expecting 34 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib  # For loading the pickle model\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO  # Import YOLOv8 from ultralytics\n",
    "\n",
    "# Load the classification model (Random Forest)\n",
    "classification_model = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "\n",
    "# Define custom objects if needed (for metrics, loss functions, etc.)\n",
    "custom_objects = {\n",
    "    'mse': tf.keras.metrics.MeanSquaredError()  # Make sure to specify custom metrics if needed\n",
    "}\n",
    "\n",
    "# Load the scoring model with custom objects\n",
    "scoring_model = tf.keras.models.load_model(\n",
    "    \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\",\n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "# Load the scaler used for feature scaling\n",
    "scaler = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "\n",
    "# Load YOLOv8 Pose model (YOLOv8s-pose)\n",
    "yolo_model = YOLO('yolov8s-pose.pt')  # Load the pretrained pose model from Ultralytics\n",
    "\n",
    "# Function to extract features from a video using YOLOv8 Pose\n",
    "def extract_features_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints_list = []\n",
    "    bounding_boxes = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame from BGR to RGB for YOLOv8 Pose model\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Use YOLOv8 Pose model to detect keypoints\n",
    "        results = yolo_model.predict(frame_rgb)\n",
    "\n",
    "        # Debugging: Inspect detection results\n",
    "        print(f\"Detection results: {results}\")\n",
    "\n",
    "        # Check if results contain boxes and keypoints\n",
    "        if len(results[0].boxes) > 0:\n",
    "            for result in results[0].boxes:\n",
    "                print(f\"Bounding Box: {result.xywh}\")\n",
    "            for keypoints in results[0].keypoints:\n",
    "                print(f\"Keypoints: {keypoints}\")\n",
    "\n",
    "            # Extract keypoints and bounding box\n",
    "            for result in results[0].boxes:\n",
    "                bbox = result.xywh[0][:4].numpy()  # Bounding box: [x, y, width, height]\n",
    "                keypoints = result.xywh[0][5:].numpy()  # Keypoints\n",
    "                keypoints_list.append(keypoints)\n",
    "                bounding_boxes.append(bbox)\n",
    "\n",
    "        else:\n",
    "            print(\"No bounding boxes detected in this frame.\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not keypoints_list or not bounding_boxes:\n",
    "        raise ValueError(\"No keypoints or bounding boxes extracted from the video.\")\n",
    "\n",
    "    # Calculate features: width, height, aspect ratio, etc.\n",
    "    width = np.mean([bbox[2] for bbox in bounding_boxes])\n",
    "    height = np.mean([bbox[3] for bbox in bounding_boxes])\n",
    "    aspect_ratio = width / height\n",
    "    area = np.mean([bbox[2] * bbox[3] for bbox in bounding_boxes])\n",
    "\n",
    "    avg_distance = np.mean([\n",
    "        np.linalg.norm(keypoints[i][:2] - keypoints[j][:2])\n",
    "        for keypoints in keypoints_list\n",
    "        for i in range(len(keypoints))\n",
    "        for j in range(i + 1, len(keypoints))\n",
    "        if keypoints[i][2] > 0 and keypoints[j][2] > 0\n",
    "    ])\n",
    "\n",
    "    # Additional features if needed (for example, keypoint distances, pose angles, etc.)\n",
    "    # For now, using the existing 5 features (width, height, aspect_ratio, area, avg_distance)\n",
    "    return np.array([[width, height, aspect_ratio, area, avg_distance]])\n",
    "\n",
    "# Function to classify and score a video\n",
    "def classify_and_score_video(video_path):\n",
    "    # Extract features\n",
    "    try:\n",
    "        features = extract_features_from_video(video_path)\n",
    "        print(f\"Extracted features: {features}\")  # Debugging line\n",
    "    except ValueError as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None, None  # Return None if no features are extracted\n",
    "\n",
    "    # Check if features have the expected shape (5 features)\n",
    "    print(f\"Features shape before scaling: {features.shape}\")  # Debugging line\n",
    "    features_scaled = scaler.transform(features)  # Scale features using the same scaler\n",
    "\n",
    "    # Check if features are correctly scaled\n",
    "    print(f\"Scaled features: {features_scaled}\")  # Debugging line\n",
    "\n",
    "    # Predict exercise type using the Random Forest model\n",
    "    exercise_class = classification_model.predict(features_scaled)[0]  # Direct prediction\n",
    "    print(f\"Classified Exercise: {exercise_class}\")\n",
    "\n",
    "    # Predict score using the scoring model\n",
    "    score = scoring_model.predict(features_scaled)[0][0]\n",
    "    print(f\"Predicted Score: {score}\")\n",
    "\n",
    "    return exercise_class, score\n",
    "\n",
    "# Example usage\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_001557.mp4\"\n",
    "try:\n",
    "    exercise_class, predicted_score = classify_and_score_video(video_path)\n",
    "    if exercise_class is not None and predicted_score is not None:\n",
    "        print(f\"Exercise Class: {exercise_class}, Score: {predicted_score}\")\n",
    "    else:\n",
    "        print(\"No valid results from video processing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing video: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 138.1ms\n",
      "Speed: 1.3ms preprocess, 138.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x640 1 person, 203.5ms\n",
      "Speed: 2.1ms preprocess, 203.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 141.1ms\n",
      "Speed: 1.6ms preprocess, 141.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 196.8ms\n",
      "Speed: 1.6ms preprocess, 196.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.2ms\n",
      "Speed: 1.5ms preprocess, 123.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 08:31:25.096 python[1164:9505] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-22 08:31:25.096 python[1164:9505] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 person, 123.8ms\n",
      "Speed: 1.3ms preprocess, 123.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.5ms\n",
      "Speed: 1.3ms preprocess, 125.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.6ms\n",
      "Speed: 1.3ms preprocess, 121.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.3ms\n",
      "Speed: 1.3ms preprocess, 129.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.4ms\n",
      "Speed: 1.6ms preprocess, 126.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.7ms\n",
      "Speed: 1.2ms preprocess, 116.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.6ms\n",
      "Speed: 1.4ms preprocess, 136.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.4ms\n",
      "Speed: 1.5ms preprocess, 132.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.8ms\n",
      "Speed: 1.4ms preprocess, 121.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.3ms\n",
      "Speed: 1.5ms preprocess, 122.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.2ms\n",
      "Speed: 1.5ms preprocess, 129.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.9ms\n",
      "Speed: 1.3ms preprocess, 120.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.7ms\n",
      "Speed: 1.5ms preprocess, 119.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.6ms\n",
      "Speed: 1.4ms preprocess, 134.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.0ms\n",
      "Speed: 1.4ms preprocess, 134.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 174.0ms\n",
      "Speed: 45.0ms preprocess, 174.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 131.1ms\n",
      "Speed: 1.4ms preprocess, 131.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 153.6ms\n",
      "Speed: 1.5ms preprocess, 153.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.7ms\n",
      "Speed: 2.3ms preprocess, 123.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.0ms\n",
      "Speed: 1.3ms preprocess, 134.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 191.9ms\n",
      "Speed: 4.5ms preprocess, 191.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.9ms\n",
      "Speed: 1.8ms preprocess, 129.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.1ms\n",
      "Speed: 1.4ms preprocess, 123.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.3ms\n",
      "Speed: 1.2ms preprocess, 120.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.1ms\n",
      "Speed: 1.4ms preprocess, 118.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.3ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.1ms\n",
      "Speed: 1.3ms preprocess, 112.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.8ms\n",
      "Speed: 1.3ms preprocess, 115.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.4ms\n",
      "Speed: 1.4ms preprocess, 121.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.1ms\n",
      "Speed: 1.5ms preprocess, 121.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.0ms\n",
      "Speed: 1.4ms preprocess, 119.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.2ms preprocess, 116.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.5ms\n",
      "Speed: 1.4ms preprocess, 110.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.3ms preprocess, 114.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.7ms\n",
      "Speed: 1.2ms preprocess, 113.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.9ms\n",
      "Speed: 1.7ms preprocess, 121.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.7ms\n",
      "Speed: 1.4ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.4ms\n",
      "Speed: 1.3ms preprocess, 113.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.7ms\n",
      "Speed: 1.2ms preprocess, 115.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.8ms\n",
      "Speed: 2.6ms preprocess, 117.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.9ms\n",
      "Speed: 1.2ms preprocess, 110.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.8ms\n",
      "Speed: 1.2ms preprocess, 120.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.1ms\n",
      "Speed: 1.3ms preprocess, 112.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 1.2ms preprocess, 116.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.2ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.1ms\n",
      "Speed: 1.3ms preprocess, 112.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.6ms\n",
      "Speed: 1.2ms preprocess, 119.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.2ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.4ms preprocess, 114.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.1ms\n",
      "Speed: 1.2ms preprocess, 120.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.2ms\n",
      "Speed: 1.2ms preprocess, 115.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.9ms\n",
      "Speed: 2.8ms preprocess, 121.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.3ms\n",
      "Speed: 1.4ms preprocess, 115.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.8ms\n",
      "Speed: 1.3ms preprocess, 114.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.3ms\n",
      "Speed: 1.2ms preprocess, 114.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.3ms\n",
      "Speed: 1.5ms preprocess, 119.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.4ms preprocess, 116.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.4ms\n",
      "Speed: 1.2ms preprocess, 114.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.3ms\n",
      "Speed: 1.2ms preprocess, 117.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.5ms\n",
      "Speed: 1.6ms preprocess, 120.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 1.3ms preprocess, 117.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.6ms\n",
      "Speed: 1.3ms preprocess, 114.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.1ms\n",
      "Speed: 1.3ms preprocess, 113.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.4ms\n",
      "Speed: 1.2ms preprocess, 117.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.3ms preprocess, 116.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.4ms\n",
      "Speed: 1.3ms preprocess, 119.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.3ms preprocess, 116.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.2ms\n",
      "Speed: 1.4ms preprocess, 123.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.2ms\n",
      "Speed: 1.2ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.3ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.2ms preprocess, 114.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.5ms\n",
      "Speed: 1.3ms preprocess, 117.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 131.4ms\n",
      "Speed: 2.9ms preprocess, 131.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 176.5ms\n",
      "Speed: 1.3ms preprocess, 176.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 144.4ms\n",
      "Speed: 1.3ms preprocess, 144.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.2ms\n",
      "Speed: 1.6ms preprocess, 125.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.4ms\n",
      "Speed: 1.5ms preprocess, 127.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.5ms\n",
      "Speed: 1.7ms preprocess, 122.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.2ms\n",
      "Speed: 1.4ms preprocess, 134.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 128.9ms\n",
      "Speed: 1.2ms preprocess, 128.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.4ms\n",
      "Speed: 1.3ms preprocess, 119.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.8ms\n",
      "Speed: 1.2ms preprocess, 115.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.4ms\n",
      "Speed: 1.2ms preprocess, 123.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 1.4ms preprocess, 116.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 1.2ms preprocess, 116.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.5ms\n",
      "Speed: 1.4ms preprocess, 121.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.3ms preprocess, 114.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.5ms\n",
      "Speed: 1.2ms preprocess, 127.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.1ms\n",
      "Speed: 1.3ms preprocess, 127.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.7ms\n",
      "Speed: 1.4ms preprocess, 133.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 128.3ms\n",
      "Speed: 1.4ms preprocess, 128.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.6ms\n",
      "Speed: 1.5ms preprocess, 126.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.8ms\n",
      "Speed: 6.6ms preprocess, 136.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.8ms\n",
      "Speed: 1.3ms preprocess, 126.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 1.4ms preprocess, 116.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.1ms\n",
      "Speed: 1.2ms preprocess, 129.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.8ms\n",
      "Speed: 1.3ms preprocess, 118.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.7ms\n",
      "Speed: 1.2ms preprocess, 120.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.5ms\n",
      "Speed: 1.5ms preprocess, 118.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.4ms\n",
      "Speed: 1.2ms preprocess, 124.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.3ms\n",
      "Speed: 1.2ms preprocess, 111.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.0ms\n",
      "Speed: 1.2ms preprocess, 116.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.8ms\n",
      "Speed: 1.2ms preprocess, 118.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.3ms\n",
      "Speed: 1.2ms preprocess, 132.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.6ms\n",
      "Speed: 1.4ms preprocess, 114.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.1ms\n",
      "Speed: 1.2ms preprocess, 119.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.1ms\n",
      "Speed: 1.3ms preprocess, 120.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 148.1ms\n",
      "Speed: 1.5ms preprocess, 148.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.2ms\n",
      "Speed: 1.2ms preprocess, 123.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.3ms preprocess, 113.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.4ms\n",
      "Speed: 1.3ms preprocess, 114.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.2ms\n",
      "Speed: 1.4ms preprocess, 115.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.1ms\n",
      "Speed: 1.2ms preprocess, 122.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.2ms preprocess, 114.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.2ms preprocess, 114.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.2ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.6ms\n",
      "Speed: 1.3ms preprocess, 118.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.0ms\n",
      "Speed: 1.3ms preprocess, 115.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 128.8ms\n",
      "Speed: 1.5ms preprocess, 128.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.9ms\n",
      "Speed: 1.3ms preprocess, 122.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.4ms\n",
      "Speed: 1.2ms preprocess, 119.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.4ms\n",
      "Speed: 1.3ms preprocess, 147.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.3ms\n",
      "Speed: 1.3ms preprocess, 122.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.7ms\n",
      "Speed: 3.4ms preprocess, 127.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.1ms\n",
      "Speed: 1.2ms preprocess, 118.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.6ms\n",
      "Speed: 1.3ms preprocess, 121.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.7ms\n",
      "Speed: 1.4ms preprocess, 118.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.6ms\n",
      "Speed: 1.6ms preprocess, 123.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.5ms\n",
      "Speed: 1.3ms preprocess, 114.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.7ms\n",
      "Speed: 1.2ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.4ms preprocess, 114.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.2ms preprocess, 116.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.2ms\n",
      "Speed: 1.2ms preprocess, 117.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.2ms\n",
      "Speed: 1.2ms preprocess, 122.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.1ms\n",
      "Speed: 1.3ms preprocess, 112.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.3ms\n",
      "Speed: 1.2ms preprocess, 116.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.7ms\n",
      "Speed: 1.3ms preprocess, 114.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.6ms\n",
      "Speed: 3.0ms preprocess, 124.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.0ms\n",
      "Speed: 1.3ms preprocess, 117.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.7ms\n",
      "Speed: 1.4ms preprocess, 117.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.8ms\n",
      "Speed: 1.4ms preprocess, 113.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.5ms\n",
      "Speed: 1.3ms preprocess, 120.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.2ms\n",
      "Speed: 1.4ms preprocess, 115.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.3ms\n",
      "Speed: 1.2ms preprocess, 117.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.1ms\n",
      "Speed: 1.2ms preprocess, 118.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.0ms\n",
      "Speed: 1.3ms preprocess, 116.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.1ms\n",
      "Speed: 1.3ms preprocess, 115.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.0ms\n",
      "Speed: 1.2ms preprocess, 115.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.3ms preprocess, 114.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.8ms\n",
      "Speed: 1.2ms preprocess, 117.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.3ms\n",
      "Speed: 1.4ms preprocess, 114.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.5ms\n",
      "Speed: 3.6ms preprocess, 122.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.9ms\n",
      "Speed: 1.2ms preprocess, 115.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.6ms\n",
      "Speed: 1.2ms preprocess, 117.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.7ms\n",
      "Speed: 1.3ms preprocess, 116.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 1.4ms preprocess, 114.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.6ms\n",
      "Speed: 1.2ms preprocess, 122.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.0ms\n",
      "Speed: 1.2ms preprocess, 115.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.6ms\n",
      "Speed: 1.3ms preprocess, 115.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 1.2ms preprocess, 116.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.2ms preprocess, 116.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.7ms\n",
      "Speed: 1.2ms preprocess, 116.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.2ms\n",
      "Speed: 1.2ms preprocess, 116.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.5ms\n",
      "Speed: 1.4ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.8ms\n",
      "Speed: 1.2ms preprocess, 117.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.9ms\n",
      "Speed: 1.2ms preprocess, 113.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.6ms\n",
      "Speed: 1.3ms preprocess, 119.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.8ms\n",
      "Speed: 2.5ms preprocess, 120.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.4ms\n",
      "Speed: 1.3ms preprocess, 117.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 1.3ms preprocess, 116.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.7ms\n",
      "Speed: 1.4ms preprocess, 136.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.1ms\n",
      "Speed: 1.2ms preprocess, 115.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.0ms\n",
      "Speed: 1.4ms preprocess, 121.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.5ms\n",
      "Speed: 1.8ms preprocess, 127.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.3ms\n",
      "Speed: 1.3ms preprocess, 117.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 1.3ms preprocess, 114.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 1.5ms preprocess, 117.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.4ms\n",
      "Speed: 1.2ms preprocess, 116.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.2ms\n",
      "Speed: 1.3ms preprocess, 119.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.0ms\n",
      "Speed: 1.2ms preprocess, 116.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.4ms\n",
      "Speed: 1.2ms preprocess, 115.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.9ms\n",
      "Speed: 1.3ms preprocess, 112.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.4ms\n",
      "Speed: 1.4ms preprocess, 118.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.1ms\n",
      "Speed: 1.2ms preprocess, 117.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Class Percentages (Based on Frame Count):\n",
      "Discurweper.json: 90.05%\n",
      "Estafette.json: 6.28%\n",
      "Speerwerpen.json: 3.66%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 0.19%\n",
      "Estafette.json: 0.11%\n",
      "Hoogspringen.json: 0.10%\n",
      "Hordenlopen.json: 0.10%\n",
      "Kogelstoten.json: 0.10%\n",
      "Speerwerpen.json: 0.10%\n",
      "sprint_start.json: 0.10%\n",
      "sprint.json: 0.10%\n",
      "Verspringen.json: 0.10%\n",
      "\n",
      "Average Score for the Video: 3.51\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from scipy.special import softmax\n",
    "from ultralytics import YOLO\n",
    "from keras.metrics import MeanSquaredError\n",
    "\n",
    "# Define the custom metric if necessary\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Register the custom function if needed\n",
    "tf.keras.utils.get_custom_objects()[\"mse\"] = mse\n",
    "\n",
    "# Load the scoring model with the custom metric\n",
    "scoring_model = tf.keras.models.load_model(\n",
    "    \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\",\n",
    "    custom_objects={\"mse\": mse}\n",
    ")\n",
    "\n",
    "# Load models and scalers\n",
    "clf = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "scaler_classification = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "exercise_labels_inv = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/exercise_labels.pkl\")\n",
    "scoring_model = tf.keras.models.load_model(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\")\n",
    "scaler_scoring = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_scaler.pkl\")\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8s-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_001557.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "class_counts = Counter()\n",
    "class_probabilities = []\n",
    "frame_scores = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "\n",
    "    # Step 2: Classify Exercise\n",
    "    if keypoints.size == scaler_classification.n_features_in_:\n",
    "        normalized_keypoints = scaler_classification.transform([keypoints])\n",
    "        exercise_class = clf.predict(normalized_keypoints)[0]\n",
    "        exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "        exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "        class_counts[exercise_class] += 1\n",
    "        class_probabilities.append(exercise_class_proba[0])\n",
    "\n",
    "        # Step 3: Extract Scoring Features\n",
    "        bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "        keypoints_reshaped = keypoints.reshape(-1, 2)\n",
    "\n",
    "        width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        aspect_ratio = width / height\n",
    "        distances = [\n",
    "            np.linalg.norm(keypoints_reshaped[i] - keypoints_reshaped[j])\n",
    "            for i in range(len(keypoints_reshaped))\n",
    "            for j in range(i + 1, len(keypoints_reshaped))\n",
    "        ]\n",
    "        avg_distance = np.mean(distances) if distances else 0\n",
    "        area = width * height\n",
    "\n",
    "        features = np.array([width, height, aspect_ratio, area, avg_distance]).reshape(1, -1)\n",
    "        features_scaled = scaler_scoring.transform(features)\n",
    "\n",
    "        # Function to round score to the closest 0.5\n",
    "        def round_to_closest_half(score):\n",
    "            return round(score * 2) / 2\n",
    "\n",
    "        # Step 4: Predict Score\n",
    "        score = scoring_model.predict(features_scaled).flatten()[0]\n",
    "        rounded_score = round_to_closest_half(score)\n",
    "        frame_scores.append(rounded_score)\n",
    "\n",
    "        # Step 5: Annotate Frame\n",
    "        label = f\"{exercise_labels_inv[exercise_class]} (Score: {rounded_score:.2f})\"\n",
    "        cv2.putText(frame, label, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 6: Aggregate Results\n",
    "total_frames = sum(class_counts.values())\n",
    "if total_frames > 0:\n",
    "    class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "    average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "    average_scores = np.mean(frame_scores)\n",
    "\n",
    "    print(\"Class Percentages (Based on Frame Count):\")\n",
    "    for cls, pct in class_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "    for cls, pct in enumerate(average_probabilities):\n",
    "        print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "    print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Percentages (Based on Frame Count):\n",
      "Discurweper.json: 90.05%\n",
      "Estafette.json: 6.28%\n",
      "Speerwerpen.json: 3.66%\n",
      "\n",
      "Average Probabilities (Softmax Scores):\n",
      "Discurweper.json: 0.19%\n",
      "Estafette.json: 0.11%\n",
      "Hoogspringen.json: 0.10%\n",
      "Hordenlopen.json: 0.10%\n",
      "Kogelstoten.json: 0.10%\n",
      "Speerwerpen.json: 0.10%\n",
      "sprint_start.json: 0.10%\n",
      "sprint.json: 0.10%\n",
      "Verspringen.json: 0.10%\n",
      "\n",
      "Average Score for the Video: 3.51\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Aggregate Results\n",
    "total_frames = sum(class_counts.values())\n",
    "if total_frames > 0:\n",
    "    class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "    average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "    average_scores = np.mean(frame_scores)\n",
    "\n",
    "    print(\"Class Percentages (Based on Frame Count):\")\n",
    "    for cls, pct in class_percentages.items():\n",
    "        print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "    for cls, pct in enumerate(average_probabilities):\n",
    "        print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "    print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "# from scipy.special import softmax\n",
    "# from ultralytics import YOLO\n",
    "# from keras.metrics import MeanSquaredError\n",
    "\n",
    "# # Define the custom metric if necessary\n",
    "# def mse(y_true, y_pred):\n",
    "#     return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# # Register the custom function if needed\n",
    "# tf.keras.utils.get_custom_objects()[\"mse\"] = mse\n",
    "\n",
    "# # Load the scoring model with the custom metric\n",
    "# scoring_model = tf.keras.models.load_model(\n",
    "#     \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model2.h5\",\n",
    "#     custom_objects={\"mse\": mse}\n",
    "# )\n",
    "\n",
    "# # Load models and scalers\n",
    "# clf = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "# scaler_classification = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "# exercise_labels_inv = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/exercise_labels.pkl\")\n",
    "# scoring_model = tf.keras.models.load_model(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\")\n",
    "# scaler_scoring = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_scaler.pkl\")\n",
    "\n",
    "# # Load YOLO model\n",
    "# model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# # Path to the video\n",
    "# video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/DownloadVideoTest/VID20250113141952.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# frame_count = 0\n",
    "# class_counts = Counter()\n",
    "# class_probabilities = []\n",
    "# frame_scores = []\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Step 1: Extract Keypoints\n",
    "#     results = model(frame)\n",
    "#     if len(results) == 0 or results[0].keypoints is None:\n",
    "#         continue\n",
    "\n",
    "#     keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "\n",
    "#     # Step 2: Classify Exercise\n",
    "#     if keypoints.size == scaler_classification.n_features_in_:\n",
    "#         normalized_keypoints = scaler_classification.transform([keypoints])\n",
    "#         exercise_class = clf.predict(normalized_keypoints)[0]\n",
    "#         exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "#         exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "#         class_counts[exercise_class] += 1\n",
    "#         class_probabilities.append(exercise_class_proba[0])\n",
    "\n",
    "#         # Step 3: Extract Scoring Features\n",
    "#         bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "#         keypoints_reshaped = keypoints.reshape(-1, 2)\n",
    "\n",
    "#         width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "#         aspect_ratio = width / height\n",
    "#         distances = [\n",
    "#             np.linalg.norm(keypoints_reshaped[i] - keypoints_reshaped[j])\n",
    "#             for i in range(len(keypoints_reshaped))\n",
    "#             for j in range(i + 1, len(keypoints_reshaped))\n",
    "#         ]\n",
    "#         avg_distance = np.mean(distances) if distances else 0\n",
    "#         area = width * height\n",
    "\n",
    "#         features = np.array([width, height, aspect_ratio, area, avg_distance]).reshape(1, -1)\n",
    "#         features_scaled = scaler_scoring.transform(features)\n",
    "\n",
    "#         # Function to round score to the closest 0.5\n",
    "#         def round_to_closest_half(score):\n",
    "#             return round(score * 2) / 2\n",
    "\n",
    "#         # Step 4: Predict Score\n",
    "#         score = scoring_model.predict(features_scaled).flatten()[0]\n",
    "#         rounded_score = round_to_closest_half(score)\n",
    "#         frame_scores.append(rounded_score)\n",
    "\n",
    "#         # Step 5: Annotate Frame\n",
    "#         label = f\"{exercise_labels_inv[exercise_class]} (Score: {rounded_score:.2f})\"\n",
    "#         cv2.putText(frame, label, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "#     frame_count += 1\n",
    "#     cv2.imshow('Video', frame)\n",
    "\n",
    "#     # Press 'q' to exit\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "#     average_scores = np.mean(frame_scores)\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "#     average_scores = np.mean(frame_scores)\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "# from scipy.special import softmax\n",
    "# from ultralytics import YOLO\n",
    "# from keras.metrics import MeanSquaredError\n",
    "\n",
    "# # Define the custom metric if necessary\n",
    "# def mse(y_true, y_pred):\n",
    "#     return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# # Register the custom function if needed\n",
    "# tf.keras.utils.get_custom_objects()[\"mse\"] = mse\n",
    "\n",
    "# # Load the scoring model with the custom metric\n",
    "# scoring_model = tf.keras.models.load_model(\n",
    "#     \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model2.h5\",\n",
    "#     custom_objects={\"mse\": mse}\n",
    "# )\n",
    "\n",
    "# # Load models and scalers\n",
    "# clf = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "# scaler_classification = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "# exercise_labels_inv = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/exercise_labels.pkl\")\n",
    "# scoring_model = tf.keras.models.load_model(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\")\n",
    "# scaler_scoring = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_scaler.pkl\")\n",
    "\n",
    "# # Load YOLO model\n",
    "# model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# # Path to the video\n",
    "# video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/DownloadVideoTest/VID20250113142324.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# frame_count = 0\n",
    "# class_counts = Counter()\n",
    "# class_probabilities = []\n",
    "# frame_scores = []\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Step 1: Extract Keypoints\n",
    "#     results = model(frame)\n",
    "#     if len(results) == 0 or results[0].keypoints is None:\n",
    "#         continue\n",
    "\n",
    "#     keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "\n",
    "#     # Step 2: Classify Exercise\n",
    "#     if keypoints.size == scaler_classification.n_features_in_:\n",
    "#         normalized_keypoints = scaler_classification.transform([keypoints])\n",
    "#         exercise_class = clf.predict(normalized_keypoints)[0]\n",
    "#         exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "#         exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "#         class_counts[exercise_class] += 1\n",
    "#         class_probabilities.append(exercise_class_proba[0])\n",
    "\n",
    "#         # Step 3: Extract Scoring Features\n",
    "#         bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "#         keypoints_reshaped = keypoints.reshape(-1, 2)\n",
    "\n",
    "#         width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "#         aspect_ratio = width / height\n",
    "#         distances = [\n",
    "#             np.linalg.norm(keypoints_reshaped[i] - keypoints_reshaped[j])\n",
    "#             for i in range(len(keypoints_reshaped))\n",
    "#             for j in range(i + 1, len(keypoints_reshaped))\n",
    "#         ]\n",
    "#         avg_distance = np.mean(distances) if distances else 0\n",
    "#         area = width * height\n",
    "\n",
    "#         features = np.array([width, height, aspect_ratio, area, avg_distance]).reshape(1, -1)\n",
    "#         features_scaled = scaler_scoring.transform(features)\n",
    "\n",
    "#         # Function to round score to the closest 0.5\n",
    "#         def round_to_closest_half(score):\n",
    "#             return round(score * 2) / 2\n",
    "\n",
    "#         # Step 4: Predict Score\n",
    "#         score = scoring_model.predict(features_scaled).flatten()[0]\n",
    "#         rounded_score = round_to_closest_half(score)\n",
    "#         frame_scores.append(rounded_score)\n",
    "\n",
    "#         # Step 5: Annotate Frame\n",
    "#         label = f\"{exercise_labels_inv[exercise_class]} (Score: {rounded_score:.2f})\"\n",
    "#         cv2.putText(frame, label, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "#     frame_count += 1\n",
    "#     cv2.imshow('Video', frame)\n",
    "\n",
    "#     # Press 'q' to exit\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "#     average_scores = np.mean(frame_scores)\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "#     average_scores = np.mean(frame_scores)\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - loss: 2.1763 - mae: 2.6434 - val_loss: 0.5267 - val_mae: 0.9040\n",
      "Epoch 2/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8234 - mae: 1.2357 - val_loss: 0.4884 - val_mae: 0.8679\n",
      "Epoch 3/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.6676 - mae: 1.0636 - val_loss: 0.4808 - val_mae: 0.8556\n",
      "Epoch 4/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.6113 - mae: 0.9990 - val_loss: 0.4842 - val_mae: 0.8658\n",
      "Epoch 5/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.5771 - mae: 0.9637 - val_loss: 0.4758 - val_mae: 0.8570\n",
      "Epoch 6/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.5522 - mae: 0.9331 - val_loss: 0.4754 - val_mae: 0.8571\n",
      "Epoch 7/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.5517 - mae: 0.9333 - val_loss: 0.4758 - val_mae: 0.8517\n",
      "Epoch 8/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.5376 - mae: 0.9180 - val_loss: 0.4762 - val_mae: 0.8577\n",
      "Epoch 9/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.5418 - mae: 0.9233 - val_loss: 0.4751 - val_mae: 0.8496\n",
      "Epoch 10/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.5258 - mae: 0.9034 - val_loss: 0.4758 - val_mae: 0.8515\n",
      "Epoch 11/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.5163 - mae: 0.8922 - val_loss: 0.4733 - val_mae: 0.8525\n",
      "Epoch 12/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.5132 - mae: 0.8894 - val_loss: 0.4729 - val_mae: 0.8484\n",
      "Epoch 13/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.5113 - mae: 0.8881 - val_loss: 0.4739 - val_mae: 0.8476\n",
      "Epoch 14/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.5163 - mae: 0.8928 - val_loss: 0.4853 - val_mae: 0.8476\n",
      "Epoch 15/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.5177 - mae: 0.8938 - val_loss: 0.4724 - val_mae: 0.8477\n",
      "Epoch 16/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.5061 - mae: 0.8824 - val_loss: 0.4718 - val_mae: 0.8496\n",
      "Epoch 17/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.5095 - mae: 0.8868 - val_loss: 0.4715 - val_mae: 0.8467\n",
      "Epoch 18/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.5111 - mae: 0.8859 - val_loss: 0.4720 - val_mae: 0.8459\n",
      "Epoch 19/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.5081 - mae: 0.8859 - val_loss: 0.4733 - val_mae: 0.8435\n",
      "Epoch 20/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.5011 - mae: 0.8752 - val_loss: 0.4710 - val_mae: 0.8452\n",
      "Epoch 21/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.5023 - mae: 0.8761 - val_loss: 0.4706 - val_mae: 0.8445\n",
      "Epoch 22/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.5051 - mae: 0.8823 - val_loss: 0.4712 - val_mae: 0.8453\n",
      "Epoch 23/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4993 - mae: 0.8745 - val_loss: 0.4709 - val_mae: 0.8467\n",
      "Epoch 24/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.4943 - mae: 0.8708 - val_loss: 0.4717 - val_mae: 0.8477\n",
      "Epoch 25/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.4927 - mae: 0.8657 - val_loss: 0.4723 - val_mae: 0.8521\n",
      "Epoch 26/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4856 - mae: 0.8618 - val_loss: 0.4724 - val_mae: 0.8479\n",
      "Epoch 27/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4938 - mae: 0.8686 - val_loss: 0.4731 - val_mae: 0.8433\n",
      "Epoch 28/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4858 - mae: 0.8593 - val_loss: 0.4735 - val_mae: 0.8518\n",
      "Epoch 29/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4900 - mae: 0.8642 - val_loss: 0.4747 - val_mae: 0.8402\n",
      "Epoch 30/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.4846 - mae: 0.8566 - val_loss: 0.4717 - val_mae: 0.8405\n",
      "Epoch 31/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4993 - mae: 0.8750 - val_loss: 0.4702 - val_mae: 0.8429\n",
      "Epoch 32/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.4930 - mae: 0.8673 - val_loss: 0.4696 - val_mae: 0.8456\n",
      "Epoch 33/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.4903 - mae: 0.8640 - val_loss: 0.4698 - val_mae: 0.8476\n",
      "Epoch 34/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4916 - mae: 0.8679 - val_loss: 0.4697 - val_mae: 0.8431\n",
      "Epoch 35/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.4890 - mae: 0.8644 - val_loss: 0.4686 - val_mae: 0.8444\n",
      "Epoch 36/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4856 - mae: 0.8589 - val_loss: 0.4686 - val_mae: 0.8449\n",
      "Epoch 37/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4807 - mae: 0.8541 - val_loss: 0.4683 - val_mae: 0.8382\n",
      "Epoch 38/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.4797 - mae: 0.8540 - val_loss: 0.4671 - val_mae: 0.8371\n",
      "Epoch 39/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4920 - mae: 0.8645 - val_loss: 0.4678 - val_mae: 0.8375\n",
      "Epoch 40/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4938 - mae: 0.8682 - val_loss: 0.4688 - val_mae: 0.8445\n",
      "Epoch 41/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4944 - mae: 0.8666 - val_loss: 0.4672 - val_mae: 0.8409\n",
      "Epoch 42/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4839 - mae: 0.8580 - val_loss: 0.4696 - val_mae: 0.8389\n",
      "Epoch 43/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.4836 - mae: 0.8582 - val_loss: 0.4662 - val_mae: 0.8373\n",
      "Epoch 44/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4859 - mae: 0.8598 - val_loss: 0.4669 - val_mae: 0.8378\n",
      "Epoch 45/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4893 - mae: 0.8622 - val_loss: 0.4671 - val_mae: 0.8407\n",
      "Epoch 46/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.4857 - mae: 0.8600 - val_loss: 0.4652 - val_mae: 0.8373\n",
      "Epoch 47/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4806 - mae: 0.8522 - val_loss: 0.4667 - val_mae: 0.8363\n",
      "Epoch 48/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4865 - mae: 0.8569 - val_loss: 0.4674 - val_mae: 0.8399\n",
      "Epoch 49/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.4828 - mae: 0.8546 - val_loss: 0.4668 - val_mae: 0.8378\n",
      "Epoch 50/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4722 - mae: 0.8443 - val_loss: 0.4655 - val_mae: 0.8367\n",
      "Epoch 51/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.4877 - mae: 0.8625 - val_loss: 0.4666 - val_mae: 0.8411\n",
      "Epoch 52/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.4900 - mae: 0.8641 - val_loss: 0.4679 - val_mae: 0.8387\n",
      "Epoch 53/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4845 - mae: 0.8578 - val_loss: 0.4682 - val_mae: 0.8440\n",
      "Epoch 54/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.4819 - mae: 0.8563 - val_loss: 0.4668 - val_mae: 0.8351\n",
      "Epoch 55/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4917 - mae: 0.8662 - val_loss: 0.4670 - val_mae: 0.8380\n",
      "Epoch 56/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4781 - mae: 0.8492 - val_loss: 0.4648 - val_mae: 0.8323\n",
      "Epoch 57/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.4838 - mae: 0.8556 - val_loss: 0.4667 - val_mae: 0.8408\n",
      "Epoch 58/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4742 - mae: 0.8475 - val_loss: 0.4639 - val_mae: 0.8344\n",
      "Epoch 59/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4738 - mae: 0.8430 - val_loss: 0.4668 - val_mae: 0.8417\n",
      "Epoch 60/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.4745 - mae: 0.8458 - val_loss: 0.4663 - val_mae: 0.8390\n",
      "Epoch 61/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.4882 - mae: 0.8618 - val_loss: 0.4668 - val_mae: 0.8424\n",
      "Epoch 62/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.4808 - mae: 0.8541 - val_loss: 0.4656 - val_mae: 0.8354\n",
      "Epoch 63/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4867 - mae: 0.8600 - val_loss: 0.4655 - val_mae: 0.8393\n",
      "Epoch 64/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.4753 - mae: 0.8483 - val_loss: 0.4657 - val_mae: 0.8342\n",
      "Epoch 65/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4852 - mae: 0.8565 - val_loss: 0.4661 - val_mae: 0.8344\n",
      "Epoch 66/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4746 - mae: 0.8461 - val_loss: 0.4651 - val_mae: 0.8361\n",
      "Epoch 67/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4751 - mae: 0.8456 - val_loss: 0.4647 - val_mae: 0.8374\n",
      "Epoch 68/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4810 - mae: 0.8512 - val_loss: 0.4673 - val_mae: 0.8346\n",
      "Epoch 69/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4830 - mae: 0.8544 - val_loss: 0.4630 - val_mae: 0.8360\n",
      "Epoch 70/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.4737 - mae: 0.8455 - val_loss: 0.4650 - val_mae: 0.8361\n",
      "Epoch 71/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4872 - mae: 0.8607 - val_loss: 0.4653 - val_mae: 0.8364\n",
      "Epoch 72/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.4785 - mae: 0.8501 - val_loss: 0.4667 - val_mae: 0.8414\n",
      "Epoch 73/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4802 - mae: 0.8523 - val_loss: 0.4644 - val_mae: 0.8364\n",
      "Epoch 74/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4785 - mae: 0.8511 - val_loss: 0.4649 - val_mae: 0.8333\n",
      "Epoch 75/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4742 - mae: 0.8458 - val_loss: 0.4645 - val_mae: 0.8351\n",
      "Epoch 76/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4832 - mae: 0.8548 - val_loss: 0.4655 - val_mae: 0.8350\n",
      "Epoch 77/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.4729 - mae: 0.8451 - val_loss: 0.4632 - val_mae: 0.8310\n",
      "Epoch 78/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.4806 - mae: 0.8499 - val_loss: 0.4634 - val_mae: 0.8298\n",
      "Epoch 79/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4748 - mae: 0.8454 - val_loss: 0.4635 - val_mae: 0.8318\n",
      "Epoch 80/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4737 - mae: 0.8450 - val_loss: 0.4649 - val_mae: 0.8324\n",
      "Epoch 81/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.4867 - mae: 0.8583 - val_loss: 0.4652 - val_mae: 0.8307\n",
      "Epoch 82/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4749 - mae: 0.8454 - val_loss: 0.4648 - val_mae: 0.8314\n",
      "Epoch 83/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.4705 - mae: 0.8406 - val_loss: 0.4640 - val_mae: 0.8301\n",
      "Epoch 84/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.4701 - mae: 0.8394 - val_loss: 0.4635 - val_mae: 0.8357\n",
      "Epoch 85/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4778 - mae: 0.8510 - val_loss: 0.4653 - val_mae: 0.8335\n",
      "Epoch 86/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4696 - mae: 0.8419 - val_loss: 0.4634 - val_mae: 0.8323\n",
      "Epoch 87/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.4795 - mae: 0.8535 - val_loss: 0.4628 - val_mae: 0.8305\n",
      "Epoch 88/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4719 - mae: 0.8426 - val_loss: 0.4642 - val_mae: 0.8320\n",
      "Epoch 89/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4711 - mae: 0.8412 - val_loss: 0.4635 - val_mae: 0.8319\n",
      "Epoch 90/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.4778 - mae: 0.8488 - val_loss: 0.4624 - val_mae: 0.8325\n",
      "Epoch 91/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4888 - mae: 0.8615 - val_loss: 0.4642 - val_mae: 0.8311\n",
      "Epoch 92/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.4769 - mae: 0.8481 - val_loss: 0.4634 - val_mae: 0.8317\n",
      "Epoch 93/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.4809 - mae: 0.8529 - val_loss: 0.4647 - val_mae: 0.8381\n",
      "Epoch 94/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4844 - mae: 0.8555 - val_loss: 0.4622 - val_mae: 0.8318\n",
      "Epoch 95/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.4832 - mae: 0.8559 - val_loss: 0.4615 - val_mae: 0.8312\n",
      "Epoch 96/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.4816 - mae: 0.8529 - val_loss: 0.4623 - val_mae: 0.8336\n",
      "Epoch 97/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.4783 - mae: 0.8525 - val_loss: 0.4680 - val_mae: 0.8332\n",
      "Epoch 98/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.4771 - mae: 0.8478 - val_loss: 0.4635 - val_mae: 0.8325\n",
      "Epoch 99/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.4733 - mae: 0.8421 - val_loss: 0.4608 - val_mae: 0.8325\n",
      "Epoch 100/100\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.4707 - mae: 0.8425 - val_loss: 0.4630 - val_mae: 0.8317\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error (rounded): 1.266122159090909\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_dir = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore\"\n",
    "\n",
    "# Function to extract features from a single JSON file\n",
    "def extract_features(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    rows = []\n",
    "    for segment, details in data['segments'].items():\n",
    "        score = details['score']\n",
    "        for annotation in details['annotations']:\n",
    "            bbox = annotation['bbox']\n",
    "            area = annotation['area']\n",
    "            keypoints = np.array(annotation['keypoints']).reshape(-1, 3)\n",
    "\n",
    "            # Compute features\n",
    "            width, height = bbox[2], bbox[3]\n",
    "            aspect_ratio = width / height\n",
    "            distances = [\n",
    "                np.linalg.norm(keypoints[i][:2] - keypoints[j][:2])\n",
    "                for i in range(len(keypoints))\n",
    "                for j in range(i + 1, len(keypoints))\n",
    "                if keypoints[i][2] > 0 and keypoints[j][2] > 0\n",
    "            ]\n",
    "            avg_distance = np.mean(distances) if distances else 0\n",
    "\n",
    "            # Append row\n",
    "            rows.append({\n",
    "                'segment': segment,\n",
    "                'score': score,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'area': area,\n",
    "                'avg_distance': avg_distance,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Aggregate all data\n",
    "all_data = pd.DataFrame()\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_dir, file_name)\n",
    "        df = extract_features(file_path)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# Prepare the dataset\n",
    "X = all_data[['width', 'height', 'aspect_ratio', 'area', 'avg_distance']].values\n",
    "y = all_data['score'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scoring_scaler.pkl\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the DNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "def round_to_nearest_half(x):\n",
    "    return round(x * 2) / 2\n",
    "\n",
    "\n",
    "# Round predictions to the nearest 0.5\n",
    "y_pred_rounded = [round_to_nearest_half(score) for score in y_pred]\n",
    "\n",
    "# Calculate MSE for the rounded predictions\n",
    "mse = mean_squared_error(y_test, y_pred_rounded)  # Corrected line\n",
    "print(f\"Test Mean Squared Error (rounded): {mse}\")\n",
    "\n",
    "\n",
    "\n",
    "#save the trained model\n",
    "model.save(\"scoring_model3.h5\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "# from scipy.special import softmax\n",
    "# from ultralytics import YOLO\n",
    "# import json\n",
    "\n",
    "# # Define the custom metric if necessary\n",
    "# def mse(y_true, y_pred):\n",
    "#     return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# # Register the custom function if needed\n",
    "# tf.keras.utils.get_custom_objects()[\"mse\"] = mse\n",
    "\n",
    "\n",
    "# # Load classification model and scalers\n",
    "# # Load the scoring model with the custom metric\n",
    "# scoring_model = tf.keras.models.load_model(\n",
    "#     \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model3.h5\",\n",
    "#     custom_objects={\"mse\": mse}\n",
    "# )\n",
    "\n",
    "# # Load models and scalers\n",
    "# clf = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl\")\n",
    "# scaler_classification = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl\")\n",
    "# exercise_labels_inv = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/exercise_labels.pkl\")\n",
    "# scoring_model = tf.keras.models.load_model(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_model.h5\")\n",
    "# scaler_scoring = joblib.load(\"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/scoring_scaler.pkl\")\n",
    "\n",
    "# # Load YOLO model\n",
    "# model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# # Path to the video\n",
    "# video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/DownloadVideoTest/Athletics.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# frame_count = 0\n",
    "# class_counts = Counter()\n",
    "# class_probabilities = []\n",
    "# frame_scores = []\n",
    "# output_data = []  # For saving frame-level results\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     try:\n",
    "#         # Step 1: Extract Keypoints\n",
    "#         results = model(frame)\n",
    "#         if len(results) == 0 or results[0].keypoints is None:\n",
    "#             continue\n",
    "\n",
    "#         keypoints_reshaped = results[0].keypoints.xy.cpu().numpy().reshape(-1, 3)\n",
    "#         valid_keypoints = keypoints_reshaped[keypoints_reshaped[:, 2] > 0]  # Confidence > 0\n",
    "\n",
    "#         if len(valid_keypoints) == 0:\n",
    "#             continue  # Skip frames with no valid keypoints\n",
    "\n",
    "#         # Step 2: Classify Exercise\n",
    "#         flattened_keypoints = valid_keypoints[:, :2].flatten()  # Only x, y coordinates\n",
    "#         if flattened_keypoints.size == scaler_classification.n_features_in_:\n",
    "#             normalized_keypoints = scaler_classification.transform([flattened_keypoints])\n",
    "#             exercise_class = clf.predict(normalized_keypoints)[0]\n",
    "#             exercise_class_proba = clf.predict_proba(normalized_keypoints)\n",
    "#             exercise_class_proba = softmax(exercise_class_proba, axis=1)\n",
    "#             class_counts[exercise_class] += 1\n",
    "#             class_probabilities.append(exercise_class_proba[0])\n",
    "\n",
    "#             # Step 3: Extract Scoring Features\n",
    "#             bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "#             width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "#             aspect_ratio = width / height\n",
    "#             area = width * height\n",
    "#             distances = [\n",
    "#                 np.linalg.norm(valid_keypoints[i, :2] - valid_keypoints[j, :2])\n",
    "#                 for i in range(len(valid_keypoints))\n",
    "#                 for j in range(i + 1, len(valid_keypoints))\n",
    "#             ]\n",
    "#             avg_distance = np.mean(distances) if distances else 0\n",
    "\n",
    "#             features = np.array([width, height, aspect_ratio, area, avg_distance]).reshape(1, -1)\n",
    "#             features_scaled = scaler_scoring.transform(features)\n",
    "\n",
    "#             # Function to round score to the closest 0.5\n",
    "#             def round_to_closest_half(score):\n",
    "#                 return round(score * 2) / 2\n",
    "\n",
    "#             # Step 4: Predict Score\n",
    "#             score = scoring_model.predict(features_scaled).flatten()[0]\n",
    "#             rounded_score = round_to_closest_half(score)\n",
    "#             frame_scores.append(rounded_score)\n",
    "\n",
    "#             # Save frame-level data\n",
    "#             output_data.append({\n",
    "#                 \"frame\": frame_count,\n",
    "#                 \"exercise_class\": exercise_labels_inv[exercise_class],\n",
    "#                 \"probabilities\": exercise_class_proba[0].tolist(),\n",
    "#                 \"score\": rounded_score\n",
    "#             })\n",
    "\n",
    "#             # Step 5: Annotate Frame\n",
    "#             label = f\"{exercise_labels_inv[exercise_class]} (Score: {rounded_score:.2f})\"\n",
    "#             cv2.putText(frame, label, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#         frame_count += 1\n",
    "#         cv2.imshow('Video', frame)\n",
    "\n",
    "#         # Press 'q' to exit\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing frame {frame_count}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0) if class_probabilities else []\n",
    "#     average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_score:.2f}\")\n",
    "\n",
    "# # Save results to a JSON file\n",
    "# with open(\"frame_results.json\", \"w\") as f:\n",
    "#     json.dump(output_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Aggregate Results\n",
    "# total_frames = sum(class_counts.values())\n",
    "# if total_frames > 0:\n",
    "#     class_percentages = {exercise_labels_inv[c]: (count / total_frames) * 100 for c, count in class_counts.items()}\n",
    "#     average_probabilities = np.mean(class_probabilities, axis=0)\n",
    "#     average_scores = np.mean(frame_scores)\n",
    "\n",
    "#     print(\"Class Percentages (Based on Frame Count):\")\n",
    "#     for cls, pct in class_percentages.items():\n",
    "#         print(f\"{cls}: {pct:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAverage Probabilities (Softmax Scores):\")\n",
    "#     for cls, pct in enumerate(average_probabilities):\n",
    "#         print(f\"{exercise_labels_inv[cls]}: {pct:.2f}%\")\n",
    "\n",
    "#     print(f\"\\nAverage Score for the Video: {average_scores:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699us/step - accuracy: 0.2360 - loss: 2.0303 - val_accuracy: 0.3527 - val_loss: 1.7608\n",
      "Epoch 2/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.3813 - loss: 1.7151 - val_accuracy: 0.4104 - val_loss: 1.6439\n",
      "Epoch 3/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.4172 - loss: 1.6083 - val_accuracy: 0.4478 - val_loss: 1.5565\n",
      "Epoch 4/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.4503 - loss: 1.5143 - val_accuracy: 0.4750 - val_loss: 1.4710\n",
      "Epoch 5/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.4865 - loss: 1.4209 - val_accuracy: 0.4956 - val_loss: 1.4111\n",
      "Epoch 6/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - accuracy: 0.5047 - loss: 1.3916 - val_accuracy: 0.5058 - val_loss: 1.3923\n",
      "Epoch 7/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.5314 - loss: 1.3254 - val_accuracy: 0.5214 - val_loss: 1.3459\n",
      "Epoch 8/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.5415 - loss: 1.2861 - val_accuracy: 0.5420 - val_loss: 1.2912\n",
      "Epoch 9/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.5592 - loss: 1.2506 - val_accuracy: 0.5403 - val_loss: 1.3055\n",
      "Epoch 10/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.5755 - loss: 1.2054 - val_accuracy: 0.5641 - val_loss: 1.2699\n",
      "Epoch 11/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.5981 - loss: 1.1579 - val_accuracy: 0.6035 - val_loss: 1.1909\n",
      "Epoch 12/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - accuracy: 0.6041 - loss: 1.1381 - val_accuracy: 0.5978 - val_loss: 1.1745\n",
      "Epoch 13/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.6011 - loss: 1.1314 - val_accuracy: 0.5763 - val_loss: 1.2225\n",
      "Epoch 14/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.6169 - loss: 1.0926 - val_accuracy: 0.5885 - val_loss: 1.1449\n",
      "Epoch 15/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6301 - loss: 1.0544 - val_accuracy: 0.6264 - val_loss: 1.1206\n",
      "Epoch 16/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.6354 - loss: 1.0492 - val_accuracy: 0.6408 - val_loss: 1.0591\n",
      "Epoch 17/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.6543 - loss: 1.0102 - val_accuracy: 0.6117 - val_loss: 1.0951\n",
      "Epoch 18/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.6480 - loss: 1.0172 - val_accuracy: 0.6199 - val_loss: 1.1047\n",
      "Epoch 19/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.6590 - loss: 0.9831 - val_accuracy: 0.6482 - val_loss: 1.0357\n",
      "Epoch 20/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.6656 - loss: 0.9613 - val_accuracy: 0.6496 - val_loss: 1.0116\n",
      "Epoch 21/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6702 - loss: 0.9691 - val_accuracy: 0.6609 - val_loss: 0.9673\n",
      "Epoch 22/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.6817 - loss: 0.9301 - val_accuracy: 0.6606 - val_loss: 1.0008\n",
      "Epoch 23/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.6833 - loss: 0.9207 - val_accuracy: 0.6799 - val_loss: 0.9598\n",
      "Epoch 24/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.6873 - loss: 0.9136 - val_accuracy: 0.6886 - val_loss: 0.9521\n",
      "Epoch 25/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6975 - loss: 0.8820 - val_accuracy: 0.6946 - val_loss: 0.9396\n",
      "Epoch 26/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.7013 - loss: 0.8644 - val_accuracy: 0.6773 - val_loss: 0.9469\n",
      "Epoch 27/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.6997 - loss: 0.8704 - val_accuracy: 0.6963 - val_loss: 0.9136\n",
      "Epoch 28/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.7251 - loss: 0.8280 - val_accuracy: 0.6666 - val_loss: 0.9773\n",
      "Epoch 29/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.7080 - loss: 0.8436 - val_accuracy: 0.6855 - val_loss: 0.9416\n",
      "Epoch 30/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 0.7237 - loss: 0.8177 - val_accuracy: 0.7107 - val_loss: 0.8785\n",
      "Epoch 31/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.7287 - loss: 0.7991 - val_accuracy: 0.6969 - val_loss: 0.9168\n",
      "Epoch 32/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.7353 - loss: 0.7866 - val_accuracy: 0.7119 - val_loss: 0.8676\n",
      "Epoch 33/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.7282 - loss: 0.7960 - val_accuracy: 0.7144 - val_loss: 0.8537\n",
      "Epoch 34/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.7433 - loss: 0.7668 - val_accuracy: 0.7121 - val_loss: 0.8769\n",
      "Epoch 35/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - accuracy: 0.7509 - loss: 0.7501 - val_accuracy: 0.7195 - val_loss: 0.8316\n",
      "Epoch 36/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - accuracy: 0.7544 - loss: 0.7283 - val_accuracy: 0.7158 - val_loss: 0.8533\n",
      "Epoch 37/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.7502 - loss: 0.7286 - val_accuracy: 0.7246 - val_loss: 0.8382\n",
      "Epoch 38/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - accuracy: 0.7545 - loss: 0.7165 - val_accuracy: 0.7291 - val_loss: 0.8436\n",
      "Epoch 39/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.7512 - loss: 0.7426 - val_accuracy: 0.7288 - val_loss: 0.8215\n",
      "Epoch 40/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.7631 - loss: 0.7063 - val_accuracy: 0.7416 - val_loss: 0.7974\n",
      "Epoch 41/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - accuracy: 0.7659 - loss: 0.7020 - val_accuracy: 0.7195 - val_loss: 0.8596\n",
      "Epoch 42/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.7738 - loss: 0.6801 - val_accuracy: 0.7365 - val_loss: 0.8235\n",
      "Epoch 43/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.7739 - loss: 0.6798 - val_accuracy: 0.7472 - val_loss: 0.7912\n",
      "Epoch 44/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.7837 - loss: 0.6496 - val_accuracy: 0.7354 - val_loss: 0.7986\n",
      "Epoch 45/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.7720 - loss: 0.6592 - val_accuracy: 0.7501 - val_loss: 0.7651\n",
      "Epoch 46/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.7719 - loss: 0.6744 - val_accuracy: 0.7181 - val_loss: 0.8330\n",
      "Epoch 47/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.7771 - loss: 0.6572 - val_accuracy: 0.7430 - val_loss: 0.7767\n",
      "Epoch 48/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.7907 - loss: 0.6295 - val_accuracy: 0.7407 - val_loss: 0.8161\n",
      "Epoch 49/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.7862 - loss: 0.6331 - val_accuracy: 0.7543 - val_loss: 0.7508\n",
      "Epoch 50/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.7857 - loss: 0.6369 - val_accuracy: 0.7557 - val_loss: 0.7444\n",
      "Test Accuracy: 0.76\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       475\n",
      "           1       0.59      0.77      0.67       381\n",
      "           2       0.78      0.70      0.73       348\n",
      "           3       0.83      0.60      0.69       420\n",
      "           4       0.86      0.80      0.83       394\n",
      "           5       0.75      0.85      0.80       424\n",
      "           6       0.82      0.84      0.83       533\n",
      "           7       0.66      0.60      0.63       257\n",
      "           8       0.75      0.79      0.77       301\n",
      "\n",
      "    accuracy                           0.76      3533\n",
      "   macro avg       0.76      0.75      0.75      3533\n",
      "weighted avg       0.76      0.76      0.76      3533\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x34d4bff20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import joblib\n",
    "\n",
    "# Assuming the 'load_keypoints' function and 'exercise_labels' are already defined as per previous steps\n",
    "\n",
    "# Load data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Updated argument\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Build a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))  # Output layer with softmax for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model performance\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_actual = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "target_names = [str(label) for label in encoder.categories_[0]]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_actual, y_pred_classes, target_names=target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_actual, y_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp.plot(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the directory to save the files\n",
    "save_dir = \"Testing\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the classifier\n",
    "joblib.dump(clf, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier_again.pkl\"))\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler_again.pkl\"))\n",
    "\n",
    "# Save the label mappings\n",
    "joblib.dump(exercise_labels_inv, os.path.join(save_dir, \"exercise_labels_again.pkl\"))\n",
    "\n",
    "print(\"Model, scaler, and labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689us/step - accuracy: 0.9026 - loss: 0.7883 - val_accuracy: 0.9836 - val_loss: 0.0600\n",
      "Epoch 2/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 0.9800 - loss: 0.0760 - val_accuracy: 0.9853 - val_loss: 0.0579\n",
      "Epoch 3/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9820 - loss: 0.0682 - val_accuracy: 0.9867 - val_loss: 0.0562\n",
      "Epoch 4/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.9814 - loss: 0.0679 - val_accuracy: 0.9856 - val_loss: 0.0563\n",
      "Epoch 5/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.9822 - loss: 0.0631 - val_accuracy: 0.9858 - val_loss: 0.0585\n",
      "Epoch 6/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9819 - loss: 0.0659 - val_accuracy: 0.9861 - val_loss: 0.0557\n",
      "Epoch 7/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9781 - loss: 0.0757 - val_accuracy: 0.9839 - val_loss: 0.0583\n",
      "Epoch 8/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9810 - loss: 0.0696 - val_accuracy: 0.9856 - val_loss: 0.0589\n",
      "Epoch 9/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.9807 - loss: 0.0697 - val_accuracy: 0.9850 - val_loss: 0.0583\n",
      "Epoch 10/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9803 - loss: 0.0702 - val_accuracy: 0.9870 - val_loss: 0.0552\n",
      "Epoch 11/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.9810 - loss: 0.0660 - val_accuracy: 0.9858 - val_loss: 0.0558\n",
      "Epoch 12/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9837 - loss: 0.0600 - val_accuracy: 0.9853 - val_loss: 0.0567\n",
      "Epoch 13/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9829 - loss: 0.0562 - val_accuracy: 0.9847 - val_loss: 0.0583\n",
      "Epoch 14/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.9827 - loss: 0.0612 - val_accuracy: 0.9856 - val_loss: 0.0563\n",
      "Epoch 15/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9825 - loss: 0.0606 - val_accuracy: 0.9858 - val_loss: 0.0556\n",
      "Epoch 16/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.9837 - loss: 0.0579 - val_accuracy: 0.9856 - val_loss: 0.0543\n",
      "Epoch 17/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.9824 - loss: 0.0610 - val_accuracy: 0.9850 - val_loss: 0.0529\n",
      "Epoch 18/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.9822 - loss: 0.0613 - val_accuracy: 0.9844 - val_loss: 0.0541\n",
      "Epoch 19/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9824 - loss: 0.0577 - val_accuracy: 0.9861 - val_loss: 0.0550\n",
      "Epoch 20/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9805 - loss: 0.0636 - val_accuracy: 0.9864 - val_loss: 0.0549\n",
      "Epoch 21/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9821 - loss: 0.0580 - val_accuracy: 0.9861 - val_loss: 0.0538\n",
      "Epoch 22/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9842 - loss: 0.0556 - val_accuracy: 0.9858 - val_loss: 0.0555\n",
      "Epoch 23/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9826 - loss: 0.0592 - val_accuracy: 0.9847 - val_loss: 0.0567\n",
      "Epoch 24/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.9814 - loss: 0.0599 - val_accuracy: 0.9847 - val_loss: 0.0564\n",
      "Epoch 25/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9854 - loss: 0.0488 - val_accuracy: 0.9861 - val_loss: 0.0560\n",
      "Epoch 26/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.9837 - loss: 0.0542 - val_accuracy: 0.9870 - val_loss: 0.0578\n",
      "Epoch 27/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.9834 - loss: 0.0571 - val_accuracy: 0.9856 - val_loss: 0.0561\n",
      "Epoch 28/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9820 - loss: 0.0562 - val_accuracy: 0.9861 - val_loss: 0.0546\n",
      "Epoch 29/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.9818 - loss: 0.0596 - val_accuracy: 0.9856 - val_loss: 0.0540\n",
      "Epoch 30/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.9836 - loss: 0.0562 - val_accuracy: 0.9858 - val_loss: 0.0551\n",
      "Epoch 31/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.9839 - loss: 0.0568 - val_accuracy: 0.9870 - val_loss: 0.0567\n",
      "Epoch 32/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9857 - loss: 0.0509 - val_accuracy: 0.9853 - val_loss: 0.0553\n",
      "Epoch 33/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9842 - loss: 0.0526 - val_accuracy: 0.9853 - val_loss: 0.0540\n",
      "Epoch 34/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9849 - loss: 0.0494 - val_accuracy: 0.9853 - val_loss: 0.0546\n",
      "Epoch 35/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - accuracy: 0.9854 - loss: 0.0492 - val_accuracy: 0.9861 - val_loss: 0.0554\n",
      "Epoch 36/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.9856 - loss: 0.0481 - val_accuracy: 0.9864 - val_loss: 0.0563\n",
      "Epoch 37/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9836 - loss: 0.0516 - val_accuracy: 0.9864 - val_loss: 0.0558\n",
      "Epoch 38/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9841 - loss: 0.0535 - val_accuracy: 0.9858 - val_loss: 0.0553\n",
      "Epoch 39/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9852 - loss: 0.0483 - val_accuracy: 0.9867 - val_loss: 0.0547\n",
      "Epoch 40/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9861 - loss: 0.0482 - val_accuracy: 0.9867 - val_loss: 0.0563\n",
      "Epoch 41/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - accuracy: 0.9851 - loss: 0.0506 - val_accuracy: 0.9861 - val_loss: 0.0556\n",
      "Epoch 42/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9866 - loss: 0.0465 - val_accuracy: 0.9853 - val_loss: 0.0568\n",
      "Epoch 43/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9865 - loss: 0.0441 - val_accuracy: 0.9844 - val_loss: 0.0553\n",
      "Epoch 44/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9842 - loss: 0.0510 - val_accuracy: 0.9853 - val_loss: 0.0565\n",
      "Epoch 45/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.9854 - loss: 0.0476 - val_accuracy: 0.9850 - val_loss: 0.0571\n",
      "Epoch 46/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.9848 - loss: 0.0490 - val_accuracy: 0.9856 - val_loss: 0.0553\n",
      "Epoch 47/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.9861 - loss: 0.0444 - val_accuracy: 0.9850 - val_loss: 0.0544\n",
      "Epoch 48/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9873 - loss: 0.0445 - val_accuracy: 0.9858 - val_loss: 0.0556\n",
      "Epoch 49/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 0.9870 - loss: 0.0431 - val_accuracy: 0.9861 - val_loss: 0.0578\n",
      "Epoch 50/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.9841 - loss: 0.0527 - val_accuracy: 0.9833 - val_loss: 0.0558\n",
      "Test Accuracy: 0.98\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       475\n",
      "           1       0.98      0.98      0.98       381\n",
      "           2       0.98      1.00      0.99       348\n",
      "           3       0.98      0.97      0.98       420\n",
      "           4       0.99      0.99      0.99       394\n",
      "           5       0.99      0.99      0.99       424\n",
      "           6       0.99      0.98      0.99       533\n",
      "           7       0.93      0.96      0.94       257\n",
      "           8       0.98      0.97      0.98       301\n",
      "\n",
      "    accuracy                           0.98      3533\n",
      "   macro avg       0.98      0.98      0.98      3533\n",
      "weighted avg       0.98      0.98      0.98      3533\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x36a066360>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Load the previously saved Random Forest model and scaler\n",
    "rf_model = joblib.load('/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/exercise_classifier.pkl')\n",
    "scaler = joblib.load('/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler.pkl')\n",
    "\n",
    "# Load data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Scale the data using the pre-trained scaler\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Updated argument\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Step 1: Get the Random Forest predictions (probabilities)\n",
    "train_rf_predictions = rf_model.predict_proba(X_train)\n",
    "test_rf_predictions = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Step 2: Build a neural network on top of the Random Forest outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=train_rf_predictions.shape[1], activation='relu'))  # Input layer (RF predictions)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))  # Output layer (softmax for multi-class)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the neural network\n",
    "model.fit(train_rf_predictions, y_train, epochs=50, batch_size=32, validation_data=(test_rf_predictions, y_test))\n",
    "\n",
    "# Step 4: Evaluate the model performance\n",
    "test_loss, test_accuracy = model.evaluate(test_rf_predictions, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Step 5: Predict on the test set\n",
    "y_pred = model.predict(test_rf_predictions)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_actual = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# #save the trained model\n",
    "# model.save(\"scoring_model4.h5\")\n",
    "\n",
    "# Step 6: Classification Report\n",
    "target_names = [str(label) for label in encoder.categories_[0]]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_actual, y_pred_classes, target_names=target_names))\n",
    "\n",
    "# Step 7: Confusion Matrix\n",
    "cm = confusion_matrix(y_test_actual, y_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp.plot(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the directory to save the files\n",
    "save_dir = \"Testing\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the classifier\n",
    "joblib.dump(clf, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/last_classifier.pkl\"))\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, os.path.join(save_dir, \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/scaler8.pkl\"))\n",
    "\n",
    "# Save the label mappings\n",
    "joblib.dump(exercise_labels_inv, os.path.join(save_dir, \"exercise_labels2.pkl\"))\n",
    "\n",
    "print(\"Model, scaler, and labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "importances = rf_model.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X_train shape: (14129, 34)\n",
      "Initial y_train shape: (14129, 9)\n",
      "Model input shape: (None, 9)\n",
      "Augmented X_train shape: (14129, 34)\n",
      "Augmented y_train shape: (14129, 9)\n",
      "Adjusting X_train shape to match model input...\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step\n",
      "train_rf_predictions shape: (14129, 9)\n",
      "Epoch 1/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - accuracy: 0.1555 - loss: 2.6415 - val_accuracy: 0.1543 - val_loss: 1.8502\n",
      "Epoch 2/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.2334 - loss: 2.0147 - val_accuracy: 0.0886 - val_loss: 1.9592\n",
      "Epoch 3/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.2557 - loss: 1.9678 - val_accuracy: 0.0215 - val_loss: 2.1289\n",
      "Epoch 4/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.2787 - loss: 1.9367 - val_accuracy: 0.1381 - val_loss: 2.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.2835 - loss: 1.9136 - val_accuracy: 0.2109 - val_loss: 2.0621\n",
      "Epoch 6/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.2892 - loss: 1.9009 - val_accuracy: 0.2284 - val_loss: 2.1284\n",
      "Epoch 7/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - accuracy: 0.2965 - loss: 1.8784 - val_accuracy: 0.2165 - val_loss: 2.2402\n",
      "Epoch 8/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.3106 - loss: 1.8599 - val_accuracy: 0.3244 - val_loss: 2.1980\n",
      "Epoch 9/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.3262 - loss: 1.8374 - val_accuracy: 0.2228 - val_loss: 2.2614\n",
      "Epoch 10/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 0.3332 - loss: 1.8304 - val_accuracy: 0.1690 - val_loss: 2.2025\n",
      "Epoch 11/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - accuracy: 0.3381 - loss: 1.7985 - val_accuracy: 0.1888 - val_loss: 2.1153\n",
      "Epoch 12/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.3318 - loss: 1.8125 - val_accuracy: 0.2344 - val_loss: 2.2290\n",
      "Epoch 13/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.3320 - loss: 1.7966 - val_accuracy: 0.2004 - val_loss: 2.3022\n",
      "Epoch 14/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - accuracy: 0.3508 - loss: 1.7788 - val_accuracy: 0.1013 - val_loss: 2.2157\n",
      "Epoch 15/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.3331 - loss: 1.7975 - val_accuracy: 0.2494 - val_loss: 2.2256\n",
      "Epoch 16/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - accuracy: 0.3399 - loss: 1.7807 - val_accuracy: 0.1916 - val_loss: 2.3156\n",
      "Epoch 17/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.3427 - loss: 1.7687 - val_accuracy: 0.1002 - val_loss: 2.2829\n",
      "Epoch 18/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.3459 - loss: 1.7684 - val_accuracy: 0.1843 - val_loss: 2.3162\n",
      "Epoch 19/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.3400 - loss: 1.7671 - val_accuracy: 0.1248 - val_loss: 2.2969\n",
      "Epoch 20/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.3379 - loss: 1.7586 - val_accuracy: 0.1746 - val_loss: 2.4738\n",
      "Epoch 21/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - accuracy: 0.3465 - loss: 1.7629 - val_accuracy: 0.0909 - val_loss: 2.5081\n",
      "Epoch 22/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.3561 - loss: 1.7330 - val_accuracy: 0.1087 - val_loss: 2.3027\n",
      "Epoch 23/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.3586 - loss: 1.7235 - val_accuracy: 0.1984 - val_loss: 2.3639\n",
      "Epoch 24/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.3566 - loss: 1.7181 - val_accuracy: 0.1611 - val_loss: 2.4291\n",
      "Epoch 25/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - accuracy: 0.3714 - loss: 1.7053 - val_accuracy: 0.2015 - val_loss: 2.4625\n",
      "Epoch 26/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.3530 - loss: 1.7330 - val_accuracy: 0.1885 - val_loss: 2.4378\n",
      "Epoch 27/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.3530 - loss: 1.7153 - val_accuracy: 0.2044 - val_loss: 2.5674\n",
      "Epoch 28/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.3628 - loss: 1.7185 - val_accuracy: 0.0892 - val_loss: 2.6661\n",
      "Epoch 29/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 0.3683 - loss: 1.7027 - val_accuracy: 0.1228 - val_loss: 2.5777\n",
      "Epoch 30/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.3667 - loss: 1.6961 - val_accuracy: 0.1562 - val_loss: 2.4568\n",
      "Epoch 31/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.3604 - loss: 1.7016 - val_accuracy: 0.1452 - val_loss: 2.5224\n",
      "Epoch 32/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.3722 - loss: 1.6852 - val_accuracy: 0.2080 - val_loss: 2.4069\n",
      "Epoch 33/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - accuracy: 0.3691 - loss: 1.6808 - val_accuracy: 0.1950 - val_loss: 2.4548\n",
      "Epoch 34/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - accuracy: 0.3782 - loss: 1.6723 - val_accuracy: 0.1098 - val_loss: 2.6419\n",
      "Epoch 35/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.3715 - loss: 1.6823 - val_accuracy: 0.1183 - val_loss: 2.4041\n",
      "Epoch 36/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.3822 - loss: 1.6770 - val_accuracy: 0.1888 - val_loss: 2.5311\n",
      "Epoch 37/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.3838 - loss: 1.6616 - val_accuracy: 0.2069 - val_loss: 2.5092\n",
      "Epoch 38/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.3835 - loss: 1.6624 - val_accuracy: 0.1305 - val_loss: 2.5657\n",
      "Epoch 39/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - accuracy: 0.3774 - loss: 1.6595 - val_accuracy: 0.1947 - val_loss: 2.4534\n",
      "Epoch 40/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.3850 - loss: 1.6533 - val_accuracy: 0.1667 - val_loss: 2.6821\n",
      "Epoch 41/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.3872 - loss: 1.6449 - val_accuracy: 0.0710 - val_loss: 2.8697\n",
      "Epoch 42/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.3864 - loss: 1.6447 - val_accuracy: 0.1384 - val_loss: 2.7000\n",
      "Epoch 43/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.3889 - loss: 1.6473 - val_accuracy: 0.1557 - val_loss: 2.5077\n",
      "Epoch 44/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.3916 - loss: 1.6271 - val_accuracy: 0.1438 - val_loss: 2.6790\n",
      "Epoch 45/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.3965 - loss: 1.6265 - val_accuracy: 0.1973 - val_loss: 2.7118\n",
      "Epoch 46/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.3877 - loss: 1.6528 - val_accuracy: 0.1180 - val_loss: 2.8260\n",
      "Epoch 47/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - accuracy: 0.3920 - loss: 1.6388 - val_accuracy: 0.1503 - val_loss: 2.7736\n",
      "Epoch 48/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.3982 - loss: 1.6339 - val_accuracy: 0.1127 - val_loss: 2.7395\n",
      "Epoch 49/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.3854 - loss: 1.6430 - val_accuracy: 0.1936 - val_loss: 2.5888\n",
      "Epoch 50/50\n",
      "\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.3934 - loss: 1.6258 - val_accuracy: 0.1868 - val_loss: 2.8669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.models import Sequential  # Assuming a Keras model is used\n",
    "\n",
    "# Example augmentation function\n",
    "def augment_data(X, y, target_class, noise_level=0.01):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for x, label in zip(X, y):\n",
    "        if np.array_equal(label, target_class):\n",
    "            noise = np.random.normal(0, noise_level, size=x.shape)  # Ensure shape matches\n",
    "            augmented_X.append(x + noise)\n",
    "            augmented_y.append(label)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Debugging and Fixing the Pipeline\n",
    "try:\n",
    "    # Step 1: Check shapes before augmentation\n",
    "    print(f\"Initial X_train shape: {X_train.shape}\")\n",
    "    print(f\"Initial y_train shape: {y_train.shape}\")\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "\n",
    "    # Step 2: Augment data for all target classes\n",
    "    target_classes = list(range(9))  # Classes from 0 to 8\n",
    "    for target_class in target_classes:\n",
    "        aug_X, aug_y = augment_data(X_train, y_train, target_class)\n",
    "        if aug_X.size > 0:  # Add augmented data if not empty\n",
    "            X_train = np.vstack([X_train, aug_X])\n",
    "            y_train = np.vstack([y_train, aug_y])\n",
    "\n",
    "    # Step 3: Validate shapes after augmentation\n",
    "    print(f\"Augmented X_train shape: {X_train.shape}\")\n",
    "    print(f\"Augmented y_train shape: {y_train.shape}\")\n",
    "\n",
    "    # Step 4: Ensure X_train matches the model's expected input shape\n",
    "    if X_train.shape[1] != model.input_shape[-1]:\n",
    "        print(\"Adjusting X_train shape to match model input...\")\n",
    "        X_train = X_train[:, :model.input_shape[-1]]  # Truncate extra features\n",
    "        # Alternatively, pad if needed:\n",
    "        # from keras.preprocessing.sequence import pad_sequences\n",
    "        # X_train = pad_sequences(X_train, maxlen=model.input_shape[-1], padding='post')\n",
    "\n",
    "    # Step 5: Recompute train_rf_predictions\n",
    "    train_rf_predictions = model.predict(X_train)\n",
    "    print(f\"train_rf_predictions shape: {train_rf_predictions.shape}\")\n",
    "\n",
    "    # Step 6: Validate consistency between predictions and labels\n",
    "    assert train_rf_predictions.shape[0] == y_train.shape[0], \"Mismatch detected between train_rf_predictions and y_train\"\n",
    "\n",
    "    # Step 7: Compute class weights for imbalance\n",
    "    y_train_actual = np.argmax(y_train, axis=1)  # Ensure integer labels\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_actual),\n",
    "        y=y_train_actual\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Step 8: Train the model\n",
    "    model.fit(\n",
    "        train_rf_predictions,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(test_rf_predictions, y_test),\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(f\"AssertionError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# import joblib\n",
    "\n",
    "# # Augmentation Function\n",
    "# def augment_data(X, y, excluded_classes, noise_level=0.01):\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for x, label in zip(X, y):\n",
    "#         if np.argmax(label) not in excluded_classes:\n",
    "#             noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "#             augmented_X.append(x + noise)\n",
    "#             augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# # Augment Data (Excluding Classes 5 and 6)\n",
    "# excluded_classes = [5, 6]\n",
    "# aug_X, aug_y = augment_data(X_train, y_train, excluded_classes)\n",
    "# if aug_X.size > 0:\n",
    "#     X_train = np.vstack([X_train, aug_X])\n",
    "#     y_train = np.vstack([y_train, aug_y])\n",
    "\n",
    "# print(f\"Augmented X_train shape: {X_train.shape}\")\n",
    "# print(f\"Augmented y_train shape: {y_train.shape}\")\n",
    "\n",
    "# # Custom Loss Function\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     num_classes = y_true.shape[1]\n",
    "#     penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    \n",
    "#     high_penalty_pairs = [\n",
    "#         ('Discurweper', 'Kogelstoten'),\n",
    "#         ('Kogelstoten', 'Discurweper'),\n",
    "#         ('Hoogspringen', 'Verspringen'),\n",
    "#         ('Verspringen', 'Hoogspringen'),\n",
    "#         (\"Estafette\", \"sprint\"),\n",
    "#         (\"sprint\", \"Estafette\"),\n",
    "#         (\"Hordelopen\", \"sprint\"),\n",
    "#         (\"sprint\", \"Hordelopen\")\n",
    "#     ]\n",
    "    \n",
    "#     for class1, class2 in high_penalty_pairs:\n",
    "#         idx1 = encoder.categories_[0].tolist().index(class1)\n",
    "#         idx2 = encoder.categories_[0].tolist().index(class2)\n",
    "#         penalty_matrix[idx1, idx2] = 2.0\n",
    "#         penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "#     penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "#     y_true_indices = tf.argmax(y_true, axis=1)\n",
    "#     y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "#     penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "    \n",
    "#     base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     return base_loss * penalty\n",
    "\n",
    "# # Compile the TensorFlow Model\n",
    "# model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# # Random Forest Optimization with Grid Search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     RandomForestClassifier(random_state=42),\n",
    "#     param_grid,\n",
    "#     cv=3,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "# grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Save Optimized Random Forest Model\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "# joblib.dump(best_rf_model, 'optimized_rf_model.pkl')\n",
    "\n",
    "# # Train MLP Classifier\n",
    "# mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=200)\n",
    "# mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Voting Classifier (Combine Models)\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('rf', best_rf_model),\n",
    "#     ('mlp', mlp_model)\n",
    "# ], voting='soft')\n",
    "# voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Save the Voting Classifier\n",
    "# joblib.dump(voting_clf, 'voting_classifier.pkl')\n",
    "\n",
    "# print(\"Training and model combination complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# import joblib\n",
    "\n",
    "# # Preprocessing Function\n",
    "# def preprocess_features(X, target_feature_count):\n",
    "#     \"\"\"Ensure the feature count matches the expected training set.\"\"\"\n",
    "#     return X[:, :target_feature_count]  # Adjust to expected number of features\n",
    "\n",
    "# # Augmentation Function\n",
    "# def augment_data(X, y, target_classes, noise_level=0.01):\n",
    "#     \"\"\"Augment data by adding Gaussian noise to samples of specific classes.\"\"\"\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for x, label in zip(X, y):\n",
    "#         if np.argmax(label) in target_classes:\n",
    "#             noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "#             augmented_X.append(x + noise)\n",
    "#             augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# # Handle Class Imbalance with SMOTE\n",
    "# def balance_classes(X, y):\n",
    "#     \"\"\"Balance the dataset using SMOTE for minority class oversampling.\"\"\"\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     y_indices = np.argmax(y, axis=1)\n",
    "#     X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "#     y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # Custom Loss Function with Penalty Matrix\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     \"\"\"Custom loss function applying penalties for specific misclassifications.\"\"\"\n",
    "#     num_classes = y_true.shape[1]\n",
    "#     penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    \n",
    "#     high_penalty_pairs = [\n",
    "#         ('Discurweper', 'Kogelstoten'),\n",
    "#         ('Kogelstoten', 'Discurweper'),\n",
    "#         ('Hoogspringen', 'Verspringen'),\n",
    "#         ('Verspringen', 'Hoogspringen'),\n",
    "#         (\"Estafette\", \"sprint\"),\n",
    "#         (\"sprint\", \"Estafette\"),\n",
    "#         (\"Hordelopen\", \"sprint\"),\n",
    "#         (\"sprint\", \"Hordelopen\"),\n",
    "#         (\"Speerwerpen\", \"Kogelstoten\"),\n",
    "#         (\"Kogelstoten\", \"Speerwerpen\"),\n",
    "#         (\"sprint\",\"sprint_start\")\n",
    "#     ]\n",
    "#     for class1, class2 in high_penalty_pairs:\n",
    "#         idx1 = encoder.categories_[0].tolist().index(class1)\n",
    "#         idx2 = encoder.categories_[0].tolist().index(class2)\n",
    "#         penalty_matrix[idx1, idx2] = 2.0\n",
    "#         penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "#     penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "#     y_true_indices = tf.argmax(y_true, axis=1)\n",
    "#     y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "#     penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "    \n",
    "#     base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     return base_loss * penalty\n",
    "\n",
    "# # Compile the TensorFlow Model\n",
    "# model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# # Augment and Balance Training Data\n",
    "# excluded_classes = [5, 6]\n",
    "# aug_X, aug_y = augment_data(X_train, y_train, excluded_classes)\n",
    "# X_train = np.vstack([X_train, aug_X])\n",
    "# y_train = np.vstack([y_train, aug_y])\n",
    "\n",
    "# X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "# print(f\"Augmented and balanced X_train shape: {X_train.shape}\")\n",
    "# print(f\"Augmented and balanced y_train shape: {y_train.shape}\")\n",
    "\n",
    "# # Ensure Consistent Features in X_test\n",
    "# X_test = preprocess_features(X_test, X_train.shape[1])\n",
    "\n",
    "# # Random Forest Optimization with Grid Search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [200, 300, 500],\n",
    "#     'max_depth': [10, 20, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     RandomForestClassifier(random_state=42),\n",
    "#     param_grid,\n",
    "#     cv=3,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "# grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Save Optimized Random Forest Model\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "# joblib.dump(best_rf_model, 'better_optimized_rf_model2.pkl')\n",
    "\n",
    "# # Train MLP Classifier with Improved Hyperparameters\n",
    "# mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "# mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Voting Classifier (Combine Models)\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('rf', best_rf_model),\n",
    "#     ('mlp', mlp_model)\n",
    "# ], voting='soft')\n",
    "# voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Save the Voting Classifier\n",
    "# joblib.dump(voting_clf, 'better_voting_classifier2.pkl')\n",
    "\n",
    "# # Evaluate Models\n",
    "# y_pred = voting_clf.predict(X_test)\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# print(\"Training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to activity_labels_better2.pkl\n",
      "Scaler saved to feature_scaler_better_again.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Save Labels Function\n",
    "def save_labels(label_encoder, file_name='labels_better2.pkl'):\n",
    "    \"\"\"\n",
    "    Save the label encoder or class labels for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        label_encoder: The fitted label encoder or a list of class labels.\n",
    "        file_name: The file name for saving the labels.\n",
    "    \"\"\"\n",
    "    joblib.dump(label_encoder, file_name)\n",
    "    print(f\"Labels saved to {file_name}\")\n",
    "\n",
    "# Save Scaler Function\n",
    "def save_scaler(scaler, file_name='scaler_better2.pkl'):\n",
    "    \"\"\"\n",
    "    Save the fitted scaler for consistent data preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        scaler: The fitted scaler instance (e.g., StandardScaler).\n",
    "        file_name: The file name for saving the scaler.\n",
    "    \"\"\"\n",
    "    joblib.dump(scaler, file_name)\n",
    "    print(f\"Scaler saved to {file_name}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example labels and scaler for demonstration purposes\n",
    "    class_labels = [\n",
    "    'Discurweper', 'Estafette', 'Hoogspringen', \n",
    "    'Hordelopen', 'Kogelstoten', 'Speerwerpen', \n",
    "    'sprint', 'sprint_start', 'Verspringen'\n",
    "]\n",
    "\n",
    "    label_encoder = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    \n",
    "    # Example feature data to fit the scaler\n",
    "    example_features = np.random.rand(100, 34)\n",
    "    scaler = MinMaxScaler().fit(example_features)\n",
    "\n",
    "    # Save the labels and scaler\n",
    "    save_labels(label_encoder, file_name='activity_labels_better2.pkl')\n",
    "    save_scaler(scaler, file_name='feature_scaler_better_again.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented and balanced X_train shape: (153504, 9)\n",
      "Augmented and balanced y_train shape: (153504, 9)\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  59.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  59.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 2.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 2.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  59.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 2.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 2.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 2.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 1.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 2.9min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 3.9min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 4.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 9.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=10.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=20.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=19.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=19.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=12.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=21.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=13.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=22.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=21.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 4.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 4.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 4.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 4.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 3.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 3.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       475\n",
      "           1       0.78      0.73      0.75       381\n",
      "           2       0.61      0.82      0.70       348\n",
      "           3       0.73      0.74      0.73       420\n",
      "           4       0.79      0.77      0.78       394\n",
      "           5       0.81      0.79      0.80       424\n",
      "           6       0.82      0.75      0.78       533\n",
      "           7       0.68      0.78      0.73       257\n",
      "           8       0.72      0.71      0.71       301\n",
      "\n",
      "    accuracy                           0.75      3533\n",
      "   macro avg       0.75      0.75      0.75      3533\n",
      "weighted avg       0.76      0.75      0.75      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[321  15  21  24  13  26  23  17  15]\n",
      " [  8 279  28  22   5   8  12   9  10]\n",
      " [  7  15 286  19   4   3   7   6   1]\n",
      " [ 14   9  31 309  16   3  13  14  11]\n",
      " [ 14   9  12   9 303   7  13   9  18]\n",
      " [ 15   9  27   5  14 333   3   4  14]\n",
      " [ 15   9  23  18  17   9 402  31   9]\n",
      " [  7   3  20   8   1   6   6 201   5]\n",
      " [  7  11  20   7   9  17  13   4 213]]\n",
      "Training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Preprocessing Function\n",
    "# def preprocess_features(X, target_feature_count):\n",
    "#     \"\"\"Ensure the feature count matches the expected training set.\"\"\"\n",
    "#     return X[:, :target_feature_count]  # Adjust to expected number of features\n",
    "# # Define the target classes for augmentation (for example, classes 0, 1, 2)\n",
    "# target_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "def augment_data(X, y, noise_level=0.01):\n",
    "    \"\"\"Augment data by adding Gaussian noise to all samples.\"\"\"\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for x, label in zip(X, y):\n",
    "        # Add Gaussian noise to each sample\n",
    "        noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "        augmented_X.append(x + noise)\n",
    "        augmented_y.append(label)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "\n",
    "\n",
    "# Handle Class Imbalance with SMOTE\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"Balance the dataset using SMOTE for minority class oversampling.\"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Custom Loss Function with Penalty Matrix\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"Custom loss function applying penalties for specific misclassifications.\"\"\"\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    \n",
    "    high_penalty_pairs = [\n",
    "        ('Discurweper', 'Kogelstoten'),\n",
    "        ('Kogelstoten', 'Discurweper'),\n",
    "        ('Hoogspringen', 'Verspringen'),\n",
    "        ('Verspringen', 'Hoogspringen'),\n",
    "        (\"Estafette\", \"sprint\"),\n",
    "        (\"sprint\", \"Estafette\"),\n",
    "        (\"Hordelopen\", \"sprint\"),\n",
    "        (\"sprint\", \"Hordelopen\"),\n",
    "        (\"Speerwerpen\", \"Kogelstoten\"),\n",
    "        (\"Kogelstoten\", \"Speerwerpen\"),\n",
    "        (\"sprint\",\"sprint_start\")\n",
    "    ]\n",
    "    for class1, class2 in high_penalty_pairs:\n",
    "        idx1 = encoder.categories_[0].tolist().index(class1)\n",
    "        idx2 = encoder.categories_[0].tolist().index(class2)\n",
    "        penalty_matrix[idx1, idx2] = 2.0\n",
    "        penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "    penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "    y_true_indices = tf.argmax(y_true, axis=1)\n",
    "    y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "    penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "    \n",
    "    base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return base_loss * penalty\n",
    "\n",
    "# Compile the TensorFlow Model\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Augment and Balance Training Data\n",
    "# Augment and Balance Training Data (augmenting all classes)\n",
    "aug_X, aug_y = augment_data(X_train, y_train)\n",
    "X_train = np.vstack([X_train, aug_X])\n",
    "y_train = np.vstack([y_train, aug_y])\n",
    "\n",
    "X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "print(f\"Augmented and balanced X_train shape: {X_train.shape}\")\n",
    "print(f\"Augmented and balanced y_train shape: {y_train.shape}\")\n",
    "\n",
    "def preprocess_features(X, target_feature_count):\n",
    "    \"\"\"Ensure the feature count matches the expected training set.\"\"\"\n",
    "    return X[:, :target_feature_count]  # Adjust to the expected number of features\n",
    "\n",
    "# Ensure Consistent Features in X_test\n",
    "X_test = preprocess_features(X_test, X_train.shape[1])\n",
    "\n",
    "# Ensure Consistent Features in X_test\n",
    "X_test = preprocess_features(X_test, X_train.shape[1])\n",
    "\n",
    "# Compute class weights based on the training labels\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     classes=np.unique(np.argmax(y_train, axis=1)), \n",
    "                                     y=np.argmax(y_train, axis=1))\n",
    "\n",
    "# Convert class_weights into a dictionary for RandomForestClassifier\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Initialize RandomForestClassifier with class weights\n",
    "best_rf_model = RandomForestClassifier(class_weight=class_weight_dict, random_state=42)\n",
    "\n",
    "# Random Forest Optimization with Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    best_rf_model, \n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# After training, save the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "joblib.dump(best_rf_model, 'better_optimized_rf_model3.pkl')\n",
    "\n",
    "# Train MLP Classifier with Improved Hyperparameters\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Voting Classifier (Combine Models)\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf_model),\n",
    "    ('mlp', mlp_model)\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Save the Voting Classifier\n",
    "joblib.dump(voting_clf, 'better_voting_classifier3.pkl')\n",
    "\n",
    "# Evaluate Models\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "print(\"Training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to activity_labels_better3.pkl\n",
      "Scaler saved to feature_scaler_better_again3.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Save Labels Function\n",
    "def save_labels(label_encoder, file_name='labels_better2.pkl'):\n",
    "    \"\"\"\n",
    "    Save the label encoder or class labels for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        label_encoder: The fitted label encoder or a list of class labels.\n",
    "        file_name: The file name for saving the labels.\n",
    "    \"\"\"\n",
    "    joblib.dump(label_encoder, file_name)\n",
    "    print(f\"Labels saved to {file_name}\")\n",
    "\n",
    "# Save Scaler Function\n",
    "def save_scaler(scaler, file_name='scaler_better3.pkl'):\n",
    "    \"\"\"\n",
    "    Save the fitted scaler for consistent data preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        scaler: The fitted scaler instance (e.g., StandardScaler).\n",
    "        file_name: The file name for saving the scaler.\n",
    "    \"\"\"\n",
    "    joblib.dump(scaler, file_name)\n",
    "    print(f\"Scaler saved to {file_name}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example labels and scaler for demonstration purposes\n",
    "    class_labels = [\n",
    "    'Discurweper', 'Estafette', 'Hoogspringen', \n",
    "    'Hordelopen', 'Kogelstoten', 'Speerwerpen', \n",
    "    'sprint', 'sprint_start', 'Verspringen'\n",
    "]\n",
    "\n",
    "    label_encoder = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    \n",
    "    # Example feature data to fit the scaler\n",
    "    example_features = np.random.rand(100, 34)\n",
    "    scaler = MinMaxScaler().fit(example_features)\n",
    "\n",
    "    # Save the labels and scaler\n",
    "    save_labels(label_encoder, file_name='activity_labels_better3.pkl')\n",
    "    save_scaler(scaler, file_name='feature_scaler_better_again3.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to activity_labels_better3.pkl\n",
      "Scaler saved to feature_scaler_better_again3.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Save Labels Function\n",
    "def save_labels(label_encoder, file_name='labels_better2.pkl'):\n",
    "    \"\"\"\n",
    "    Save the label encoder or class labels for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        label_encoder: The fitted label encoder or a list of class labels.\n",
    "        file_name: The file name for saving the labels.\n",
    "    \"\"\"\n",
    "    joblib.dump(label_encoder, file_name)\n",
    "    print(f\"Labels saved to {file_name}\")\n",
    "\n",
    "# Save Scaler Function\n",
    "def save_scaler(scaler, file_name='scaler_better3.pkl'):\n",
    "    \"\"\"\n",
    "    Save the fitted scaler for consistent data preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        scaler: The fitted scaler instance (e.g., StandardScaler).\n",
    "        file_name: The file name for saving the scaler.\n",
    "    \"\"\"\n",
    "    joblib.dump(scaler, file_name)\n",
    "    print(f\"Scaler saved to {file_name}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example labels and scaler for demonstration purposes\n",
    "    class_labels = [\n",
    "    'Discurweper', 'Estafette', 'Hoogspringen', \n",
    "    'Hordelopen', 'Kogelstoten', 'Speerwerpen', \n",
    "    'sprint', 'sprint_start', 'Verspringen'\n",
    "]\n",
    "\n",
    "    label_encoder = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    \n",
    "    # Example feature data to fit the scaler (replace this with actual training keypoints)\n",
    "    keypoints_data_from_training = np.random.rand(100, 34)  # This should be actual training data\n",
    "    \n",
    "    # Fit the scaler on actual keypoints data\n",
    "    scaler = MinMaxScaler().fit(keypoints_data_from_training)\n",
    "\n",
    "    # Save the labels and scaler\n",
    "    save_labels(label_encoder, file_name='activity_labels_better3.pkl')\n",
    "    save_scaler(scaler, file_name='feature_scaler_better_again3.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 652us/step - accuracy: 0.2788 - loss: 1.9464 - val_accuracy: 0.3844 - val_loss: 1.6659\n",
      "Epoch 2/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 642us/step - accuracy: 0.3738 - loss: 1.6982 - val_accuracy: 0.4104 - val_loss: 1.5488\n",
      "Epoch 3/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 677us/step - accuracy: 0.4025 - loss: 1.6144 - val_accuracy: 0.4461 - val_loss: 1.4699\n",
      "Epoch 4/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 633us/step - accuracy: 0.4262 - loss: 1.5537 - val_accuracy: 0.4682 - val_loss: 1.4311\n",
      "Epoch 5/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 625us/step - accuracy: 0.4408 - loss: 1.5154 - val_accuracy: 0.4905 - val_loss: 1.3934\n",
      "Epoch 6/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 639us/step - accuracy: 0.4486 - loss: 1.4881 - val_accuracy: 0.4990 - val_loss: 1.3818\n",
      "Epoch 7/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618us/step - accuracy: 0.4548 - loss: 1.4755 - val_accuracy: 0.4979 - val_loss: 1.3509\n",
      "Epoch 8/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618us/step - accuracy: 0.4621 - loss: 1.4556 - val_accuracy: 0.5064 - val_loss: 1.3373\n",
      "Epoch 9/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627us/step - accuracy: 0.4664 - loss: 1.4438 - val_accuracy: 0.5123 - val_loss: 1.3227\n",
      "Epoch 10/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 725us/step - accuracy: 0.4718 - loss: 1.4302 - val_accuracy: 0.5197 - val_loss: 1.3101\n",
      "Epoch 11/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 619us/step - accuracy: 0.4753 - loss: 1.4152 - val_accuracy: 0.5333 - val_loss: 1.3008\n",
      "Epoch 12/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 632us/step - accuracy: 0.4777 - loss: 1.4124 - val_accuracy: 0.5307 - val_loss: 1.2779\n",
      "Epoch 13/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 621us/step - accuracy: 0.4822 - loss: 1.4036 - val_accuracy: 0.5245 - val_loss: 1.2814\n",
      "Epoch 14/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 628us/step - accuracy: 0.4839 - loss: 1.3924 - val_accuracy: 0.5330 - val_loss: 1.2618\n",
      "Epoch 15/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617us/step - accuracy: 0.4852 - loss: 1.3900 - val_accuracy: 0.5324 - val_loss: 1.2643\n",
      "Epoch 16/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617us/step - accuracy: 0.4907 - loss: 1.3822 - val_accuracy: 0.5477 - val_loss: 1.2548\n",
      "Epoch 17/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629us/step - accuracy: 0.4885 - loss: 1.3801 - val_accuracy: 0.5451 - val_loss: 1.2460\n",
      "Epoch 18/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 654us/step - accuracy: 0.4932 - loss: 1.3665 - val_accuracy: 0.5417 - val_loss: 1.2369\n",
      "Epoch 19/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 619us/step - accuracy: 0.4928 - loss: 1.3707 - val_accuracy: 0.5468 - val_loss: 1.2294\n",
      "Epoch 20/20\n",
      "\u001b[1m4797/4797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 636us/step - accuracy: 0.4961 - loss: 1.3684 - val_accuracy: 0.5437 - val_loss: 1.2416\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - accuracy: 0.5439 - loss: 1.2143\n",
      "Test loss: 1.24161696434021, Test accuracy: 0.5437305569648743\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.55      0.63       475\n",
      "           1       0.62      0.67      0.64       381\n",
      "           2       0.54      0.77      0.64       348\n",
      "           3       0.72      0.66      0.69       420\n",
      "           4       0.66      0.78      0.72       394\n",
      "           5       0.76      0.70      0.73       424\n",
      "           6       0.83      0.67      0.74       533\n",
      "           7       0.66      0.71      0.69       257\n",
      "           8       0.64      0.70      0.67       301\n",
      "\n",
      "    accuracy                           0.68      3533\n",
      "   macro avg       0.69      0.69      0.68      3533\n",
      "weighted avg       0.70      0.68      0.68      3533\n",
      "\n",
      "Confusion Matrix for MLP:\n",
      "[[261  38  22  28  42  38   9  20  17]\n",
      " [ 11 254  50  17  11   5   6  10  17]\n",
      " [  7  21 269  22   5   8   8   7   1]\n",
      " [ 12  23  43 277  21   6   7  11  20]\n",
      " [  5  15  10   9 308   8  12   1  26]\n",
      " [ 16  16  30  11  14 295  14  10  18]\n",
      " [ 22  20  29  13  40   7 359  30  13]\n",
      " [ 11   9  24   4  13   4   6 182   4]\n",
      " [ 13  11  18   6  12  18  10   3 210]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['better_voting_classifier_with_dropout.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# TensorFlow model with Dropout\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer with 50% rate\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer with 50% rate\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Dropout layer with 50% rate\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the model\n",
    "input_shape = X_train.shape[1:]  # Assuming X_train is your training data\n",
    "num_classes = y_train.shape[1]   # Number of classes (i.e., the number of output neurons)\n",
    "model = build_model(input_shape, num_classes)\n",
    "\n",
    "# Train the TensorFlow model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "\n",
    "# If you are using the MLPClassifier\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    max_iter=300,\n",
    "    alpha=0.001,  # L2 regularization to prevent overfitting\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Evaluate MLP model\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_mlp))\n",
    "print(\"Confusion Matrix for MLP:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred_mlp))\n",
    "\n",
    "# Add voting classifier as before\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf_model),\n",
    "    ('mlp', mlp_model)\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Save the Voting Classifier\n",
    "joblib.dump(voting_clf, 'better_voting_classifier_with_dropout.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to activity_labels_better_dropout.pkl\n",
      "Scaler saved to feature_scaler_better_again_dropout.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Save Labels Function\n",
    "def save_labels(label_encoder, file_name='labels_better_dropout.pkl'):\n",
    "    \"\"\"\n",
    "    Save the label encoder or class labels for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        label_encoder: The fitted label encoder or a list of class labels.\n",
    "        file_name: The file name for saving the labels.\n",
    "    \"\"\"\n",
    "    joblib.dump(label_encoder, file_name)\n",
    "    print(f\"Labels saved to {file_name}\")\n",
    "\n",
    "# Save Scaler Function\n",
    "def save_scaler(scaler, file_name='scaler_better3_dropout.pkl'):\n",
    "    \"\"\"\n",
    "    Save the fitted scaler for consistent data preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        scaler: The fitted scaler instance (e.g., StandardScaler).\n",
    "        file_name: The file name for saving the scaler.\n",
    "    \"\"\"\n",
    "    joblib.dump(scaler, file_name)\n",
    "    print(f\"Scaler saved to {file_name}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example labels and scaler for demonstration purposes\n",
    "    class_labels = [\n",
    "    'Discurweper', 'Estafette', 'Hoogspringen', \n",
    "    'Hordelopen', 'Kogelstoten', 'Speerwerpen', \n",
    "    'sprint', 'sprint_start', 'Verspringen'\n",
    "]\n",
    "\n",
    "    label_encoder = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    \n",
    "    # Example feature data to fit the scaler (replace this with actual training keypoints)\n",
    "    keypoints_data_from_training = np.random.rand(100, 34)  # This should be actual training data\n",
    "    \n",
    "    # Fit the scaler on actual keypoints data\n",
    "    scaler = MinMaxScaler().fit(keypoints_data_from_training)\n",
    "\n",
    "    # Save the labels and scaler\n",
    "    save_labels(label_encoder, file_name='activity_labels_better_dropout.pkl')\n",
    "    save_scaler(scaler, file_name='feature_scaler_better_again_dropout.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example augmentation: Add noise to keypoints\n",
    "def augment_data(X, y, target_class, noise_level=0.01):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for x, label in zip(X, y):\n",
    "        # Compare the label index instead of using array equality\n",
    "        if np.argmax(label) == target_class:\n",
    "            noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "            augmented_X.append(x + noise)\n",
    "            augmented_y.append(label)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Apply augmentation for all classes\n",
    "target_classes = list(range(y_train.shape[1]))  # Class indices (0 to 8 for 9 classes)\n",
    "for target_class in target_classes:\n",
    "    aug_X, aug_y = augment_data(X_train, y_train, target_class)\n",
    "    if aug_X.size > 0:  # Check if aug_X is not empty\n",
    "        X_train = np.vstack([X_train, aug_X])  # Stack new samples\n",
    "        y_train = np.vstack([y_train, aug_y])  # Stack new labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  21.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  20.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  21.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  20.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  21.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  21.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  22.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   9.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   9.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  22.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  22.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  21.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  21.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  19.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  19.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  18.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  18.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  19.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  19.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  20.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  20.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  20.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  21.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  20.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  18.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  17.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  18.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  18.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  18.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  17.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  17.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  17.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  17.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  17.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  17.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  16.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  16.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  16.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  16.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  16.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  16.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  16.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  16.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  16.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  16.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  16.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  16.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  15.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  16.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  16.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  21.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  21.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  21.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  21.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  21.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  21.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  21.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  21.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  20.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  21.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  21.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  21.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  21.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  21.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  20.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  20.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  19.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  19.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  19.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  18.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  18.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  18.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  19.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  20.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  19.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  20.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  20.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  19.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  19.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  20.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  19.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  20.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  20.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  20.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  20.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  20.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  20.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  19.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  19.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  19.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  19.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  17.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  15.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  15.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  13.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optimized_rf_model.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Update the RF model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "joblib.dump(best_rf_model, 'optimized_rf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 650us/step - accuracy: 0.3612 - loss: 1.9302 - val_accuracy: 0.4854 - val_loss: 1.5274\n",
      "Epoch 2/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 644us/step - accuracy: 0.4511 - loss: 1.6390 - val_accuracy: 0.5143 - val_loss: 1.4529\n",
      "Epoch 3/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 639us/step - accuracy: 0.4700 - loss: 1.5885 - val_accuracy: 0.5352 - val_loss: 1.4313\n",
      "Epoch 4/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 623us/step - accuracy: 0.4807 - loss: 1.5602 - val_accuracy: 0.5474 - val_loss: 1.3967\n",
      "Epoch 5/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 624us/step - accuracy: 0.4847 - loss: 1.5449 - val_accuracy: 0.5480 - val_loss: 1.4116\n",
      "Epoch 6/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 654us/step - accuracy: 0.4912 - loss: 1.5287 - val_accuracy: 0.5386 - val_loss: 1.3919\n",
      "Epoch 7/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 607us/step - accuracy: 0.4949 - loss: 1.5179 - val_accuracy: 0.5556 - val_loss: 1.3718\n",
      "Epoch 8/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 631us/step - accuracy: 0.4984 - loss: 1.5085 - val_accuracy: 0.5570 - val_loss: 1.3611\n",
      "Epoch 9/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 618us/step - accuracy: 0.5011 - loss: 1.5034 - val_accuracy: 0.5715 - val_loss: 1.3399\n",
      "Epoch 10/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 613us/step - accuracy: 0.5026 - loss: 1.4967 - val_accuracy: 0.5601 - val_loss: 1.3633\n",
      "Epoch 11/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 619us/step - accuracy: 0.5051 - loss: 1.4927 - val_accuracy: 0.5667 - val_loss: 1.3338\n",
      "Epoch 12/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 616us/step - accuracy: 0.5056 - loss: 1.4872 - val_accuracy: 0.5709 - val_loss: 1.3352\n",
      "Epoch 13/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 616us/step - accuracy: 0.5078 - loss: 1.4841 - val_accuracy: 0.5534 - val_loss: 1.3331\n",
      "Epoch 14/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 616us/step - accuracy: 0.5080 - loss: 1.4833 - val_accuracy: 0.5851 - val_loss: 1.3083\n",
      "Epoch 15/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 617us/step - accuracy: 0.5099 - loss: 1.4789 - val_accuracy: 0.5667 - val_loss: 1.3210\n",
      "Epoch 16/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 616us/step - accuracy: 0.5102 - loss: 1.4766 - val_accuracy: 0.5735 - val_loss: 1.3219\n",
      "Epoch 17/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 631us/step - accuracy: 0.5115 - loss: 1.4748 - val_accuracy: 0.5735 - val_loss: 1.3195\n",
      "Epoch 18/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 642us/step - accuracy: 0.5126 - loss: 1.4739 - val_accuracy: 0.5853 - val_loss: 1.3174\n",
      "Epoch 19/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 616us/step - accuracy: 0.5134 - loss: 1.4707 - val_accuracy: 0.5749 - val_loss: 1.3066\n",
      "Epoch 20/20\n",
      "\u001b[1m38376/38376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 615us/step - accuracy: 0.5145 - loss: 1.4678 - val_accuracy: 0.5794 - val_loss: 1.2994\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y106sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_test, y_test))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y106sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# Train Random Forest with class weights\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y106sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m class_weights \u001b[39m=\u001b[39m compute_class_weight(\u001b[39m'\u001b[39;49m\u001b[39mbalanced\u001b[39;49m\u001b[39m'\u001b[39;49m, np\u001b[39m.\u001b[39;49munique(np\u001b[39m.\u001b[39;49margmax(y_train, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)), np\u001b[39m.\u001b[39;49margmax(y_train, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y106sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m class_weight_dict \u001b[39m=\u001b[39m {i: class_weights[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(class_weights))}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y106sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m best_rf_model \u001b[39m=\u001b[39m RandomForestClassifier(class_weight\u001b[39m=\u001b[39mclass_weight_dict, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:194\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m params \u001b[39m=\u001b[39m func_sig\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    195\u001b[0m params\u001b[39m.\u001b[39mapply_defaults()\n\u001b[1;32m    197\u001b[0m \u001b[39m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/inspect.py:3277\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3272\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   3273\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3274\u001b[0m \u001b[39m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3275\u001b[0m \u001b[39m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bind(args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/inspect.py:3201\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3198\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3199\u001b[0m         \u001b[39m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3200\u001b[0m         \u001b[39m# argument\u001b[39;00m\n\u001b[0;32m-> 3201\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   3202\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtoo many positional arguments\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3204\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m _VAR_POSITIONAL:\n\u001b[1;32m   3205\u001b[0m         \u001b[39m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[1;32m   3206\u001b[0m         \u001b[39m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[1;32m   3207\u001b[0m         \u001b[39m# the next phase\u001b[39;00m\n\u001b[1;32m   3208\u001b[0m         values \u001b[39m=\u001b[39m [arg_val]\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# TensorFlow model with Dropout\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Data Augmentation with Noise\n",
    "def augment_data(X, y, noise_level=0.01):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for x, label in zip(X, y):\n",
    "        noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "        augmented_X.append(x + noise)\n",
    "        augmented_y.append(label)\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Handle Class Imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_features(X, target_feature_count):\n",
    "    return X[:, :target_feature_count]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `encoder` is initialized with your label mapping\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(list(exercise_labels.keys()))\n",
    "\n",
    "# Custom loss function with updated penalty logic\n",
    "def custom_loss(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    high_penalty_pairs = [\n",
    "        ('Discurweper.json', 'Kogelstoten.json'),\n",
    "        ('Kogelstoten.json', 'Discurweper.json'),\n",
    "        ('Hoogspringen.json', 'Verspringen.json'),\n",
    "        ('Verspringen.json', 'Hoogspringen.json'),\n",
    "        (\"Estafette.json\", \"sprint.json\"),\n",
    "        (\"sprint.json\", \"Estafette.json\"),\n",
    "        (\"Hordenlopen.json\", \"sprint.json\"),\n",
    "        (\"sprint.json\", \"Hordenlopen.json\"),\n",
    "        (\"Speerwerpen.json\", \"Kogelstoten.json\"),\n",
    "        (\"Kogelstoten.json\", \"Speerwerpen.json\"),\n",
    "        (\"sprint.json\", \"sprint_start.json\")\n",
    "    ]\n",
    "    \n",
    "    for class1, class2 in high_penalty_pairs:\n",
    "        idx1 = encoder.transform([class1])[0]\n",
    "        idx2 = encoder.transform([class2])[0]\n",
    "        penalty_matrix[idx1, idx2] = 2.0\n",
    "        penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "    penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "    y_true_indices = tf.argmax(y_true, axis=1)\n",
    "    y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "    penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "    \n",
    "    base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return base_loss * penalty\n",
    "\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are available\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_model(input_shape, num_classes)\n",
    "\n",
    "# Augment and balance data\n",
    "aug_X, aug_y = augment_data(X_train, y_train)\n",
    "X_train = np.vstack([X_train, aug_X])\n",
    "y_train = np.vstack([y_train, aug_y])\n",
    "X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "# Compile model with custom loss\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train TensorFlow model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Train Random Forest with class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "    y=np.argmax(y_train, axis=1)\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "best_rf_model = RandomForestClassifier(class_weight=class_weight_dict, random_state=42)\n",
    "\n",
    "# Random Forest optimization with Grid Search\n",
    "param_grid = {'n_estimators': [200, 300, 500], 'max_depth': [10, 20, None]}\n",
    "grid_search = GridSearchCV(best_rf_model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Train MLP classifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', mlp_model)], voting='soft')\n",
    "voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Save the models\n",
    "joblib.dump(voting_clf, 'better_voting_classifier_with_dropout.pkl')\n",
    "joblib.dump(best_rf_model, 'best_rf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Save Labels Function\n",
    "def save_labels(label_encoder, file_name='labels_better_dropout.pkl'):\n",
    "    \"\"\"\n",
    "    Save the label encoder or class labels for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        label_encoder: The fitted label encoder or a list of class labels.\n",
    "        file_name: The file name for saving the labels.\n",
    "    \"\"\"\n",
    "    joblib.dump(label_encoder, file_name)\n",
    "    print(f\"Labels saved to {file_name}\")\n",
    "\n",
    "# Save Scaler Function\n",
    "def save_scaler(scaler, file_name='scaler_better3_dropout_aug.pkl'):\n",
    "    \"\"\"\n",
    "    Save the fitted scaler for consistent data preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "        scaler: The fitted scaler instance (e.g., StandardScaler).\n",
    "        file_name: The file name for saving the scaler.\n",
    "    \"\"\"\n",
    "    joblib.dump(scaler, file_name)\n",
    "    print(f\"Scaler saved to {file_name}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example labels and scaler for demonstration purposes\n",
    "    class_labels = [\n",
    "    'Discurweper', 'Estafette', 'Hoogspringen', \n",
    "    'Hordelopen', 'Kogelstoten', 'Speerwerpen', \n",
    "    'sprint', 'sprint_start', 'Verspringen'\n",
    "]\n",
    "\n",
    "    label_encoder = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    \n",
    "    # Example feature data to fit the scaler (replace this with actual training keypoints)\n",
    "    keypoints_data_from_training = np.random.rand(100, 34)  # This should be actual training data\n",
    "    \n",
    "    # Fit the scaler on actual keypoints data\n",
    "    scaler = MinMaxScaler().fit(keypoints_data_from_training)\n",
    "\n",
    "    # Save the labels and scaler\n",
    "    save_labels(label_encoder, file_name='activity_labels_better_dropout_aug.pkl')\n",
    "    save_scaler(scaler, file_name='feature_scaler_better_again_dropout_aug.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 34)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 34)\n",
      "Testing labels shape: (3533, 9)\n",
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 780us/step - accuracy: 0.1176 - loss: 13.4890 - val_accuracy: 0.1234 - val_loss: 2.6972\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  18.8s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  19.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  20.7s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  28.9s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  29.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  30.8s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  26.7s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  26.0s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  49.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  51.8s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  25.8s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  49.9s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  39.6s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  43.0s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  29.4s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  41.7s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  28.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  27.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  44.0s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.1min\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.1min\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.2min\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  40.9s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  39.6s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  53.7s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  56.5s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  46.3s\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       458\n",
      "           1       0.86      0.86      0.86       396\n",
      "           2       0.91      0.91      0.91       345\n",
      "           3       0.85      0.85      0.85       424\n",
      "           4       0.88      0.96      0.92       406\n",
      "           5       0.92      0.89      0.90       445\n",
      "           6       0.94      0.90      0.92       504\n",
      "           7       0.82      0.80      0.81       259\n",
      "           8       0.85      0.88      0.86       296\n",
      "\n",
      "    accuracy                           0.88      3533\n",
      "   macro avg       0.88      0.88      0.88      3533\n",
      "weighted avg       0.88      0.88      0.88      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[404   3   2   9  16  17   1   0   6]\n",
      " [  4 341   4  20   7   3   2   9   6]\n",
      " [  5   3 315   9   2   4   1   3   3]\n",
      " [ 11  21   4 361   3   3   9   6   6]\n",
      " [  4   2   1   4 388   0   2   0   5]\n",
      " [ 13   6   7   7   8 395   0   4   5]\n",
      " [  4   4   3   1  10   0 452  23   7]\n",
      " [  8  11   8   7   0   4   8 207   6]\n",
      " [  6   7   3   5   5   3   7   1 259]]\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 2  # 17 keypoints with x, y coordinates\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < 51:  # 17 keypoints * 3 (x, y, visibility)\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract x, y coordinates only and flatten\n",
    "            normalized_keypoints = np.array(keypoints).reshape(-1, 3)[:, :2].flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Import train_test_split and OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use 'sparse_output' instead of 'sparse'\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Proceed with preprocessing, training, and evaluation using X_train, X_test, y_train, and y_test\n",
    "\n",
    "\n",
    "# TensorFlow model with Dropout\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data Augmentation with Noise\n",
    "def augment_data(X, y, noise_level=0.01):\n",
    "    augmented_X = X + np.random.normal(0, noise_level, size=X.shape)\n",
    "    augmented_y = y.copy()\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Handle Class Imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Custom loss function with penalty matrix\n",
    "def custom_loss(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    high_penalty_pairs = [\n",
    "        (\"Discurweper.json\", \"Kogelstoten.json\"),\n",
    "        (\"Hoogspringen.json\", \"Verspringen.json\"),\n",
    "        (\"Estafette.json\", \"sprint.json\"),\n",
    "        (\"Hordenlopen.json\", \"sprint.json\"),\n",
    "        (\"Speerwerpen.json\", \"Kogelstoten.json\"),\n",
    "        (\"sprint.json\", \"sprint_start.json\")\n",
    "    ]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(exercise_labels.keys()))\n",
    "\n",
    "    for class1, class2 in high_penalty_pairs:\n",
    "        idx1 = encoder.transform([class1])[0]\n",
    "        idx2 = encoder.transform([class2])[0]\n",
    "        penalty_matrix[idx1, idx2] = 2.0\n",
    "        penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "    penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "    y_true_indices = tf.argmax(y_true, axis=1)\n",
    "    y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "    penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "\n",
    "    base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(base_loss * penalty)\n",
    "\n",
    "# Path to folder containing JSON files\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are available\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    input_shape = X_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Build and compile TensorFlow model\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "    # Augment and balance data\n",
    "    aug_X, aug_y = augment_data(X_train, y_train)\n",
    "    X_train = np.vstack([X_train, aug_X])\n",
    "    y_train = np.vstack([y_train, aug_y])\n",
    "    X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "    # Train TensorFlow model\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Compute class weights\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    classes = np.unique(y_train_indices)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_indices)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Random Forest optimization with Grid Search\n",
    "    param_grid = {'n_estimators': [200, 300, 500], 'max_depth': [10, 20, None]}\n",
    "    best_rf_model = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(best_rf_model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Train MLP classifier\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "    mlp_model.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', mlp_model)], voting='soft')\n",
    "    voting_clf.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Evaluate the models\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "    # Save the models\n",
    "    joblib.dump(voting_clf, 'better_voting_classifier_with_dropout.pkl')\n",
    "    joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "else:\n",
    "    print(\"Error: Training data not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       458\n",
      "           1       0.86      0.86      0.86       396\n",
      "           2       0.91      0.91      0.91       345\n",
      "           3       0.85      0.85      0.85       424\n",
      "           4       0.88      0.96      0.92       406\n",
      "           5       0.92      0.89      0.90       445\n",
      "           6       0.94      0.90      0.92       504\n",
      "           7       0.82      0.80      0.81       259\n",
      "           8       0.85      0.88      0.86       296\n",
      "\n",
      "    accuracy                           0.88      3533\n",
      "   macro avg       0.88      0.88      0.88      3533\n",
      "weighted avg       0.88      0.88      0.88      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[404   3   2   9  16  17   1   0   6]\n",
      " [  4 341   4  20   7   3   2   9   6]\n",
      " [  5   3 315   9   2   4   1   3   3]\n",
      " [ 11  21   4 361   3   3   9   6   6]\n",
      " [  4   2   1   4 388   0   2   0   5]\n",
      " [ 13   6   7   7   8 395   0   4   5]\n",
      " [  4   4   3   1  10   0 452  23   7]\n",
      " [  8  11   8   7   0   4   8 207   6]\n",
      " [  6   7   3   5   5   3   7   1 259]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1023 - loss: 2.4663 - val_accuracy: 0.0590 - val_loss: 2.4534\n",
      "Epoch 2/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1186 - loss: 2.4874 - val_accuracy: 0.0730 - val_loss: 2.4079\n",
      "Epoch 3/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1286 - loss: 2.4879 - val_accuracy: 0.1039 - val_loss: 2.3852\n",
      "Epoch 4/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1566 - loss: 2.4086 - val_accuracy: 0.1376 - val_loss: 2.3607\n",
      "Epoch 5/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1733 - loss: 2.3855 - val_accuracy: 0.1573 - val_loss: 2.3517\n",
      "Epoch 6/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1821 - loss: 2.3588 - val_accuracy: 0.1685 - val_loss: 2.3107\n",
      "Epoch 7/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2234 - loss: 2.3130 - val_accuracy: 0.1994 - val_loss: 2.2960\n",
      "Epoch 8/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2244 - loss: 2.3049 - val_accuracy: 0.1713 - val_loss: 2.2497\n",
      "Epoch 9/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2345 - loss: 2.2501 - val_accuracy: 0.1994 - val_loss: 2.2839\n",
      "Epoch 10/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2385 - loss: 2.2363 - val_accuracy: 0.2191 - val_loss: 2.2433\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.27      0.30        22\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.15      0.23      0.18        22\n",
      "           4       0.06      0.04      0.05        25\n",
      "           5       0.05      0.05      0.05        20\n",
      "           6       0.06      0.04      0.05        24\n",
      "           7       0.04      0.06      0.05        16\n",
      "           8       0.05      0.05      0.05        21\n",
      "           9       0.13      0.11      0.12        19\n",
      "\n",
      "    accuracy                           0.09       200\n",
      "   macro avg       0.09      0.08      0.08       200\n",
      "weighted avg       0.09      0.09      0.09       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 1 1 3 0 2 1 1 5 2]\n",
      " [1 0 2 2 1 1 0 2 2 0]\n",
      " [1 2 0 4 3 2 2 4 2 0]\n",
      " [2 2 2 5 1 3 0 4 1 2]\n",
      " [2 3 3 1 1 5 3 3 3 1]\n",
      " [1 2 1 4 1 1 4 3 0 3]\n",
      " [1 1 4 4 3 3 1 3 3 1]\n",
      " [1 1 3 3 2 1 0 1 3 1]\n",
      " [1 4 1 5 2 1 2 1 1 3]\n",
      " [2 2 3 3 2 0 3 1 1 2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.27      0.30        22\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.15      0.23      0.18        22\n",
      "           4       0.06      0.04      0.05        25\n",
      "           5       0.05      0.05      0.05        20\n",
      "           6       0.06      0.04      0.05        24\n",
      "           7       0.04      0.06      0.05        16\n",
      "           8       0.05      0.05      0.05        21\n",
      "           9       0.13      0.11      0.12        19\n",
      "\n",
      "    accuracy                           0.09       200\n",
      "   macro avg       0.09      0.08      0.08       200\n",
      "weighted avg       0.09      0.09      0.09       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 1 1 3 0 2 1 1 5 2]\n",
      " [1 0 2 2 1 1 0 2 2 0]\n",
      " [1 2 0 4 3 2 2 4 2 0]\n",
      " [2 2 2 5 1 3 0 4 1 2]\n",
      " [2 3 3 1 1 5 3 3 3 1]\n",
      " [1 2 1 4 1 1 4 3 0 3]\n",
      " [1 1 4 4 3 3 1 3 3 1]\n",
      " [1 1 3 3 2 1 0 1 3 1]\n",
      " [1 4 1 5 2 1 2 1 1 3]\n",
      " [2 2 3 3 2 0 3 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# import joblib\n",
    "\n",
    "# # Example dataset (replace with actual data)\n",
    "# num_classes = 9  # Number of classes for classification\n",
    "# num_features = 20  # Number of features in the dataset\n",
    "# X = np.random.rand(1000, num_features)\n",
    "# y = np.random.randint(0, num_classes, 1000)\n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocessing Function\n",
    "# def preprocess_features(X, target_feature_count):\n",
    "#     \"\"\"Ensure the feature count matches the expected training set.\"\"\"\n",
    "#     return X[:, :target_feature_count]  # Adjust to expected number of features\n",
    "\n",
    "# # Data Augmentation Function\n",
    "# def augment_data(X, y, noise_level=0.01):\n",
    "#     \"\"\"Augment data by adding Gaussian noise to all samples.\"\"\"\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for x, label in zip(X, y):\n",
    "#         noise = np.random.normal(0, noise_level, size=x.shape)\n",
    "#         augmented_X.append(x + noise)\n",
    "#         augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# # Handle Class Imbalance with SMOTE\n",
    "# def balance_classes(X, y):\n",
    "#     \"\"\"Balance the dataset using SMOTE for minority class oversampling.\"\"\"\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     y_indices = np.argmax(y, axis=1)\n",
    "#     X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "#     y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # Custom Loss Function with Penalty Matrix\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     \"\"\"Custom loss function applying penalties for specific misclassifications.\"\"\"\n",
    "#     penalty_matrix = np.ones((num_classes, num_classes))\n",
    "#     high_penalty_pairs = [\n",
    "#         # Define high-penalty pairs\n",
    "#         (0, 1), (1, 0), (2, 3), (3, 2),\n",
    "#     ]\n",
    "#     for class1, class2 in high_penalty_pairs:\n",
    "#         penalty_matrix[class1, class2] = 2.0\n",
    "#         penalty_matrix[class2, class1] = 2.0\n",
    "\n",
    "#     penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "#     y_true_indices = tf.argmax(y_true, axis=1)\n",
    "#     y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "#     penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "\n",
    "#     base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     return base_loss * penalty\n",
    "\n",
    "# # TensorFlow Model with Dropout\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(256, activation='relu', input_shape=(num_features,)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# # Augment and Balance Training Data\n",
    "# aug_X, aug_y = augment_data(X_train, y_train)\n",
    "# X_train = np.vstack([X_train, aug_X])\n",
    "# y_train = np.vstack([y_train, aug_y])\n",
    "# X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "# # Ensure consistent features in X_test\n",
    "# X_test = preprocess_features(X_test, X_train.shape[1])\n",
    "\n",
    "# # Train TensorFlow Model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Compute Class Weights\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "# class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# # Train and Optimize RandomForest\n",
    "# best_rf_model = RandomForestClassifier(class_weight=class_weight_dict, random_state=42)\n",
    "# param_grid = {\n",
    "#     'n_estimators': [200, 300],\n",
    "#     'max_depth': [10, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2]\n",
    "# }\n",
    "# grid_search = GridSearchCV(best_rf_model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "# joblib.dump(best_rf_model, 'optimized_rf_model.pkl')\n",
    "\n",
    "# # Train MLP Classifier\n",
    "# mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "# mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# # Voting Classifier\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('rf', best_rf_model),\n",
    "#     ('mlp', mlp_model)\n",
    "# ], voting='soft')\n",
    "# voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "# joblib.dump(voting_clf, 'voting_classifier.pkl')\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred = voting_clf.predict(X_test)\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "# # Evaluate the models\n",
    "# y_pred = voting_clf.predict(X_test)\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 34)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 34)\n",
      "Testing labels shape: (3533, 9)\n",
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - accuracy: 0.2203 - loss: 18.6281 - val_accuracy: 0.3230 - val_loss: 2.1318\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  20.1s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  20.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=200; total time=  21.6s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  31.2s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  31.3s\n",
      "[CV] END .....................max_depth=10, n_estimators=300; total time=  32.8s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  27.8s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  29.3s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  52.4s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  55.9s\n",
      "[CV] END .....................max_depth=20, n_estimators=200; total time=  28.3s\n",
      "[CV] END .....................max_depth=10, n_estimators=500; total time=  54.2s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  42.7s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  45.5s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  30.1s\n",
      "[CV] END .....................max_depth=20, n_estimators=300; total time=  43.1s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  28.8s\n",
      "[CV] END ...................max_depth=None, n_estimators=200; total time=  28.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  44.0s\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.2min\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.2min\n",
      "[CV] END .....................max_depth=20, n_estimators=500; total time= 1.1min\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  41.3s\n",
      "[CV] END ...................max_depth=None, n_estimators=300; total time=  39.9s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  53.6s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  56.5s\n",
      "[CV] END ...................max_depth=None, n_estimators=500; total time=  46.4s\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88       458\n",
      "           1       0.88      0.82      0.85       396\n",
      "           2       0.89      0.89      0.89       345\n",
      "           3       0.85      0.87      0.86       424\n",
      "           4       0.94      0.93      0.94       406\n",
      "           5       0.92      0.88      0.90       445\n",
      "           6       0.93      0.91      0.92       504\n",
      "           7       0.77      0.78      0.77       259\n",
      "           8       0.84      0.89      0.87       296\n",
      "\n",
      "    accuracy                           0.88      3533\n",
      "   macro avg       0.87      0.88      0.87      3533\n",
      "weighted avg       0.88      0.88      0.88      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[416   3   0   4   7  15   4   1   8]\n",
      " [  9 326  12  18   0   5   5  12   9]\n",
      " [  6   5 308  15   1   3   1   5   1]\n",
      " [  9  13   6 367   3   5   6   7   8]\n",
      " [  8   2   1   3 378   1   6   1   6]\n",
      " [ 22   6   6   7   3 391   0   5   5]\n",
      " [  5   4   3   0   5   0 457  25   5]\n",
      " [  6   7   9  13   1   3  10 203   7]\n",
      " [  3   6   2   4   3   3   5   6 264]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88       458\n",
      "           1       0.88      0.82      0.85       396\n",
      "           2       0.89      0.89      0.89       345\n",
      "           3       0.85      0.87      0.86       424\n",
      "           4       0.94      0.93      0.94       406\n",
      "           5       0.92      0.88      0.90       445\n",
      "           6       0.93      0.91      0.92       504\n",
      "           7       0.77      0.78      0.77       259\n",
      "           8       0.84      0.89      0.87       296\n",
      "\n",
      "    accuracy                           0.88      3533\n",
      "   macro avg       0.87      0.88      0.87      3533\n",
      "weighted avg       0.88      0.88      0.88      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[416   3   0   4   7  15   4   1   8]\n",
      " [  9 326  12  18   0   5   5  12   9]\n",
      " [  6   5 308  15   1   3   1   5   1]\n",
      " [  9  13   6 367   3   5   6   7   8]\n",
      " [  8   2   1   3 378   1   6   1   6]\n",
      " [ 22   6   6   7   3 391   0   5   5]\n",
      " [  5   4   3   0   5   0 457  25   5]\n",
      " [  6   7   9  13   1   3  10 203   7]\n",
      " [  3   6   2   4   3   3   5   6 264]]\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 2  # 17 keypoints with x, y coordinates\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < 51:  # 17 keypoints * 3 (x, y, visibility)\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract x, y coordinates only and flatten\n",
    "            normalized_keypoints = np.array(keypoints).reshape(-1, 3)[:, :2].flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Import train_test_split and OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use 'sparse_output' instead of 'sparse'\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Proceed with preprocessing, training, and evaluation using X_train, X_test, y_train, and y_test\n",
    "\n",
    "\n",
    "# TensorFlow model with Dropout\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Data Augmentation with Noise\n",
    "def augment_data(X, y, noise_level=0.01):\n",
    "    augmented_X = X + np.random.normal(0, noise_level, size=X.shape)\n",
    "    augmented_y = y.copy()\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Handle Class Imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Custom loss function with penalty matrix\n",
    "def custom_loss(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    high_penalty_pairs = [\n",
    "        (\"Discurweper.json\", \"Kogelstoten.json\"),\n",
    "        (\"Hoogspringen.json\", \"Verspringen.json\"),\n",
    "        (\"Estafette.json\", \"sprint.json\"),\n",
    "        (\"Hordenlopen.json\", \"sprint.json\"),\n",
    "        (\"Speerwerpen.json\", \"Kogelstoten.json\"),\n",
    "        (\"sprint.json\", \"sprint_start.json\")\n",
    "    ]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(exercise_labels.keys()))\n",
    "\n",
    "    for class1, class2 in high_penalty_pairs:\n",
    "        idx1 = encoder.transform([class1])[0]\n",
    "        idx2 = encoder.transform([class2])[0]\n",
    "        penalty_matrix[idx1, idx2] = 2.0\n",
    "        penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "    penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "    y_true_indices = tf.argmax(y_true, axis=1)\n",
    "    y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "    penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "\n",
    "    base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(base_loss * penalty)\n",
    "\n",
    "# Path to folder containing JSON files\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are available\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    input_shape = X_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Build and compile TensorFlow model\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "    # Augment and balance data\n",
    "    aug_X, aug_y = augment_data(X_train, y_train)\n",
    "    X_train = np.vstack([X_train, aug_X])\n",
    "    y_train = np.vstack([y_train, aug_y])\n",
    "    X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "    # Train TensorFlow model\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Compute class weights\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    classes = np.unique(y_train_indices)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_indices)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Random Forest optimization with Grid Search\n",
    "    param_grid = {'n_estimators': [200, 300, 500], 'max_depth': [10, 20, None]}\n",
    "    best_rf_model = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(best_rf_model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Train MLP classifier\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "    mlp_model.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', mlp_model)], voting='soft')\n",
    "    voting_clf.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Evaluate the models\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "    # Save the models\n",
    "    joblib.dump(voting_clf, 'better_voting_classifier_with_dropout.pkl')\n",
    "    joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "else:\n",
    "    print(\"Error: Training data not available.\")\n",
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 34)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 34)\n",
      "Testing labels shape: (3533, 9)\n",
      "Feature matrix shape: (17662, 34)\n",
      "Labels array shape: (17662,)\n",
      "Epoch 1/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2429 - loss: 8.4226 - val_accuracy: 0.3824 - val_loss: 1.9125\n",
      "Epoch 2/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.3840 - loss: 1.9526 - val_accuracy: 0.4359 - val_loss: 1.7665\n",
      "Epoch 3/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4231 - loss: 1.8093 - val_accuracy: 0.4424 - val_loss: 1.7517\n",
      "Epoch 4/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4436 - loss: 1.7507 - val_accuracy: 0.4687 - val_loss: 1.6730\n",
      "Epoch 5/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4558 - loss: 1.6960 - val_accuracy: 0.5064 - val_loss: 1.6091\n",
      "Epoch 6/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4846 - loss: 1.6086 - val_accuracy: 0.5236 - val_loss: 1.5424\n",
      "Epoch 7/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4946 - loss: 1.5795 - val_accuracy: 0.5185 - val_loss: 1.5264\n",
      "Epoch 8/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5104 - loss: 1.5156 - val_accuracy: 0.5381 - val_loss: 1.4986\n",
      "Epoch 9/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5256 - loss: 1.4725 - val_accuracy: 0.5757 - val_loss: 1.3729\n",
      "Epoch 10/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5323 - loss: 1.4491 - val_accuracy: 0.5788 - val_loss: 1.3919\n",
      "Epoch 11/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5486 - loss: 1.4106 - val_accuracy: 0.6052 - val_loss: 1.3666\n",
      "Epoch 12/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5561 - loss: 1.3818 - val_accuracy: 0.6156 - val_loss: 1.3323\n",
      "Epoch 13/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5677 - loss: 1.3430 - val_accuracy: 0.6221 - val_loss: 1.2974\n",
      "Epoch 14/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5852 - loss: 1.3024 - val_accuracy: 0.6185 - val_loss: 1.2961\n",
      "Epoch 15/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5929 - loss: 1.2912 - val_accuracy: 0.6230 - val_loss: 1.2929\n",
      "Epoch 16/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5981 - loss: 1.2555 - val_accuracy: 0.6326 - val_loss: 1.2803\n",
      "Epoch 17/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6013 - loss: 1.2461 - val_accuracy: 0.6369 - val_loss: 1.2636\n",
      "Epoch 18/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6057 - loss: 1.2349 - val_accuracy: 0.6400 - val_loss: 1.2292\n",
      "Epoch 19/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6158 - loss: 1.2042 - val_accuracy: 0.6402 - val_loss: 1.2552\n",
      "Epoch 20/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6129 - loss: 1.2232 - val_accuracy: 0.6567 - val_loss: 1.1916\n",
      "Epoch 21/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6258 - loss: 1.1850 - val_accuracy: 0.6468 - val_loss: 1.2395\n",
      "Epoch 22/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6288 - loss: 1.1789 - val_accuracy: 0.6629 - val_loss: 1.1873\n",
      "Epoch 23/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6381 - loss: 1.1437 - val_accuracy: 0.6671 - val_loss: 1.1631\n",
      "Epoch 24/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6404 - loss: 1.1403 - val_accuracy: 0.6765 - val_loss: 1.1464\n",
      "Epoch 25/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6483 - loss: 1.1104 - val_accuracy: 0.6569 - val_loss: 1.2139\n",
      "Epoch 26/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6514 - loss: 1.1101 - val_accuracy: 0.6875 - val_loss: 1.1122\n",
      "Epoch 27/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6493 - loss: 1.1149 - val_accuracy: 0.6807 - val_loss: 1.1415\n",
      "Epoch 28/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6527 - loss: 1.0912 - val_accuracy: 0.6929 - val_loss: 1.1302\n",
      "Epoch 29/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6582 - loss: 1.0768 - val_accuracy: 0.6872 - val_loss: 1.0974\n",
      "Epoch 30/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6677 - loss: 1.0562 - val_accuracy: 0.6861 - val_loss: 1.1441\n",
      "Epoch 31/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6660 - loss: 1.0639 - val_accuracy: 0.6977 - val_loss: 1.0830\n",
      "Epoch 32/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6733 - loss: 1.0312 - val_accuracy: 0.6867 - val_loss: 1.1221\n",
      "Epoch 33/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6730 - loss: 1.0244 - val_accuracy: 0.6867 - val_loss: 1.1303\n",
      "Epoch 34/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6727 - loss: 1.0344 - val_accuracy: 0.6994 - val_loss: 1.0982\n",
      "Epoch 35/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6778 - loss: 1.0228 - val_accuracy: 0.7076 - val_loss: 1.0521\n",
      "Epoch 36/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6830 - loss: 1.0142 - val_accuracy: 0.6957 - val_loss: 1.1603\n",
      "Epoch 37/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6815 - loss: 1.0148 - val_accuracy: 0.7039 - val_loss: 1.1719\n",
      "Epoch 38/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6833 - loss: 1.0163 - val_accuracy: 0.7005 - val_loss: 1.1783\n",
      "Epoch 39/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6872 - loss: 1.0035 - val_accuracy: 0.7076 - val_loss: 1.1341\n",
      "Epoch 40/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6946 - loss: 0.9759 - val_accuracy: 0.7116 - val_loss: 1.0867\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  18.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  18.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  19.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  20.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  28.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  29.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  19.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  18.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  19.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  47.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  30.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  50.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  18.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  28.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  19.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  31.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  29.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  51.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  30.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  29.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  32.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  50.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  30.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  49.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  30.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  28.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  20.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  50.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  31.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  19.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  49.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  53.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  49.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  19.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  49.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  29.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  20.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  20.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  52.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  49.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  20.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  19.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  29.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  31.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  53.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  21.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  20.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  51.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  31.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  52.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  33.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  20.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  34.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  20.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  56.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  53.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  30.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  20.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  56.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  29.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  31.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  30.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  29.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  20.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  21.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  20.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  49.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  49.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  53.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  53.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  24.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  53.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  33.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  33.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  35.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  23.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  21.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  22.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.0min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  57.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  36.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  56.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  55.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  33.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  33.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  22.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  21.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  22.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  59.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  32.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  34.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  53.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=  54.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  31.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  22.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  21.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  21.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  33.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  53.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  32.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  56.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=  55.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  32.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  22.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  24.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  56.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  59.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  36.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  34.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  56.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  34.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  21.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  21.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  55.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  58.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  34.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  32.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=  53.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  32.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  21.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  21.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  57.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  55.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  35.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=  55.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  33.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  33.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  23.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  22.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  56.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  59.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  35.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  32.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  54.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  32.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  58.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  55.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  35.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  33.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  55.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  32.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  21.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  20.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  53.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  57.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  31.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  34.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=  52.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  31.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  29.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  53.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  56.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=  52.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  43.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  30.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  46.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  29.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  30.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  30.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  47.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  48.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  32.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  34.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  51.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  52.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  32.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  49.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  51.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  48.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  31.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  46.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  33.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  46.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  32.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  46.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  30.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  29.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  44.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  29.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb Cell 50\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y122sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m best_rf_model \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y122sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(best_rf_model, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y122sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train_indices)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y122sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m best_rf_model \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/try.ipynb#Y122sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39m# Train MLP classifier\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m   1026\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    971\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m         clone(base_estimator),\n\u001b[1;32m    973\u001b[0m         X,\n\u001b[1;32m    974\u001b[0m         y,\n\u001b[1;32m    975\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    976\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    977\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    978\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    979\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    980\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    983\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params),\n\u001b[1;32m    984\u001b[0m         \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49msplitter\u001b[39m.\u001b[39;49msplit)),\n\u001b[1;32m    985\u001b[0m     )\n\u001b[1;32m    986\u001b[0m )\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   1763\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 2  # 17 keypoints with x, y coordinates\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < 34:  # 17 keypoints * 3 (x, y, visibility)\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract x, y coordinates only and flatten\n",
    "            normalized_keypoints = np.array(keypoints).reshape(-1, 3)[:, :2].flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Import train_test_split and OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use 'sparse_output' instead of 'sparse'\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Proceed with preprocessing, training, and evaluation using X_train, X_test, y_train, and y_test\n",
    "\n",
    "\n",
    "# TensorFlow model with Dropout\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(shape=input_shape),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Data Augmentation with Noise\n",
    "def augment_data(X, y, noise_level=0.01):\n",
    "    augmented_X = X + np.random.normal(0, noise_level, size=X.shape)\n",
    "    augmented_y = y.copy()\n",
    "    return np.array(augmented_X), np.array(augmented_y)\n",
    "\n",
    "# Handle Class Imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Custom loss function with penalty matrix\n",
    "def custom_loss(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    high_penalty_pairs = [\n",
    "        (\"Discurweper.json\", \"Kogelstoten.json\"),\n",
    "        (\"Hoogspringen.json\", \"Verspringen.json\"),\n",
    "        (\"Estafette.json\", \"sprint.json\"),\n",
    "        (\"Hordenlopen.json\", \"sprint.json\"),\n",
    "        (\"Speerwerpen.json\", \"Kogelstoten.json\"),\n",
    "        (\"sprint.json\", \"sprint_start.json\")\n",
    "    ]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(exercise_labels.keys()))\n",
    "\n",
    "    for class1, class2 in high_penalty_pairs:\n",
    "        idx1 = encoder.transform([class1])[0]\n",
    "        idx2 = encoder.transform([class2])[0]\n",
    "        penalty_matrix[idx1, idx2] = 2.0\n",
    "        penalty_matrix[idx2, idx1] = 2.0\n",
    "\n",
    "    penalties = tf.convert_to_tensor(penalty_matrix, dtype=tf.float32)\n",
    "    y_true_indices = tf.argmax(y_true, axis=1)\n",
    "    y_pred_indices = tf.argmax(y_pred, axis=1)\n",
    "    penalty = tf.gather_nd(penalties, tf.stack([y_true_indices, y_pred_indices], axis=1))\n",
    "\n",
    "    base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(base_loss * penalty)\n",
    "\n",
    "# Path to folder containing JSON files\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are available\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    input_shape = X_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    # Build and compile TensorFlow model\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "    # Augment and balance data\n",
    "    aug_X, aug_y = augment_data(X_train, y_train)\n",
    "    X_train = np.vstack([X_train, aug_X])\n",
    "    y_train = np.vstack([y_train, aug_y])\n",
    "    X_train, y_train = balance_classes(X_train, y_train)\n",
    "\n",
    "    # Train TensorFlow model\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "\n",
    "\n",
    "    # Compute class weights\n",
    "    y_train_indices = np.argmax(y_train, axis=1)\n",
    "    classes = np.unique(y_train_indices)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_indices)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Random Forest optimization with Grid Search\n",
    "    param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\n",
    "    best_rf_model = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(best_rf_model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train_indices)\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Train MLP classifier\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', max_iter=300)\n",
    "    mlp_model.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', mlp_model)], voting='soft')\n",
    "    voting_clf.fit(X_train, y_train_indices)\n",
    "\n",
    "    # Evaluate the models\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "    # Save the models\n",
    "    joblib.dump(voting_clf, 'better_voting_classifier_with_dropout.pkl')\n",
    "    joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "else:\n",
    "    print(\"Error: Training data not available.\")\n",
    "# Evaluate the models\n",
    "# After training and predictions\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2971 - loss: 7.6176 - val_accuracy: 0.4648 - val_loss: 1.5043\n",
      "Epoch 2/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.4764 - loss: 1.4762 - val_accuracy: 0.5508 - val_loss: 1.2980\n",
      "Epoch 3/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5186 - loss: 1.3493 - val_accuracy: 0.5562 - val_loss: 1.2564\n",
      "Epoch 4/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5415 - loss: 1.2692 - val_accuracy: 0.5692 - val_loss: 1.2113\n",
      "Epoch 5/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5690 - loss: 1.2006 - val_accuracy: 0.6187 - val_loss: 1.1204\n",
      "Epoch 6/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5918 - loss: 1.1379 - val_accuracy: 0.6185 - val_loss: 1.1069\n",
      "Epoch 7/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6064 - loss: 1.1009 - val_accuracy: 0.6388 - val_loss: 1.0590\n",
      "Epoch 8/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6250 - loss: 1.0465 - val_accuracy: 0.6547 - val_loss: 1.0337\n",
      "Epoch 9/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6375 - loss: 0.9999 - val_accuracy: 0.6640 - val_loss: 0.9688\n",
      "Epoch 10/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6526 - loss: 0.9657 - val_accuracy: 0.6688 - val_loss: 0.9697\n",
      "Epoch 11/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6655 - loss: 0.9441 - val_accuracy: 0.6867 - val_loss: 0.9470\n",
      "Epoch 12/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6750 - loss: 0.9149 - val_accuracy: 0.6844 - val_loss: 0.9269\n",
      "Epoch 13/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6800 - loss: 0.8950 - val_accuracy: 0.7170 - val_loss: 0.8543\n",
      "Epoch 14/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6910 - loss: 0.8722 - val_accuracy: 0.7144 - val_loss: 0.8906\n",
      "Epoch 15/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6942 - loss: 0.8643 - val_accuracy: 0.6954 - val_loss: 0.8903\n",
      "Epoch 16/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7028 - loss: 0.8331 - val_accuracy: 0.7218 - val_loss: 0.8884\n",
      "Epoch 17/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7084 - loss: 0.8189 - val_accuracy: 0.7269 - val_loss: 0.8507\n",
      "Epoch 18/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7173 - loss: 0.8041 - val_accuracy: 0.7303 - val_loss: 0.8282\n",
      "Epoch 19/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7213 - loss: 0.7873 - val_accuracy: 0.7059 - val_loss: 0.9223\n",
      "Epoch 20/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7209 - loss: 0.7822 - val_accuracy: 0.7342 - val_loss: 0.8549\n",
      "Epoch 21/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7304 - loss: 0.7706 - val_accuracy: 0.7339 - val_loss: 0.8407\n",
      "Epoch 22/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7340 - loss: 0.7573 - val_accuracy: 0.7447 - val_loss: 0.8176\n",
      "Epoch 23/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7450 - loss: 0.7320 - val_accuracy: 0.7489 - val_loss: 0.8312\n",
      "Epoch 24/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7453 - loss: 0.7341 - val_accuracy: 0.7421 - val_loss: 0.8275\n",
      "Epoch 25/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7479 - loss: 0.7230 - val_accuracy: 0.7498 - val_loss: 0.8177\n",
      "Epoch 26/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7412 - loss: 0.7367 - val_accuracy: 0.7679 - val_loss: 0.8228\n",
      "Epoch 27/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7426 - loss: 0.7307 - val_accuracy: 0.7523 - val_loss: 0.8416\n",
      "Epoch 28/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7441 - loss: 0.7347 - val_accuracy: 0.7719 - val_loss: 0.7917\n",
      "Epoch 29/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7536 - loss: 0.7049 - val_accuracy: 0.7591 - val_loss: 0.8250\n",
      "Epoch 30/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7545 - loss: 0.6975 - val_accuracy: 0.7549 - val_loss: 0.8232\n",
      "Epoch 31/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7648 - loss: 0.6812 - val_accuracy: 0.7489 - val_loss: 0.8043\n",
      "Epoch 32/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7609 - loss: 0.6813 - val_accuracy: 0.7518 - val_loss: 0.8169\n",
      "Epoch 33/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7645 - loss: 0.6885 - val_accuracy: 0.7617 - val_loss: 0.8133\n",
      "Epoch 34/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7628 - loss: 0.6775 - val_accuracy: 0.7676 - val_loss: 0.8208\n",
      "Epoch 35/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7609 - loss: 0.6832 - val_accuracy: 0.7716 - val_loss: 0.8421\n",
      "Epoch 36/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7703 - loss: 0.6717 - val_accuracy: 0.7724 - val_loss: 0.8032\n",
      "Epoch 37/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7702 - loss: 0.6800 - val_accuracy: 0.7518 - val_loss: 0.8823\n",
      "Epoch 38/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7665 - loss: 0.6775 - val_accuracy: 0.7877 - val_loss: 0.8321\n",
      "Epoch 39/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7787 - loss: 0.6368 - val_accuracy: 0.7586 - val_loss: 0.8712\n",
      "Epoch 40/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7723 - loss: 0.6643 - val_accuracy: 0.7671 - val_loss: 0.8216\n",
      "Epoch 41/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7803 - loss: 0.6453 - val_accuracy: 0.7733 - val_loss: 0.8361\n",
      "Epoch 42/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7750 - loss: 0.6615 - val_accuracy: 0.7772 - val_loss: 0.8229\n",
      "Epoch 43/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7823 - loss: 0.6335 - val_accuracy: 0.7673 - val_loss: 0.8432\n",
      "Epoch 44/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7786 - loss: 0.6388 - val_accuracy: 0.7823 - val_loss: 0.8215\n",
      "Epoch 45/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7853 - loss: 0.6206 - val_accuracy: 0.7767 - val_loss: 0.8277\n",
      "Epoch 46/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7812 - loss: 0.6275 - val_accuracy: 0.7679 - val_loss: 0.8215\n",
      "Epoch 47/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7863 - loss: 0.6214 - val_accuracy: 0.7886 - val_loss: 0.7785\n",
      "Epoch 48/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7896 - loss: 0.6153 - val_accuracy: 0.7925 - val_loss: 0.8232\n",
      "Epoch 49/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7928 - loss: 0.6038 - val_accuracy: 0.7874 - val_loss: 0.8450\n",
      "Epoch 50/50\n",
      "\u001b[1m1216/1216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7824 - loss: 0.6377 - val_accuracy: 0.7835 - val_loss: 0.8580\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       458\n",
      "           1       0.91      0.87      0.89       396\n",
      "           2       0.89      0.92      0.90       345\n",
      "           3       0.88      0.85      0.86       424\n",
      "           4       0.94      0.95      0.94       406\n",
      "           5       0.88      0.90      0.89       445\n",
      "           6       0.93      0.88      0.91       504\n",
      "           7       0.79      0.84      0.81       259\n",
      "           8       0.88      0.90      0.89       296\n",
      "\n",
      "    accuracy                           0.89      3533\n",
      "   macro avg       0.89      0.89      0.89      3533\n",
      "weighted avg       0.89      0.89      0.89      3533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ultralytics import YOLO\n",
    "from scipy.special import softmax\n",
    "import cv2\n",
    "\n",
    "# Define exercise-label mapping\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "# Paths\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "classification_model_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/better_voting_classifier_with_dropout9.pkl\"\n",
    "scaler_classification_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/feature_scaler_better_again_dropout9.pkl\"\n",
    "\n",
    "# Load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 2  # 17 keypoints with x, y coordinates\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < 34:\n",
    "                print(f\"Warning: Insufficient keypoints in {json_file}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            normalized_keypoints = np.array(keypoints).reshape(-1, 3)[:, :2].flatten()\n",
    "            normalized_keypoints = np.pad(\n",
    "                normalized_keypoints,\n",
    "                (0, max(0, expected_num_keypoints - len(normalized_keypoints))),\n",
    "                mode=\"constant\"\n",
    "            )[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load data\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Train-test split\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Augment and balance the data\n",
    "def augment_and_balance(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    augmented_X = X + np.random.normal(0, 0.01, X.shape)\n",
    "    X_balanced, y_indices = smote.fit_resample(np.vstack([X, augmented_X]), np.argmax(np.vstack([y, y]), axis=1))\n",
    "    y_balanced = tf.keras.utils.to_categorical(y_indices, num_classes=y.shape[1])\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "X_train, y_train = augment_and_balance(X_train, y_train)\n",
    "\n",
    "# Train TensorFlow model\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# TensorFlow training\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=20, random_state=42)\n",
    "rf_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Voting Classifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=300)\n",
    "mlp_model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf_model), ('mlp', mlp_model)], voting='soft')\n",
    "voting_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Save models\n",
    "joblib.dump(voting_clf, classification_model_path)\n",
    "scaler_classification = MinMaxScaler().fit(X)\n",
    "joblib.dump(scaler_classification, scaler_classification_path)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 51)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 51)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 51)\n",
      "Testing labels shape: (3533, 9)\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  13.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  11.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  11.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  11.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  11.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  13.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "243 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "215 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84389943 0.84528768 0.84770425\n",
      " 0.84472209 0.84554476 0.84683017 0.8428711  0.84199702 0.84333385\n",
      " 0.84271685 0.84297393 0.84497918 0.84204843 0.84405368 0.84580184\n",
      " 0.83762661 0.83983752 0.84158569 0.83958044 0.83778086 0.83968327\n",
      " 0.83958044 0.83778086 0.83968327 0.83423312 0.83469587 0.83736953\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93084477 0.93223302 0.9324901\n",
      " 0.92775978 0.92837678 0.92817111 0.91670523 0.91809348 0.91891614\n",
      " 0.92436629 0.92601162 0.92570312 0.92323513 0.92359504 0.92369788\n",
      " 0.9141344  0.91547123 0.91552265 0.9100725  0.91233482 0.91202633\n",
      " 0.9100725  0.91233482 0.91202633 0.90832434 0.909764   0.91043241\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93213019 0.93320993 0.93320993\n",
      " 0.92678287 0.9278112  0.92817111 0.91855622 0.91901897 0.9200473\n",
      " 0.92503471 0.92652579 0.92637154 0.92287521 0.92472621 0.92467479\n",
      " 0.91557407 0.91716798 0.91649956 0.91218057 0.91341457 0.91315749\n",
      " 0.91218057 0.91341457 0.91315749 0.90801584 0.90981541 0.90986683]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  27.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  29.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  29.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  20.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  20.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time=  59.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  20.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  31.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  31.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  34.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.4min\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  28.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  28.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  40.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  40.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  44.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  29.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  27.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  29.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  42.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  42.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  44.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.1min\n",
      "Best MLP Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (256, 128, 64), 'max_iter': 300, 'solver': 'adam'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       458\n",
      "           1       0.89      0.86      0.87       396\n",
      "           2       0.89      0.91      0.90       345\n",
      "           3       0.89      0.88      0.88       424\n",
      "           4       0.94      0.95      0.95       406\n",
      "           5       0.92      0.91      0.92       445\n",
      "           6       0.92      0.92      0.92       504\n",
      "           7       0.82      0.81      0.82       259\n",
      "           8       0.88      0.92      0.90       296\n",
      "\n",
      "    accuracy                           0.90      3533\n",
      "   macro avg       0.89      0.90      0.90      3533\n",
      "weighted avg       0.90      0.90      0.90      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[417   3   3   7   7  11   2   2   6]\n",
      " [  8 339  13   7   3   4   6   7   9]\n",
      " [  3   5 315   6   2   2   3   5   4]\n",
      " [  4  14  10 371   1   5   6   7   6]\n",
      " [  1   2   2   4 385   2   5   0   5]\n",
      " [ 17   6   4   5   3 406   2   0   2]\n",
      " [  2   3   2   1   4   2 462  25   3]\n",
      " [  5   8   4  12   2   2  12 211   3]\n",
      " [  3   3   2   4   1   5   5   1 272]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "# Function to load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 3  # 17 keypoints with x, y, visibility\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < expected_num_keypoints:\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Flatten the keypoints (x, y, visibility)\n",
    "            normalized_keypoints = np.array(keypoints).flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data from folder\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# One-hot encoding of labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Balance the data\n",
    "X_train_scaled, y_train_scaled = balance_classes(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "rf_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "print(f\"Best RF Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# MLPClassifier GridSearchCV\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64), (256, 128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'max_iter': [200, 300],\n",
    "    'alpha': [0.0001, 0.001]\n",
    "}\n",
    "mlp_model = MLPClassifier(random_state=42)\n",
    "mlp_grid_search = GridSearchCV(mlp_model, mlp_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "mlp_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_mlp_model = mlp_grid_search.best_estimator_\n",
    "print(f\"Best MLP Parameters: {mlp_grid_search.best_params_}\")\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', best_mlp_model)], voting='soft')\n",
    "voting_clf.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test_scaled)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Save the models and scaler\n",
    "joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "joblib.dump(best_mlp_model, 'best_mlp_model.pkl')\n",
    "joblib.dump(voting_clf, 'voting_classifier1.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 51)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 51)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 51)\n",
      "Testing labels shape: (3533, 9)\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  11.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   9.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   6.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   9.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   6.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  10.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   9.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   9.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   9.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  10.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   9.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  10.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   9.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   9.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   9.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   9.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   9.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   9.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   9.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   9.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   9.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  11.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  12.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  12.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  12.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  17.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  13.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  11.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   9.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   9.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   8.6s\n",
      "Best RF Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=   7.7s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=   8.2s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=   9.9s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  10.6s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  12.0s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  12.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=   9.3s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=   9.4s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  27.2s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=   9.4s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  10.3s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  24.0s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  31.8s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  24.1s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  28.1s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=   9.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  11.1s\n",
      "[CV] END activation=relu, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  32.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  21.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  14.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  21.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  17.6s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  30.7s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  32.8s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  30.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  12.9s\n",
      "[CV] END activation=relu, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  32.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  13.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  15.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  17.9s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  11.4s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  18.0s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  13.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  37.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  39.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  45.0s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  24.9s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  18.7s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(128, 64), max_iter=500, solver=adam; total time=  22.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  53.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.1min\n",
      "[CV] END activation=tanh, alpha=0.0001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time= 1.2min\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  49.9s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  52.6s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  47.1s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  41.3s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time=  49.7s\n",
      "[CV] END activation=tanh, alpha=0.001, early_stopping=True, hidden_layer_sizes=(256, 128, 64), max_iter=500, solver=adam; total time=  42.7s\n",
      "Best MLP Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (256, 128, 64), 'max_iter': 300, 'solver': 'adam'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90       458\n",
      "           1       0.88      0.85      0.86       396\n",
      "           2       0.90      0.90      0.90       345\n",
      "           3       0.87      0.85      0.86       424\n",
      "           4       0.95      0.95      0.95       406\n",
      "           5       0.92      0.92      0.92       445\n",
      "           6       0.94      0.90      0.92       504\n",
      "           7       0.82      0.83      0.83       259\n",
      "           8       0.86      0.90      0.88       296\n",
      "\n",
      "    accuracy                           0.90      3533\n",
      "   macro avg       0.89      0.89      0.89      3533\n",
      "weighted avg       0.90      0.90      0.90      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[420   4   2   5   5  14   1   1   6]\n",
      " [  7 338  12  11   3   2   5   7  11]\n",
      " [  4   4 310  14   3   3   1   3   3]\n",
      " [ 13  17   8 362   2   5   5   7   5]\n",
      " [  3   1   0   1 387   3   2   1   8]\n",
      " [ 16   5   5   5   1 408   1   2   2]\n",
      " [  1   4   3   3   7   3 455  24   4]\n",
      " [  6   6   4   9   0   4   9 216   5]\n",
      " [  4   7   1   5   1   3   5   3 267]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['encoder.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "# Function to load and normalize keypoints from JSON files\n",
    "def load_keypoints(json_folder, img_width=1920, img_height=1080):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 3  # 17 keypoints with x, y, visibility\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < expected_num_keypoints:\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize and flatten the keypoints\n",
    "            keypoints = np.array(keypoints)\n",
    "            keypoints[0::3] /= img_width  # Normalize x-coordinates\n",
    "            keypoints[1::3] /= img_height  # Normalize y-coordinates\n",
    "            normalized_keypoints = keypoints.flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# One-hot encoding of labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Balance the data\n",
    "X_train_scaled, y_train_scaled = balance_classes(X_train_scaled, y_train)\n",
    "\n",
    "# Compute class weights\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y),  # Use y directly as it's already a 1D array\n",
    "    y=y\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Random Forest GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight=class_weights_dict)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "rf_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "print(f\"Best RF Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# MLPClassifier GridSearchCV\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64), (256, 128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'max_iter': [300, 500],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'early_stopping': [True]\n",
    "}\n",
    "mlp_model = MLPClassifier(random_state=42)\n",
    "mlp_grid_search = GridSearchCV(mlp_model, mlp_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "mlp_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_mlp_model = mlp_grid_search.best_estimator_\n",
    "print(f\"Best MLP Parameters: {mlp_grid_search.best_params_}\")\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', best_rf_model), ('mlp', best_mlp_model)],\n",
    "    voting='soft',\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "voting_clf.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test_scaled)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Save the models and preprocessors\n",
    "joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "joblib.dump(best_mlp_model, 'best_mlp_model.pkl')\n",
    "joblib.dump(voting_clf, 'voting_classifier.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(encoder, 'encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (17662, 51)\n",
      "Labels array shape: (17662,)\n",
      "Training features shape: (14129, 51)\n",
      "Training labels shape: (14129, 9)\n",
      "Testing features shape: (3533, 51)\n",
      "Testing labels shape: (3533, 9)\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  14.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  14.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   9.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  14.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  13.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   9.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  13.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  13.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  13.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  12.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  13.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  13.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  13.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  17.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  16.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  17.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  16.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  17.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  17.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  16.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  16.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  16.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  17.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  17.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  17.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  15.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   9.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  15.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  14.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  15.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  14.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   9.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   9.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   9.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  15.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  14.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  14.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  14.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   9.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  14.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  14.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  14.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  11.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "243 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "102 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "141 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84389943 0.84528768 0.84770425\n",
      " 0.84472209 0.84554476 0.84683017 0.8428711  0.84199702 0.84333385\n",
      " 0.84271685 0.84297393 0.84497918 0.84204843 0.84405368 0.84580184\n",
      " 0.83762661 0.83983752 0.84158569 0.83958044 0.83778086 0.83968327\n",
      " 0.83958044 0.83778086 0.83968327 0.83423312 0.83469587 0.83736953\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93084477 0.93223302 0.9324901\n",
      " 0.92775978 0.92837678 0.92817111 0.91670523 0.91809348 0.91891614\n",
      " 0.92436629 0.92601162 0.92570312 0.92323513 0.92359504 0.92369788\n",
      " 0.9141344  0.91547123 0.91552265 0.9100725  0.91233482 0.91202633\n",
      " 0.9100725  0.91233482 0.91202633 0.90832434 0.909764   0.91043241\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93213019 0.93320993 0.93320993\n",
      " 0.92678287 0.9278112  0.92817111 0.91855622 0.91901897 0.9200473\n",
      " 0.92503471 0.92652579 0.92637154 0.92287521 0.92472621 0.92467479\n",
      " 0.91557407 0.91716798 0.91649956 0.91218057 0.91341457 0.91315749\n",
      " 0.91218057 0.91341457 0.91315749 0.90801584 0.90981541 0.90986683]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  18.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  18.6s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  25.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  28.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time=  55.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time=  59.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  19.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time=  58.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  29.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  28.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  30.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.1min\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  23.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  25.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  25.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  38.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  38.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  37.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  25.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=200, solver=adam; total time=  24.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  36.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  37.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(128, 64), max_iter=300, solver=adam; total time=  38.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=200, solver=adam; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(256, 128, 64), max_iter=300, solver=adam; total time= 1.1min\n",
      "Best MLP Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (256, 128, 64), 'max_iter': 300, 'solver': 'adam'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       458\n",
      "           1       0.89      0.86      0.87       396\n",
      "           2       0.89      0.91      0.90       345\n",
      "           3       0.89      0.88      0.88       424\n",
      "           4       0.94      0.95      0.95       406\n",
      "           5       0.92      0.91      0.92       445\n",
      "           6       0.92      0.92      0.92       504\n",
      "           7       0.82      0.81      0.82       259\n",
      "           8       0.88      0.92      0.90       296\n",
      "\n",
      "    accuracy                           0.90      3533\n",
      "   macro avg       0.89      0.90      0.90      3533\n",
      "weighted avg       0.90      0.90      0.90      3533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[417   3   3   7   7  11   2   2   6]\n",
      " [  8 339  13   7   3   4   6   7   9]\n",
      " [  3   5 315   6   2   2   3   5   4]\n",
      " [  4  14  10 371   1   5   6   7   6]\n",
      " [  1   2   2   4 385   2   5   0   5]\n",
      " [ 17   6   4   5   3 406   2   0   2]\n",
      " [  2   3   2   1   4   2 462  25   3]\n",
      " [  5   8   4  12   2   2  12 211   3]\n",
      " [  3   3   2   4   1   5   5   1 272]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the mapping of exercise names to labels\n",
    "exercise_labels = {\n",
    "    \"Discurweper.json\": 0,\n",
    "    \"Estafette.json\": 1,\n",
    "    \"Hoogspringen.json\": 2,\n",
    "    \"Hordenlopen.json\": 3,\n",
    "    \"Kogelstoten.json\": 4,\n",
    "    \"Speerwerpen.json\": 5,\n",
    "    \"sprint_start.json\": 6,\n",
    "    \"sprint.json\": 7,\n",
    "    \"Verspringen.json\": 8,\n",
    "}\n",
    "\n",
    "# Function to load keypoints from JSON files\n",
    "def load_keypoints(json_folder):\n",
    "    X, y = [], []\n",
    "    expected_num_keypoints = 17 * 3  # 17 keypoints with x, y, visibility\n",
    "\n",
    "    for json_file, label in exercise_labels.items():\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Warning: {json_file} does not exist in {json_folder}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for annotation in data[\"annotations\"]:\n",
    "            keypoints = annotation[\"keypoints\"]\n",
    "            if len(keypoints) < expected_num_keypoints:\n",
    "                print(f\"Warning: Annotation ID {annotation['id']} in {json_file} has insufficient keypoints. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Flatten the keypoints (x, y, visibility)\n",
    "            normalized_keypoints = np.array(keypoints).flatten()\n",
    "\n",
    "            # Pad or truncate to ensure consistent length\n",
    "            if len(normalized_keypoints) < expected_num_keypoints:\n",
    "                normalized_keypoints = np.pad(\n",
    "                    normalized_keypoints,\n",
    "                    (0, expected_num_keypoints - len(normalized_keypoints)),\n",
    "                    mode=\"constant\"\n",
    "                )\n",
    "            elif len(normalized_keypoints) > expected_num_keypoints:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_num_keypoints]\n",
    "\n",
    "            X.append(normalized_keypoints)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load keypoints data from folder\n",
    "json_folder = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints\"\n",
    "X, y = load_keypoints(json_folder)\n",
    "\n",
    "# Verify the shapes of the loaded data\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels array shape: {y.shape}\")\n",
    "\n",
    "# One-hot encoding of labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "def balance_classes(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    X_resampled, y_resampled_indices = smote.fit_resample(X, y_indices)\n",
    "    y_resampled = tf.keras.utils.to_categorical(y_resampled_indices, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Balance the data\n",
    "X_train_scaled, y_train_scaled = balance_classes(X_train_scaled, y_train)\n",
    "\n",
    "# Define custom loss function with high penalty pairs\n",
    "def custom_loss(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    penalty_matrix = np.ones((num_classes, num_classes))\n",
    "    high_penalty_pairs = [\n",
    "        (\"Discurweper.json\", \"Kogelstoten.json\"),\n",
    "        (\"Hoogspringen.json\", \"Verspringen.json\"),\n",
    "        (\"Estafette.json\", \"sprint.json\"),\n",
    "        (\"Hordenlopen.json\", \"sprint.json\"),\n",
    "        (\"Speerwerpen.json\", \"Kogelstoten.json\"),\n",
    "        (\"sprint.json\", \"sprint_start.json\"),\n",
    "        (\"sprint_start.json\", \"Hordelopen.json\")\n",
    "    ]\n",
    "    for pair in high_penalty_pairs:\n",
    "        i, j = exercise_labels[pair[0]], exercise_labels[pair[1]]\n",
    "        penalty_matrix[i, j] = 10  # Assign a high penalty value\n",
    "        penalty_matrix[j, i] = 10  # Ensure symmetry\n",
    "\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(penalty_matrix * tf.square(y_true - y_pred), axis=-1))\n",
    "    return loss\n",
    "\n",
    "# Random Forest GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "rf_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "print(f\"Best RF Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# MLPClassifier GridSearchCV\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64), (256, 128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "        'max_iter': [200, 300],\n",
    "        'alpha': [0.0001, 0.001]\n",
    "    }\n",
    "    \n",
    "mlp_model = MLPClassifier(random_state=42)\n",
    "mlp_grid_search = GridSearchCV(mlp_model, mlp_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "mlp_grid_search.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "best_mlp_model = mlp_grid_search.best_estimator_\n",
    "print(f\"Best MLP Parameters: {mlp_grid_search.best_params_}\")\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', best_rf_model), ('mlp', best_mlp_model)], voting='soft')\n",
    "voting_clf.fit(X_train_scaled, np.argmax(y_train_scaled, axis=1))\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = voting_clf.predict(X_test_scaled)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Save the models and scaler\n",
    "joblib.dump(best_rf_model, 'best_rf_model.pkl')\n",
    "joblib.dump(best_mlp_model, 'best_mlp_model.pkl')\n",
    "joblib.dump(voting_clf, 'voting_classifier1.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athletes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
