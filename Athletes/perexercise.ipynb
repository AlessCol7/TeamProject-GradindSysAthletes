{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load JSON data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Extract features (keypoints) and labels (scores)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load JSON data\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']\n",
    "        bbox = annotation['bbox']\n",
    "\n",
    "        if len(keypoints) % 3 != 0:\n",
    "            # Skip invalid keypoints\n",
    "            print(f\"Skipping invalid keypoints in segment: {segment}\")\n",
    "            continue\n",
    "\n",
    "        # Normalize keypoints using bbox dimensions\n",
    "        normalized_keypoints = []\n",
    "        for i in range(0, len(keypoints), 3):\n",
    "            x = keypoints[i] / bbox[2]  # Normalize x by width\n",
    "            y = keypoints[i + 1] / bbox[3]  # Normalize y by height\n",
    "            visibility = keypoints[i + 2]  # Visibility\n",
    "            normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "        if len(normalized_keypoints) == 51:  # Ensure all entries are of the same length\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Check consistency\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"athlete_score_predictor.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.0191 - mae: 1.4239 - val_loss: 5.5599 - val_mae: 2.1272\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5189 - mae: 1.0041 - val_loss: 3.8408 - val_mae: 1.7279\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3721 - mae: 0.9597 - val_loss: 2.8792 - val_mae: 1.4619\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2794 - mae: 0.9312 - val_loss: 1.8681 - val_mae: 1.1625\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2599 - mae: 0.8957 - val_loss: 1.5366 - val_mae: 1.0355\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1541 - mae: 0.8727 - val_loss: 1.1899 - val_mae: 0.9219\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2758 - mae: 0.9139 - val_loss: 1.2230 - val_mae: 0.9000\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1725 - mae: 0.8773 - val_loss: 1.0286 - val_mae: 0.8341\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1381 - mae: 0.8645 - val_loss: 1.0199 - val_mae: 0.8307\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2255 - mae: 0.9083 - val_loss: 0.9946 - val_mae: 0.8056\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1650 - mae: 0.8746 - val_loss: 1.0595 - val_mae: 0.8488\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1151 - mae: 0.8742 - val_loss: 1.0703 - val_mae: 0.8444\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1193 - mae: 0.8524 - val_loss: 0.9586 - val_mae: 0.7931\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1067 - mae: 0.8557 - val_loss: 0.9485 - val_mae: 0.7795\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0850 - mae: 0.8395 - val_loss: 0.8839 - val_mae: 0.7316\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1057 - mae: 0.8523 - val_loss: 0.9687 - val_mae: 0.7834\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0480 - mae: 0.8355 - val_loss: 0.9476 - val_mae: 0.7682\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0592 - mae: 0.8386 - val_loss: 0.9121 - val_mae: 0.7483\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9706 - mae: 0.8008 - val_loss: 0.9583 - val_mae: 0.7973\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0424 - mae: 0.8395 - val_loss: 0.8481 - val_mae: 0.7244\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0848 - mae: 0.8490 - val_loss: 0.9712 - val_mae: 0.8028\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0814 - mae: 0.8470 - val_loss: 1.0105 - val_mae: 0.7523\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9994 - mae: 0.8096 - val_loss: 0.9024 - val_mae: 0.7497\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1360 - mae: 0.8694 - val_loss: 0.9466 - val_mae: 0.7737\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9858 - mae: 0.8030 - val_loss: 0.9222 - val_mae: 0.7256\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9877 - mae: 0.8006 - val_loss: 1.0626 - val_mae: 0.8190\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0396 - mae: 0.8225 - val_loss: 1.0474 - val_mae: 0.8315\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0356 - mae: 0.8314 - val_loss: 0.9824 - val_mae: 0.7990\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9861 - mae: 0.7962 - val_loss: 1.1511 - val_mae: 0.9009\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0460 - mae: 0.8346 - val_loss: 0.8960 - val_mae: 0.7559\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0395 - mae: 0.8280 - val_loss: 1.0302 - val_mae: 0.8399\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0509 - mae: 0.8184 - val_loss: 0.9808 - val_mae: 0.8005\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9317 - mae: 0.7752 - val_loss: 0.9111 - val_mae: 0.7651\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0475 - mae: 0.8175 - val_loss: 0.9129 - val_mae: 0.7613\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0683 - mae: 0.8361 - val_loss: 0.8719 - val_mae: 0.7356\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9609 - mae: 0.8049 - val_loss: 0.9054 - val_mae: 0.7390\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9533 - mae: 0.7955 - val_loss: 0.8358 - val_mae: 0.7208\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9332 - mae: 0.7741 - val_loss: 0.8456 - val_mae: 0.7318\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9178 - mae: 0.7590 - val_loss: 0.7955 - val_mae: 0.7050\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9504 - mae: 0.7837 - val_loss: 0.8219 - val_mae: 0.6879\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9935 - mae: 0.7993 - val_loss: 0.7456 - val_mae: 0.6679\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9194 - mae: 0.7730 - val_loss: 0.8733 - val_mae: 0.7499\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8980 - mae: 0.7607 - val_loss: 0.7977 - val_mae: 0.6839\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9201 - mae: 0.7642 - val_loss: 0.7893 - val_mae: 0.7140\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9004 - mae: 0.7688 - val_loss: 0.9216 - val_mae: 0.7499\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8914 - mae: 0.7520 - val_loss: 0.9011 - val_mae: 0.7352\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9186 - mae: 0.7624 - val_loss: 0.8017 - val_mae: 0.6824\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8489 - mae: 0.7379 - val_loss: 0.7680 - val_mae: 0.6586\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8485 - mae: 0.7407 - val_loss: 0.7853 - val_mae: 0.6611\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9104 - mae: 0.7582 - val_loss: 0.7610 - val_mae: 0.6817\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 516.9ms\n",
      "Speed: 23.6ms preprocess, 516.9ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 344.4ms\n",
      "Speed: 4.8ms preprocess, 344.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.5ms\n",
      "Speed: 3.5ms preprocess, 417.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 430.4ms\n",
      "Speed: 2.9ms preprocess, 430.4ms inference, 13.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 371.2ms\n",
      "Speed: 32.4ms preprocess, 371.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 467.7ms\n",
      "Speed: 3.5ms preprocess, 467.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 470.8ms\n",
      "Speed: 3.9ms preprocess, 470.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 469.8ms\n",
      "Speed: 3.9ms preprocess, 469.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.5ms\n",
      "Speed: 9.1ms preprocess, 392.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.0ms\n",
      "Speed: 4.8ms preprocess, 374.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 465.1ms\n",
      "Speed: 4.5ms preprocess, 465.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.2ms\n",
      "Speed: 4.0ms preprocess, 386.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 349.6ms\n",
      "Speed: 3.4ms preprocess, 349.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.6ms\n",
      "Speed: 4.3ms preprocess, 406.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 407.1ms\n",
      "Speed: 4.1ms preprocess, 407.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 380.1ms\n",
      "Speed: 3.7ms preprocess, 380.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.2ms\n",
      "Speed: 4.2ms preprocess, 397.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 411.7ms\n",
      "Speed: 3.8ms preprocess, 411.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.4ms\n",
      "Speed: 4.1ms preprocess, 389.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.4ms\n",
      "Speed: 4.2ms preprocess, 392.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.2ms\n",
      "Speed: 3.8ms preprocess, 367.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 439.1ms\n",
      "Speed: 4.4ms preprocess, 439.1ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 351.4ms\n",
      "Speed: 2.3ms preprocess, 351.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.5ms\n",
      "Speed: 3.9ms preprocess, 367.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.8ms\n",
      "Speed: 3.9ms preprocess, 378.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 475.7ms\n",
      "Speed: 3.6ms preprocess, 475.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.2ms\n",
      "Speed: 6.9ms preprocess, 402.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 410.7ms\n",
      "Speed: 4.0ms preprocess, 410.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.7ms\n",
      "Speed: 3.9ms preprocess, 403.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 425.9ms\n",
      "Speed: 3.4ms preprocess, 425.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 358.0ms\n",
      "Speed: 4.6ms preprocess, 358.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.0ms\n",
      "Speed: 7.1ms preprocess, 400.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.7ms\n",
      "Speed: 3.6ms preprocess, 392.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.9ms\n",
      "Speed: 4.0ms preprocess, 388.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.0ms\n",
      "Speed: 4.0ms preprocess, 399.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.4ms\n",
      "Speed: 4.2ms preprocess, 394.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.2ms\n",
      "Speed: 3.6ms preprocess, 363.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 381.9ms\n",
      "Speed: 4.3ms preprocess, 381.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.9ms\n",
      "Speed: 3.6ms preprocess, 377.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 413.3ms\n",
      "Speed: 3.9ms preprocess, 413.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 412.7ms\n",
      "Speed: 1.9ms preprocess, 412.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.7ms\n",
      "Speed: 3.8ms preprocess, 379.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.5ms\n",
      "Speed: 3.6ms preprocess, 406.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 405.4ms\n",
      "Speed: 3.7ms preprocess, 405.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.1ms\n",
      "Speed: 3.6ms preprocess, 400.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.9ms\n",
      "Speed: 3.5ms preprocess, 382.9ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 428.8ms\n",
      "Speed: 3.3ms preprocess, 428.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 448.4ms\n",
      "Speed: 4.1ms preprocess, 448.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 385.4ms\n",
      "Speed: 4.1ms preprocess, 385.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.3ms\n",
      "Speed: 4.5ms preprocess, 378.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.8ms\n",
      "Speed: 3.4ms preprocess, 396.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 383.8ms\n",
      "Speed: 4.5ms preprocess, 383.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 460.0ms\n",
      "Speed: 3.8ms preprocess, 460.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.8ms\n",
      "Speed: 3.9ms preprocess, 397.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 357.7ms\n",
      "Speed: 3.6ms preprocess, 357.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 488.3ms\n",
      "Speed: 4.3ms preprocess, 488.3ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.9ms\n",
      "Speed: 5.1ms preprocess, 378.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 371.7ms\n",
      "Speed: 3.7ms preprocess, 371.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.9ms\n",
      "Speed: 4.0ms preprocess, 392.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 409.4ms\n",
      "Speed: 3.8ms preprocess, 409.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.0ms\n",
      "Speed: 3.6ms preprocess, 403.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.0ms\n",
      "Speed: 4.0ms preprocess, 363.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 362.0ms\n",
      "Speed: 3.3ms preprocess, 362.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.5ms\n",
      "Speed: 4.0ms preprocess, 375.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.1ms\n",
      "Speed: 3.6ms preprocess, 373.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.6ms\n",
      "Speed: 3.3ms preprocess, 382.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 366.4ms\n",
      "Speed: 3.3ms preprocess, 366.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 461.0ms\n",
      "Speed: 3.6ms preprocess, 461.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.2ms\n",
      "Speed: 1.6ms preprocess, 389.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.4ms\n",
      "Speed: 3.6ms preprocess, 374.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 350.0ms\n",
      "Speed: 3.4ms preprocess, 350.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 439.5ms\n",
      "Speed: 7.4ms preprocess, 439.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.0ms\n",
      "Speed: 9.1ms preprocess, 394.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 387.4ms\n",
      "Speed: 1.8ms preprocess, 387.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.5ms\n",
      "Speed: 3.5ms preprocess, 421.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.7ms\n",
      "Speed: 30.7ms preprocess, 378.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.0ms\n",
      "Speed: 3.3ms preprocess, 373.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.2ms\n",
      "Speed: 3.4ms preprocess, 403.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.7ms\n",
      "Speed: 3.7ms preprocess, 386.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.0ms\n",
      "Speed: 3.7ms preprocess, 403.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.6ms\n",
      "Speed: 4.0ms preprocess, 372.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.5ms\n",
      "Speed: 4.1ms preprocess, 386.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.0ms\n",
      "Speed: 3.1ms preprocess, 417.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.7ms\n",
      "Speed: 3.7ms preprocess, 372.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.4ms\n",
      "Speed: 4.1ms preprocess, 391.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.9ms\n",
      "Speed: 3.6ms preprocess, 377.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.2ms\n",
      "Speed: 4.1ms preprocess, 377.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 522.1ms\n",
      "Speed: 3.8ms preprocess, 522.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.0ms\n",
      "Speed: 8.3ms preprocess, 399.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.0ms\n",
      "Speed: 4.0ms preprocess, 391.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 418.9ms\n",
      "Speed: 4.0ms preprocess, 418.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 407.8ms\n",
      "Speed: 9.3ms preprocess, 407.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 368.8ms\n",
      "Speed: 3.8ms preprocess, 368.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 3.9ms preprocess, 394.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 463.3ms\n",
      "Speed: 3.4ms preprocess, 463.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.8ms\n",
      "Speed: 3.9ms preprocess, 367.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.7ms\n",
      "Speed: 2.8ms preprocess, 417.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 401.8ms\n",
      "Speed: 3.9ms preprocess, 401.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 440.3ms\n",
      "Speed: 3.7ms preprocess, 440.3ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 424.2ms\n",
      "Speed: 3.9ms preprocess, 424.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 369.3ms\n",
      "Speed: 3.7ms preprocess, 369.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 434.5ms\n",
      "Speed: 3.5ms preprocess, 434.5ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 431.7ms\n",
      "Speed: 3.4ms preprocess, 431.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 354.5ms\n",
      "Speed: 3.8ms preprocess, 354.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.9ms\n",
      "Speed: 3.7ms preprocess, 375.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.0ms\n",
      "Speed: 3.2ms preprocess, 396.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.9ms preprocess, 398.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 429.1ms\n",
      "Speed: 3.9ms preprocess, 429.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.7ms\n",
      "Speed: 8.5ms preprocess, 421.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.0ms\n",
      "Speed: 4.0ms preprocess, 375.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 405.7ms\n",
      "Speed: 3.6ms preprocess, 405.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.5ms\n",
      "Speed: 4.2ms preprocess, 391.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 408.2ms\n",
      "Speed: 3.4ms preprocess, 408.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 357.3ms\n",
      "Speed: 3.8ms preprocess, 357.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 390.2ms\n",
      "Speed: 4.2ms preprocess, 390.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.8ms\n",
      "Speed: 3.3ms preprocess, 406.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 413.4ms\n",
      "Speed: 3.9ms preprocess, 413.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 366.0ms\n",
      "Speed: 3.9ms preprocess, 366.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.3ms\n",
      "Speed: 4.2ms preprocess, 389.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 277.0ms\n",
      "Speed: 3.7ms preprocess, 277.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 393.6ms\n",
      "Speed: 3.9ms preprocess, 393.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 401.8ms\n",
      "Speed: 3.7ms preprocess, 401.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.7ms\n",
      "Speed: 4.2ms preprocess, 399.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.9ms\n",
      "Speed: 3.2ms preprocess, 370.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.3ms\n",
      "Speed: 3.6ms preprocess, 379.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.4ms\n",
      "Speed: 3.9ms preprocess, 370.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.3ms\n",
      "Speed: 4.0ms preprocess, 400.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.8ms\n",
      "Speed: 3.7ms preprocess, 389.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.8ms\n",
      "Speed: 4.0ms preprocess, 391.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.7ms\n",
      "Speed: 4.4ms preprocess, 399.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.8ms\n",
      "Speed: 3.9ms preprocess, 363.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.1ms\n",
      "Speed: 4.3ms preprocess, 370.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 416.8ms\n",
      "Speed: 3.5ms preprocess, 416.8ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.4ms\n",
      "Speed: 1.9ms preprocess, 421.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 384.3ms\n",
      "Speed: 4.2ms preprocess, 384.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.2ms\n",
      "Speed: 3.9ms preprocess, 374.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.7ms\n",
      "Speed: 4.9ms preprocess, 375.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.2ms\n",
      "Speed: 3.7ms preprocess, 397.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 360.1ms\n",
      "Speed: 3.6ms preprocess, 360.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 380.7ms\n",
      "Speed: 3.9ms preprocess, 380.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.3ms\n",
      "Speed: 4.4ms preprocess, 402.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 359.2ms\n",
      "Speed: 3.4ms preprocess, 359.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.9ms\n",
      "Speed: 3.5ms preprocess, 373.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.6ms\n",
      "Speed: 4.0ms preprocess, 382.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 383.7ms\n",
      "Speed: 3.3ms preprocess, 383.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 4.1ms preprocess, 394.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 395.8ms\n",
      "Speed: 5.0ms preprocess, 395.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 474.8ms\n",
      "Speed: 4.3ms preprocess, 474.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 430.6ms\n",
      "Speed: 4.8ms preprocess, 430.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.1ms\n",
      "Speed: 3.9ms preprocess, 388.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.1ms\n",
      "Speed: 3.4ms preprocess, 399.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.4ms\n",
      "Speed: 4.0ms preprocess, 400.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 356.5ms\n",
      "Speed: 3.5ms preprocess, 356.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.6ms\n",
      "Speed: 3.8ms preprocess, 372.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 3.9ms preprocess, 394.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 387.6ms\n",
      "Speed: 3.9ms preprocess, 387.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 423.2ms\n",
      "Speed: 3.8ms preprocess, 423.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.0ms\n",
      "Speed: 4.1ms preprocess, 392.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.7ms preprocess, 398.5ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 376.6ms\n",
      "Speed: 4.2ms preprocess, 376.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 360.5ms\n",
      "Speed: 3.2ms preprocess, 360.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.9ms\n",
      "Speed: 3.7ms preprocess, 379.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.2ms\n",
      "Speed: 4.3ms preprocess, 396.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.9ms\n",
      "Speed: 3.2ms preprocess, 396.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.2ms\n",
      "Speed: 4.0ms preprocess, 389.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 472.5ms\n",
      "Speed: 3.6ms preprocess, 472.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.1ms\n",
      "Speed: 3.9ms preprocess, 394.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.0ms\n",
      "Speed: 3.6ms preprocess, 391.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 356.0ms\n",
      "Speed: 3.9ms preprocess, 356.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.6ms\n",
      "Speed: 3.2ms preprocess, 388.6ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.5ms\n",
      "Speed: 9.4ms preprocess, 372.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.0ms\n",
      "Speed: 4.0ms preprocess, 402.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 438.2ms\n",
      "Speed: 4.0ms preprocess, 438.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.1ms\n",
      "Speed: 3.5ms preprocess, 386.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.3ms\n",
      "Speed: 3.7ms preprocess, 372.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.4ms\n",
      "Speed: 3.5ms preprocess, 373.4ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 458.9ms\n",
      "Speed: 3.7ms preprocess, 458.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.7ms\n",
      "Speed: 3.3ms preprocess, 400.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 393.5ms\n",
      "Speed: 4.2ms preprocess, 393.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 458.3ms\n",
      "Speed: 6.4ms preprocess, 458.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 450.8ms\n",
      "Speed: 9.3ms preprocess, 450.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 345.9ms\n",
      "Speed: 4.3ms preprocess, 345.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 484.2ms\n",
      "Speed: 2.0ms preprocess, 484.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 539.5ms\n",
      "Speed: 1.9ms preprocess, 539.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 485.8ms\n",
      "Speed: 5.0ms preprocess, 485.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.5ms\n",
      "Speed: 4.6ms preprocess, 397.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 381.6ms\n",
      "Speed: 4.4ms preprocess, 381.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.2ms\n",
      "Speed: 3.5ms preprocess, 406.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 model for pose detection\n",
    "model = YOLO('yolov8s-pose.pt')  # Load the YOLOv8 pose model\n",
    "\n",
    "# Open video capture\n",
    "video_path = '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_001557.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_idx = 0\n",
    "keypoints_data = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run pose detection on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Extract keypoints for each detected person\n",
    "    for result in results:  # loop over detections\n",
    "        keypoints = result.keypoints  # Extract keypoints (x, y, visibility)\n",
    "        bbox = result.boxes.xywh[0].cpu().numpy()  # Bounding box (x, y, w, h)\n",
    "\n",
    "        # Ensure keypoints are valid and normalize them\n",
    "        if len(keypoints) % 3 == 0:\n",
    "            normalized_keypoints = []\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize x by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize y by height\n",
    "                visibility = keypoints[i + 2]  # Visibility\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            keypoints_data.append(normalized_keypoints)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "# Close video capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.5741 - mae: 1.9591 - val_loss: 5.0813 - val_mae: 1.9990\n",
      "Epoch 2/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8606 - mae: 1.0929 - val_loss: 2.9150 - val_mae: 1.4811\n",
      "Epoch 3/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6565 - mae: 1.0443 - val_loss: 1.8774 - val_mae: 1.1757\n",
      "Epoch 4/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4841 - mae: 0.9849 - val_loss: 1.5834 - val_mae: 1.1087\n",
      "Epoch 5/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4464 - mae: 0.9789 - val_loss: 1.5183 - val_mae: 1.0839\n",
      "Epoch 6/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3944 - mae: 0.9686 - val_loss: 1.2938 - val_mae: 0.9513\n",
      "Epoch 7/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2753 - mae: 0.8944 - val_loss: 1.3064 - val_mae: 0.9596\n",
      "Epoch 8/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2277 - mae: 0.9111 - val_loss: 1.2705 - val_mae: 0.9251\n",
      "Epoch 9/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3648 - mae: 0.9540 - val_loss: 1.2158 - val_mae: 0.9042\n",
      "Epoch 10/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2193 - mae: 0.8938 - val_loss: 1.2480 - val_mae: 0.9007\n",
      "Epoch 11/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2361 - mae: 0.9046 - val_loss: 1.2803 - val_mae: 0.9136\n",
      "Epoch 12/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1693 - mae: 0.8819 - val_loss: 1.2088 - val_mae: 0.9190\n",
      "Epoch 13/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3167 - mae: 0.9288 - val_loss: 1.2065 - val_mae: 0.9195\n",
      "Epoch 14/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2976 - mae: 0.9151 - val_loss: 1.1605 - val_mae: 0.8725\n",
      "Epoch 15/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2532 - mae: 0.9128 - val_loss: 1.1260 - val_mae: 0.8707\n",
      "Epoch 16/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1803 - mae: 0.8840 - val_loss: 1.2833 - val_mae: 0.9206\n",
      "Epoch 17/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1474 - mae: 0.8687 - val_loss: 1.1815 - val_mae: 0.8798\n",
      "Epoch 18/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1649 - mae: 0.8818 - val_loss: 1.1705 - val_mae: 0.8827\n",
      "Epoch 19/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1843 - mae: 0.8854 - val_loss: 1.2029 - val_mae: 0.8890\n",
      "Epoch 20/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0757 - mae: 0.8418 - val_loss: 1.1126 - val_mae: 0.8593\n",
      "Epoch 21/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1555 - mae: 0.8698 - val_loss: 1.1586 - val_mae: 0.8585\n",
      "Epoch 22/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2281 - mae: 0.8930 - val_loss: 1.1254 - val_mae: 0.8513\n",
      "Epoch 23/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1198 - mae: 0.8467 - val_loss: 1.1601 - val_mae: 0.8669\n",
      "Epoch 24/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0501 - mae: 0.8321 - val_loss: 1.2344 - val_mae: 0.8856\n",
      "Epoch 25/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0781 - mae: 0.8442 - val_loss: 1.3236 - val_mae: 0.9277\n",
      "Epoch 26/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1664 - mae: 0.8680 - val_loss: 1.1117 - val_mae: 0.8545\n",
      "Epoch 27/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1607 - mae: 0.8760 - val_loss: 1.0981 - val_mae: 0.8528\n",
      "Epoch 28/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0334 - mae: 0.8213 - val_loss: 1.1162 - val_mae: 0.8545\n",
      "Epoch 29/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0357 - mae: 0.8168 - val_loss: 1.1660 - val_mae: 0.8556\n",
      "Epoch 30/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0705 - mae: 0.8384 - val_loss: 1.3799 - val_mae: 0.9521\n",
      "Epoch 31/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0682 - mae: 0.8401 - val_loss: 1.1878 - val_mae: 0.8651\n",
      "Epoch 32/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0092 - mae: 0.8102 - val_loss: 1.0826 - val_mae: 0.8390\n",
      "Epoch 33/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9977 - mae: 0.8040 - val_loss: 1.0622 - val_mae: 0.8227\n",
      "Epoch 34/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0470 - mae: 0.8243 - val_loss: 1.1301 - val_mae: 0.8600\n",
      "Epoch 35/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0514 - mae: 0.8283 - val_loss: 1.0224 - val_mae: 0.8154\n",
      "Epoch 36/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9650 - mae: 0.8044 - val_loss: 1.0018 - val_mae: 0.7974\n",
      "Epoch 37/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9788 - mae: 0.8001 - val_loss: 1.0396 - val_mae: 0.8443\n",
      "Epoch 38/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9660 - mae: 0.7863 - val_loss: 1.0076 - val_mae: 0.8080\n",
      "Epoch 39/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9963 - mae: 0.7974 - val_loss: 0.9836 - val_mae: 0.8045\n",
      "Epoch 40/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9761 - mae: 0.7895 - val_loss: 1.0881 - val_mae: 0.8314\n",
      "Epoch 41/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9496 - mae: 0.7804 - val_loss: 1.0017 - val_mae: 0.7946\n",
      "Epoch 42/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9208 - mae: 0.7804 - val_loss: 1.0058 - val_mae: 0.7866\n",
      "Epoch 43/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9170 - mae: 0.7706 - val_loss: 1.0088 - val_mae: 0.7859\n",
      "Epoch 44/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9262 - mae: 0.7524 - val_loss: 1.1418 - val_mae: 0.8243\n",
      "Epoch 45/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8994 - mae: 0.7756 - val_loss: 1.0224 - val_mae: 0.7783\n",
      "Epoch 46/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9474 - mae: 0.7775 - val_loss: 1.0102 - val_mae: 0.7892\n",
      "Epoch 47/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9273 - mae: 0.7738 - val_loss: 1.0462 - val_mae: 0.8039\n",
      "Epoch 48/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9437 - mae: 0.7778 - val_loss: 1.0132 - val_mae: 0.7801\n",
      "Epoch 49/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9303 - mae: 0.7719 - val_loss: 0.9155 - val_mae: 0.7431\n",
      "Epoch 50/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9316 - mae: 0.7681 - val_loss: 1.0361 - val_mae: 0.7862\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0422 - mae: 0.8009 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0361, Test MAE: 0.7862\n",
      "\n",
      "0: 384x640 2 persons, 127.9ms\n",
      "Speed: 2.0ms preprocess, 127.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\n",
      "0: 384x640 2 persons, 94.8ms\n",
      "Speed: 9.8ms preprocess, 94.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 2 persons, 649.2ms\n",
      "Speed: 91.5ms preprocess, 649.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 2 persons, 255.8ms\n",
      "Speed: 3.2ms preprocess, 255.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 10:13:52.224 python[1970:52658] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-20 10:13:52.224 python[1970:52658] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 181.2ms\n",
      "Speed: 1.0ms preprocess, 181.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 2 persons, 61.3ms\n",
      "Speed: 1.5ms preprocess, 61.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 63.9ms\n",
      "Speed: 1.0ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 184.8ms\n",
      "Speed: 1.0ms preprocess, 184.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.7ms\n",
      "Speed: 1.2ms preprocess, 64.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.3ms\n",
      "Speed: 1.1ms preprocess, 60.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 1.0ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 115.8ms\n",
      "Speed: 1.4ms preprocess, 115.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 0.8ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 1.0ms preprocess, 58.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 32.2ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.8ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 1.2ms preprocess, 72.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.8ms\n",
      "Speed: 0.9ms preprocess, 67.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 176.4ms\n",
      "Speed: 1.0ms preprocess, 176.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.9ms\n",
      "Speed: 1.0ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.4ms\n",
      "Speed: 1.2ms preprocess, 76.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.9ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.9ms\n",
      "Speed: 2.1ms preprocess, 97.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 176.7ms\n",
      "Speed: 0.8ms preprocess, 176.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.3ms\n",
      "Speed: 19.4ms preprocess, 88.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.8ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.5ms\n",
      "Speed: 1.0ms preprocess, 62.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.1ms\n",
      "Speed: 1.0ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 0.9ms preprocess, 79.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.9ms\n",
      "Speed: 0.9ms preprocess, 89.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 1.1ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.0ms preprocess, 65.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.0ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 176.4ms\n",
      "Speed: 1.2ms preprocess, 176.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 1.1ms preprocess, 62.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.4ms\n",
      "Speed: 11.4ms preprocess, 73.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.9ms preprocess, 74.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.9ms\n",
      "Speed: 1.0ms preprocess, 82.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 130.3ms\n",
      "Speed: 1.2ms preprocess, 130.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 0.8ms preprocess, 55.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.2ms\n",
      "Speed: 1.0ms preprocess, 81.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 125.1ms\n",
      "Speed: 15.2ms preprocess, 125.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.8ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.8ms\n",
      "Speed: 1.0ms preprocess, 76.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 0.9ms preprocess, 74.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 4.3ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 176.1ms\n",
      "Speed: 0.8ms preprocess, 176.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 0.8ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 117.6ms\n",
      "Speed: 11.9ms preprocess, 117.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 0.8ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 0.9ms preprocess, 80.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 181.2ms\n",
      "Speed: 0.9ms preprocess, 181.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.6ms\n",
      "Speed: 0.9ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 172.8ms\n",
      "Speed: 0.9ms preprocess, 172.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.4ms\n",
      "Speed: 0.8ms preprocess, 58.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 72.9ms\n",
      "Speed: 0.9ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 2 persons, 101.7ms\n",
      "Speed: 2.3ms preprocess, 101.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 55.1ms\n",
      "Speed: 0.8ms preprocess, 55.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 92.8ms\n",
      "Speed: 13.9ms preprocess, 92.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.7ms\n",
      "Speed: 0.9ms preprocess, 69.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 90.7ms\n",
      "Speed: 0.8ms preprocess, 90.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.4ms\n",
      "Speed: 0.9ms preprocess, 83.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 82.6ms\n",
      "Speed: 10.1ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 82.7ms\n",
      "Speed: 1.0ms preprocess, 82.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 88.4ms\n",
      "Speed: 1.0ms preprocess, 88.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 2 persons, 82.4ms\n",
      "Speed: 1.0ms preprocess, 82.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "0: 384x640 2 persons, 78.8ms\n",
      "Speed: 16.6ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.3ms\n",
      "Speed: 0.9ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.6ms\n",
      "Speed: 1.0ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 63.3ms\n",
      "Speed: 0.9ms preprocess, 63.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.6ms\n",
      "Speed: 1.1ms preprocess, 65.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.6ms\n",
      "Speed: 0.8ms preprocess, 59.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 2 persons, 94.0ms\n",
      "Speed: 10.0ms preprocess, 94.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.1ms\n",
      "Speed: 0.9ms preprocess, 65.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 88.4ms\n",
      "Speed: 0.9ms preprocess, 88.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.4ms\n",
      "Speed: 1.0ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.6ms\n",
      "Speed: 1.0ms preprocess, 86.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 1.0ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 1.3ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.8ms\n",
      "Speed: 1.8ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 0.8ms preprocess, 62.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 0.9ms preprocess, 68.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.0ms\n",
      "Speed: 0.9ms preprocess, 62.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.6ms\n",
      "Speed: 1.0ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.1ms\n",
      "Speed: 1.0ms preprocess, 68.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 93.4ms\n",
      "Speed: 1.2ms preprocess, 93.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.9ms\n",
      "Speed: 1.3ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 140.3ms\n",
      "Speed: 1.1ms preprocess, 140.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 2 persons, 126.0ms\n",
      "Speed: 11.8ms preprocess, 126.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.8ms\n",
      "Speed: 0.8ms preprocess, 59.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.7ms\n",
      "Speed: 1.0ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.6ms\n",
      "Speed: 0.8ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.4ms\n",
      "Speed: 0.9ms preprocess, 58.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 215.3ms\n",
      "Speed: 1.5ms preprocess, 215.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.7ms\n",
      "Speed: 1.2ms preprocess, 60.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 97.9ms\n",
      "Speed: 4.3ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 160.9ms\n",
      "Speed: 0.8ms preprocess, 160.9ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.4ms\n",
      "Speed: 0.9ms preprocess, 54.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.6ms\n",
      "Speed: 17.6ms preprocess, 84.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 70.6ms\n",
      "Speed: 0.8ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 66.2ms\n",
      "Speed: 0.7ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.1ms\n",
      "Speed: 0.9ms preprocess, 71.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 109.1ms\n",
      "Speed: 1.1ms preprocess, 109.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 87.0ms\n",
      "Speed: 1.1ms preprocess, 87.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.3ms\n",
      "Speed: 1.1ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.2ms\n",
      "Speed: 0.9ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 176.3ms\n",
      "Speed: 0.9ms preprocess, 176.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 61.5ms\n",
      "Speed: 0.9ms preprocess, 61.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\n",
      "0: 384x640 2 persons, 56.4ms\n",
      "Speed: 10.7ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.2ms\n",
      "Speed: 0.9ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.2ms\n",
      "Speed: 0.7ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 66.3ms\n",
      "Speed: 0.8ms preprocess, 66.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 174.9ms\n",
      "Speed: 1.0ms preprocess, 174.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.5ms\n",
      "Speed: 0.9ms preprocess, 58.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 56.7ms\n",
      "Speed: 1.0ms preprocess, 56.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 135.1ms\n",
      "Speed: 17.3ms preprocess, 135.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.3ms\n",
      "Speed: 0.8ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.5ms\n",
      "Speed: 1.0ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 93.1ms\n",
      "Speed: 0.9ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.1ms\n",
      "Speed: 0.9ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.8ms\n",
      "Speed: 1.0ms preprocess, 75.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 73.9ms\n",
      "Speed: 1.0ms preprocess, 73.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.9ms\n",
      "Speed: 1.0ms preprocess, 83.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 61.1ms\n",
      "Speed: 8.1ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.8ms\n",
      "Speed: 0.9ms preprocess, 75.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 81.8ms\n",
      "Speed: 0.9ms preprocess, 81.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 181.7ms\n",
      "Speed: 1.0ms preprocess, 181.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.7ms\n",
      "Speed: 0.8ms preprocess, 64.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.3ms\n",
      "Speed: 0.9ms preprocess, 65.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.6ms\n",
      "Speed: 1.3ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.7ms\n",
      "Speed: 23.9ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 79.9ms\n",
      "Speed: 0.9ms preprocess, 79.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.0ms\n",
      "Speed: 0.9ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 184.6ms\n",
      "Speed: 0.9ms preprocess, 184.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 70.3ms\n",
      "Speed: 0.9ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 179.6ms\n",
      "Speed: 0.9ms preprocess, 179.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.2ms\n",
      "Speed: 1.0ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 2 persons, 111.7ms\n",
      "Speed: 10.7ms preprocess, 111.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 74.8ms\n",
      "Speed: 0.9ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 79.1ms\n",
      "Speed: 1.0ms preprocess, 79.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.9ms\n",
      "Speed: 1.3ms preprocess, 83.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 1.0ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.9ms\n",
      "Speed: 0.8ms preprocess, 102.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.3ms\n",
      "Speed: 0.9ms preprocess, 64.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.9ms\n",
      "Speed: 0.8ms preprocess, 83.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 115.4ms\n",
      "Speed: 2.5ms preprocess, 115.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 76.7ms\n",
      "Speed: 1.8ms preprocess, 76.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.8ms\n",
      "Speed: 0.9ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "0: 384x640 2 persons, 79.1ms\n",
      "Speed: 22.8ms preprocess, 79.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 182.6ms\n",
      "Speed: 1.4ms preprocess, 182.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 177.9ms\n",
      "Speed: 0.8ms preprocess, 177.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 57.9ms\n",
      "Speed: 1.0ms preprocess, 57.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.2ms\n",
      "Speed: 0.9ms preprocess, 60.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 65.3ms\n",
      "Speed: 1.0ms preprocess, 65.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 53.5ms\n",
      "Speed: 1.0ms preprocess, 53.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 124.6ms\n",
      "Speed: 2.7ms preprocess, 124.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.6ms\n",
      "Speed: 0.9ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 2 persons, 100.4ms\n",
      "Speed: 9.0ms preprocess, 100.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.8ms\n",
      "Speed: 0.8ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 80.0ms\n",
      "Speed: 0.9ms preprocess, 80.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 74.4ms\n",
      "Speed: 0.9ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.0ms\n",
      "Speed: 1.0ms preprocess, 84.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.4ms\n",
      "Speed: 0.9ms preprocess, 64.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 118.8ms\n",
      "Speed: 1.0ms preprocess, 118.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.8ms\n",
      "Speed: 1.0ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 70.2ms\n",
      "Speed: 0.8ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.9ms\n",
      "Speed: 0.9ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 111.8ms\n",
      "Speed: 0.8ms preprocess, 111.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 2 persons, 122.6ms\n",
      "Speed: 0.9ms preprocess, 122.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.8ms\n",
      "Speed: 1.8ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 0.9ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.9ms\n",
      "Speed: 1.0ms preprocess, 62.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.7ms\n",
      "Speed: 1.1ms preprocess, 75.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.0ms\n",
      "Speed: 1.0ms preprocess, 71.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 70.4ms\n",
      "Speed: 1.2ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.5ms\n",
      "Speed: 1.3ms preprocess, 84.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 88.3ms\n",
      "Speed: 0.9ms preprocess, 88.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 101.5ms\n",
      "Speed: 2.0ms preprocess, 101.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 57.5ms\n",
      "Speed: 1.0ms preprocess, 57.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.2ms\n",
      "Speed: 1.0ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 138.8ms\n",
      "Speed: 8.3ms preprocess, 138.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 82.0ms\n",
      "Speed: 5.9ms preprocess, 82.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.5ms\n",
      "Speed: 1.1ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.1ms\n",
      "Speed: 0.9ms preprocess, 71.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.3ms\n",
      "Speed: 1.0ms preprocess, 84.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.6ms\n",
      "Speed: 0.9ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.8ms\n",
      "Speed: 1.2ms preprocess, 83.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.1ms\n",
      "Speed: 0.9ms preprocess, 69.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 80.7ms\n",
      "Speed: 1.5ms preprocess, 80.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 72.2ms\n",
      "Speed: 0.8ms preprocess, 72.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 78.0ms\n",
      "Speed: 0.8ms preprocess, 78.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.6ms\n",
      "Speed: 1.2ms preprocess, 84.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.4ms\n",
      "Speed: 1.1ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 80.3ms\n",
      "Speed: 0.9ms preprocess, 80.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 177.9ms\n",
      "Speed: 0.9ms preprocess, 177.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 81.0ms\n",
      "Speed: 0.9ms preprocess, 81.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 85.6ms\n",
      "Speed: 0.9ms preprocess, 85.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 72.8ms\n",
      "Speed: 1.0ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.9ms\n",
      "Speed: 1.2ms preprocess, 69.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 66.9ms\n",
      "Speed: 0.9ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 73.2ms\n",
      "Speed: 0.9ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 79.0ms\n",
      "Speed: 1.1ms preprocess, 79.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 77.4ms\n",
      "Speed: 0.9ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.4ms\n",
      "Speed: 1.2ms preprocess, 64.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 176.4ms\n",
      "Speed: 0.8ms preprocess, 176.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 56.4ms\n",
      "Speed: 0.9ms preprocess, 56.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 2 persons, 100.6ms\n",
      "Speed: 26.0ms preprocess, 100.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 77.7ms\n",
      "Speed: 1.0ms preprocess, 77.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 88.6ms\n",
      "Speed: 0.8ms preprocess, 88.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 162.3ms\n",
      "Speed: 5.2ms preprocess, 162.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 63.9ms\n",
      "Speed: 0.9ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 81.1ms\n",
      "Speed: 0.9ms preprocess, 81.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 82.2ms\n",
      "Speed: 0.9ms preprocess, 82.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.8ms\n",
      "Speed: 1.0ms preprocess, 58.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.6ms\n",
      "Speed: 6.3ms preprocess, 60.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.2ms\n",
      "Speed: 1.1ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 57.4ms\n",
      "Speed: 1.0ms preprocess, 57.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 86.0ms\n",
      "Speed: 12.3ms preprocess, 86.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 63.8ms\n",
      "Speed: 1.0ms preprocess, 63.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.4ms\n",
      "Speed: 1.0ms preprocess, 75.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 2 persons, 74.5ms\n",
      "Speed: 1.7ms preprocess, 74.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 77.4ms\n",
      "Speed: 1.0ms preprocess, 77.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 188.6ms\n",
      "Speed: 1.8ms preprocess, 188.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.7ms\n",
      "Speed: 1.0ms preprocess, 68.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 74.3ms\n",
      "Speed: 1.0ms preprocess, 74.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.6ms\n",
      "Speed: 1.1ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 57.5ms\n",
      "Speed: 1.0ms preprocess, 57.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 97.8ms\n",
      "Speed: 1.3ms preprocess, 97.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 78.4ms\n",
      "Speed: 4.2ms preprocess, 78.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 189.8ms\n",
      "Speed: 1.0ms preprocess, 189.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 66.6ms\n",
      "Speed: 1.0ms preprocess, 66.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.9ms\n",
      "Speed: 1.0ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 71.9ms\n",
      "Speed: 0.9ms preprocess, 71.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 109.2ms\n",
      "Speed: 1.1ms preprocess, 109.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.8ms\n",
      "Speed: 1.0ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 77.5ms\n",
      "Speed: 1.1ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 84.5ms\n",
      "Speed: 0.9ms preprocess, 84.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.5ms\n",
      "Speed: 1.0ms preprocess, 75.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 193.5ms\n",
      "Speed: 0.9ms preprocess, 193.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.0ms\n",
      "Speed: 0.8ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 2 persons, 116.1ms\n",
      "Speed: 19.7ms preprocess, 116.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 58.2ms\n",
      "Speed: 0.9ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 75.9ms\n",
      "Speed: 0.9ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 87.8ms\n",
      "Speed: 0.9ms preprocess, 87.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.9ms\n",
      "Speed: 9.5ms preprocess, 61.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.9ms\n",
      "Speed: 0.9ms preprocess, 80.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.2ms\n",
      "Speed: 1.1ms preprocess, 83.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 0.9ms preprocess, 60.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.0ms\n",
      "Speed: 1.0ms preprocess, 85.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.3ms\n",
      "Speed: 1.1ms preprocess, 92.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 137.6ms\n",
      "Speed: 12.2ms preprocess, 137.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.9ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.4ms\n",
      "Speed: 0.9ms preprocess, 82.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.8ms\n",
      "Speed: 1.0ms preprocess, 88.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.4ms\n",
      "Speed: 1.0ms preprocess, 62.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 0.9ms preprocess, 65.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.8ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 1.1ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.3ms\n",
      "Speed: 0.9ms preprocess, 59.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.5ms\n",
      "Speed: 1.0ms preprocess, 94.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.7ms\n",
      "Speed: 1.5ms preprocess, 186.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.8ms\n",
      "Speed: 0.9ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.8ms\n",
      "Speed: 7.3ms preprocess, 80.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 162.8ms\n",
      "Speed: 8.0ms preprocess, 162.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.0ms preprocess, 75.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.9ms preprocess, 72.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.9ms\n",
      "Speed: 1.0ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 1.1ms preprocess, 60.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 1.4ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.8ms\n",
      "Speed: 1.1ms preprocess, 58.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.2ms\n",
      "Speed: 1.0ms preprocess, 84.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.1ms\n",
      "Speed: 1.0ms preprocess, 94.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.6ms\n",
      "Speed: 6.1ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.9ms\n",
      "Speed: 0.9ms preprocess, 72.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 1.2ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.9ms\n",
      "Speed: 13.0ms preprocess, 65.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.0ms\n",
      "Speed: 1.0ms preprocess, 66.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 183.1ms\n",
      "Speed: 2.0ms preprocess, 183.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 384x640 1 person, 172.7ms\n",
      "Speed: 1.3ms preprocess, 172.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.0ms\n",
      "Speed: 0.7ms preprocess, 56.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.9ms\n",
      "Speed: 33.5ms preprocess, 62.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.9ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.8ms\n",
      "Speed: 0.9ms preprocess, 91.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.8ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 0.9ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 1.1ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.6ms\n",
      "Speed: 0.9ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.3ms\n",
      "Speed: 1.1ms preprocess, 93.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 2.4ms preprocess, 78.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.4ms\n",
      "Speed: 33.4ms preprocess, 110.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 0.8ms preprocess, 75.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 0.9ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.9ms\n",
      "Speed: 1.1ms preprocess, 81.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 1.1ms preprocess, 72.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 0.8ms preprocess, 80.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.4ms\n",
      "Speed: 1.2ms preprocess, 58.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.2ms\n",
      "Speed: 0.9ms preprocess, 64.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 0.8ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.1ms\n",
      "Speed: 0.9ms preprocess, 90.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 0.9ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.5ms\n",
      "Speed: 1.1ms preprocess, 85.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.8ms\n",
      "Speed: 1.1ms preprocess, 184.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.7ms\n",
      "Speed: 0.9ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 193.7ms\n",
      "Speed: 1.3ms preprocess, 193.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.9ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.9ms\n",
      "Speed: 2.1ms preprocess, 96.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.8ms preprocess, 78.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 118.7ms\n",
      "Speed: 0.8ms preprocess, 118.7ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 0.9ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 122.2ms\n",
      "Speed: 15.7ms preprocess, 122.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.2ms\n",
      "Speed: 0.9ms preprocess, 184.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 181.0ms\n",
      "Speed: 1.0ms preprocess, 181.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 0.8ms preprocess, 60.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.9ms\n",
      "Speed: 7.8ms preprocess, 103.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.9ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 1.0ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.0ms\n",
      "Speed: 0.8ms preprocess, 174.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.8ms\n",
      "Speed: 0.9ms preprocess, 58.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.1ms\n",
      "Speed: 11.7ms preprocess, 97.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.4ms\n",
      "Speed: 11.4ms preprocess, 92.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 1.0ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 0.9ms preprocess, 73.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.9ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.4ms\n",
      "Speed: 1.1ms preprocess, 57.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 0.8ms preprocess, 57.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.0ms\n",
      "Speed: 1.0ms preprocess, 86.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 0.9ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.4ms\n",
      "Speed: 1.2ms preprocess, 85.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 1.1ms preprocess, 80.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.8ms\n",
      "Speed: 1.0ms preprocess, 80.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.5ms\n",
      "Speed: 0.8ms preprocess, 59.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 1.1ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.2ms\n",
      "Speed: 1.0ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.8ms\n",
      "Speed: 1.0ms preprocess, 57.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 132.6ms\n",
      "Speed: 11.7ms preprocess, 132.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.3ms\n",
      "Speed: 3.6ms preprocess, 59.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.6ms\n",
      "Speed: 0.9ms preprocess, 55.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.5ms\n",
      "Speed: 1.0ms preprocess, 53.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.2ms\n",
      "Speed: 34.0ms preprocess, 89.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 0.8ms preprocess, 54.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.2ms\n",
      "Speed: 0.8ms preprocess, 54.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.9ms preprocess, 69.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.4ms\n",
      "Speed: 0.8ms preprocess, 55.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.6ms\n",
      "Speed: 2.6ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.3ms\n",
      "Speed: 11.5ms preprocess, 99.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.6ms\n",
      "Speed: 0.8ms preprocess, 63.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 10.9ms preprocess, 76.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.5ms\n",
      "Speed: 1.1ms preprocess, 81.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 1.0ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.6ms\n",
      "Speed: 1.0ms preprocess, 87.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.9ms\n",
      "Speed: 1.2ms preprocess, 110.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 106.3ms\n",
      "Speed: 14.9ms preprocess, 106.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.7ms\n",
      "Speed: 0.9ms preprocess, 58.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.0ms\n",
      "Speed: 0.9ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.2ms\n",
      "Speed: 0.9ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.7ms\n",
      "Speed: 1.1ms preprocess, 86.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.9ms\n",
      "Speed: 0.9ms preprocess, 60.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 7.9ms preprocess, 63.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 1.0ms preprocess, 58.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.5ms\n",
      "Speed: 24.0ms preprocess, 91.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 1.0ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.1ms\n",
      "Speed: 1.1ms preprocess, 184.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 1.0ms preprocess, 71.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.8ms\n",
      "Speed: 1.0ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.0ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.7ms\n",
      "Speed: 0.9ms preprocess, 56.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 142.1ms\n",
      "Speed: 0.9ms preprocess, 142.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.7ms\n",
      "Speed: 0.9ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.0ms\n",
      "Speed: 12.0ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 178.8ms\n",
      "Speed: 0.9ms preprocess, 178.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.5ms\n",
      "Speed: 1.2ms preprocess, 92.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 0.9ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 135.5ms\n",
      "Speed: 0.9ms preprocess, 135.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.6ms\n",
      "Speed: 0.8ms preprocess, 56.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.6ms\n",
      "Speed: 18.2ms preprocess, 99.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 1.4ms preprocess, 80.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.9ms\n",
      "Speed: 2.2ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.0ms\n",
      "Speed: 1.2ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.3ms\n",
      "Speed: 0.8ms preprocess, 61.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.2ms\n",
      "Speed: 1.1ms preprocess, 64.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.5ms\n",
      "Speed: 0.8ms preprocess, 58.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 179.4ms\n",
      "Speed: 0.8ms preprocess, 179.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 0.9ms preprocess, 60.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.9ms preprocess, 75.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 183.6ms\n",
      "Speed: 1.0ms preprocess, 183.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.9ms\n",
      "Speed: 1.0ms preprocess, 62.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 121.4ms\n",
      "Speed: 2.1ms preprocess, 121.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 178.8ms\n",
      "Speed: 0.9ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.7ms\n",
      "Speed: 25.2ms preprocess, 102.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 2.3ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 119.2ms\n",
      "Speed: 11.4ms preprocess, 119.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 113.4ms\n",
      "Speed: 1.3ms preprocess, 113.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 1.2ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.0ms\n",
      "Speed: 1.0ms preprocess, 82.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 1.1ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.7ms\n",
      "Speed: 1.0ms preprocess, 85.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 0.9ms preprocess, 77.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.2ms\n",
      "Speed: 20.0ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 1.0ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.4ms\n",
      "Speed: 1.0ms preprocess, 60.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 0.9ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.6ms\n",
      "Speed: 13.3ms preprocess, 93.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 179.2ms\n",
      "Speed: 0.8ms preprocess, 179.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.9ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 0.9ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 0.9ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.9ms\n",
      "Speed: 0.9ms preprocess, 66.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.0ms\n",
      "Speed: 0.8ms preprocess, 61.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.2ms\n",
      "Speed: 1.2ms preprocess, 186.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 0.9ms preprocess, 60.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.8ms\n",
      "Speed: 2.1ms preprocess, 96.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 1.0ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 0.9ms preprocess, 58.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 6.3ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 200.9ms\n",
      "Speed: 10.3ms preprocess, 200.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 122.6ms\n",
      "Speed: 20.2ms preprocess, 122.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 180.8ms\n",
      "Speed: 0.8ms preprocess, 180.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 0.8ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.4ms\n",
      "Speed: 0.9ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.2ms\n",
      "Speed: 0.9ms preprocess, 66.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.7ms\n",
      "Speed: 1.2ms preprocess, 64.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.1ms\n",
      "Speed: 1.2ms preprocess, 86.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.4ms\n",
      "Speed: 1.0ms preprocess, 88.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 181.9ms\n",
      "Speed: 3.5ms preprocess, 181.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 0.9ms preprocess, 60.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 102.8ms\n",
      "Speed: 1.6ms preprocess, 102.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 6.4ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 193.4ms\n",
      "Speed: 1.1ms preprocess, 193.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.7ms\n",
      "Speed: 0.9ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.9ms\n",
      "Speed: 1.0ms preprocess, 78.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.9ms\n",
      "Speed: 0.8ms preprocess, 88.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.1ms\n",
      "Speed: 9.2ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.9ms\n",
      "Speed: 0.9ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.8ms preprocess, 72.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.9ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.9ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 0.9ms preprocess, 74.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.3ms\n",
      "Speed: 0.9ms preprocess, 61.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 183.4ms\n",
      "Speed: 0.9ms preprocess, 183.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 0.9ms preprocess, 60.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 1.0ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 1.1ms preprocess, 76.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.0ms\n",
      "Speed: 1.0ms preprocess, 77.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 1.0ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.1ms\n",
      "Speed: 1.2ms preprocess, 87.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.4ms\n",
      "Speed: 1.3ms preprocess, 186.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 182.9ms\n",
      "Speed: 0.8ms preprocess, 182.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.5ms\n",
      "Speed: 1.0ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.3ms\n",
      "Speed: 1.1ms preprocess, 86.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.7ms\n",
      "Speed: 9.1ms preprocess, 72.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.0ms\n",
      "Speed: 0.9ms preprocess, 64.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.9ms\n",
      "Speed: 0.9ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 0.9ms preprocess, 74.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.7ms\n",
      "Speed: 1.0ms preprocess, 67.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 1.1ms preprocess, 77.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.3ms\n",
      "Speed: 1.0ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.1ms\n",
      "Speed: 0.9ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.3ms\n",
      "Speed: 1.1ms preprocess, 66.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 1.1ms preprocess, 72.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.9ms\n",
      "Speed: 1.3ms preprocess, 56.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 0.9ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 120.2ms\n",
      "Speed: 21.3ms preprocess, 120.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 1.0ms preprocess, 80.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 116.1ms\n",
      "Speed: 23.1ms preprocess, 116.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.2ms\n",
      "Speed: 0.9ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 79.4ms\n",
      "Speed: 1.0ms preprocess, 79.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.0ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.2ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.7ms\n",
      "Speed: 1.2ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.3ms\n",
      "Speed: 0.9ms preprocess, 63.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 113.7ms\n",
      "Speed: 0.8ms preprocess, 113.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.7ms\n",
      "Speed: 1.0ms preprocess, 82.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.0ms\n",
      "Speed: 1.5ms preprocess, 83.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 0.9ms preprocess, 60.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.6ms\n",
      "Speed: 1.0ms preprocess, 66.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 0.9ms preprocess, 79.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.2ms\n",
      "Speed: 1.3ms preprocess, 60.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.9ms\n",
      "Speed: 1.1ms preprocess, 73.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.1ms\n",
      "Speed: 0.8ms preprocess, 62.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.3ms\n",
      "Speed: 3.4ms preprocess, 101.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.3ms\n",
      "Speed: 1.2ms preprocess, 65.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 109.3ms\n",
      "Speed: 8.7ms preprocess, 109.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.7ms\n",
      "Speed: 0.7ms preprocess, 58.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.9ms\n",
      "Speed: 21.8ms preprocess, 99.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 0.9ms preprocess, 78.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 181.1ms\n",
      "Speed: 0.8ms preprocess, 181.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.1ms\n",
      "Speed: 1.9ms preprocess, 89.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 1.3ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 112.3ms\n",
      "Speed: 14.7ms preprocess, 112.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.2ms\n",
      "Speed: 0.8ms preprocess, 88.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 5.3ms preprocess, 57.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 0.9ms preprocess, 63.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.8ms\n",
      "Speed: 1.1ms preprocess, 184.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 19.3ms preprocess, 77.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 183.1ms\n",
      "Speed: 0.9ms preprocess, 183.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.6ms\n",
      "Speed: 1.0ms preprocess, 174.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.1ms\n",
      "Speed: 1.0ms preprocess, 59.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 9.8ms preprocess, 79.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.7ms\n",
      "Speed: 0.8ms preprocess, 79.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.6ms\n",
      "Speed: 1.1ms preprocess, 66.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.8ms\n",
      "Speed: 1.0ms preprocess, 85.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.6ms\n",
      "Speed: 0.9ms preprocess, 66.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.6ms\n",
      "Speed: 9.8ms preprocess, 85.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.1ms\n",
      "Speed: 1.0ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.6ms\n",
      "Speed: 0.8ms preprocess, 81.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 141.2ms\n",
      "Speed: 17.3ms preprocess, 141.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.9ms preprocess, 58.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 178.2ms\n",
      "Speed: 0.9ms preprocess, 178.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.7ms\n",
      "Speed: 0.9ms preprocess, 67.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.0ms\n",
      "Speed: 11.8ms preprocess, 64.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.8ms\n",
      "Speed: 1.1ms preprocess, 87.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.8ms\n",
      "Speed: 1.0ms preprocess, 85.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 0.9ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.1ms\n",
      "Speed: 1.0ms preprocess, 79.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.3ms\n",
      "Speed: 0.9ms preprocess, 71.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.1ms\n",
      "Speed: 0.9ms preprocess, 67.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.9ms\n",
      "Speed: 0.9ms preprocess, 103.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 1.2ms preprocess, 72.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 0.9ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.2ms\n",
      "Speed: 0.8ms preprocess, 174.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 0.9ms preprocess, 50.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 1.3ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.2ms\n",
      "Speed: 1.0ms preprocess, 63.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 1.0ms preprocess, 68.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 1.0ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 136.8ms\n",
      "Speed: 7.5ms preprocess, 136.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.2ms\n",
      "Speed: 22.2ms preprocess, 77.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.2ms\n",
      "Speed: 1.3ms preprocess, 54.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 1.0ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.9ms\n",
      "Speed: 1.2ms preprocess, 186.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 1.1ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.1ms\n",
      "Speed: 6.3ms preprocess, 64.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.4ms\n",
      "Speed: 0.9ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.8ms\n",
      "Speed: 1.0ms preprocess, 58.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.9ms\n",
      "Speed: 1.0ms preprocess, 61.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 0.9ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.9ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.9ms\n",
      "Speed: 25.6ms preprocess, 100.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.8ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 0.9ms preprocess, 72.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 1.0ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.4ms\n",
      "Speed: 1.6ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.9ms\n",
      "Speed: 0.9ms preprocess, 91.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 0.9ms preprocess, 66.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.0ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.4ms\n",
      "Speed: 0.9ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 1.4ms preprocess, 66.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 109.5ms\n",
      "Speed: 1.8ms preprocess, 109.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.5ms\n",
      "Speed: 0.9ms preprocess, 86.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.6ms\n",
      "Speed: 1.1ms preprocess, 64.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 131.2ms\n",
      "Speed: 22.3ms preprocess, 131.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.9ms\n",
      "Speed: 1.2ms preprocess, 61.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 1.1ms preprocess, 69.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.3ms\n",
      "Speed: 1.1ms preprocess, 70.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 1.0ms preprocess, 70.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 3.0ms preprocess, 77.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.2ms\n",
      "Speed: 0.8ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 165.3ms\n",
      "Speed: 1.0ms preprocess, 165.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.8ms\n",
      "Speed: 1.0ms preprocess, 92.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 1.0ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.0ms\n",
      "Speed: 1.5ms preprocess, 91.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 7.4ms preprocess, 81.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.5ms\n",
      "Speed: 0.9ms preprocess, 64.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 125.6ms\n",
      "Speed: 1.2ms preprocess, 125.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 0.9ms preprocess, 57.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.3ms\n",
      "Speed: 1.0ms preprocess, 62.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.7ms\n",
      "Speed: 1.0ms preprocess, 59.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 0.9ms preprocess, 79.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 1.2ms preprocess, 77.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 1.2ms preprocess, 70.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 0.9ms preprocess, 76.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 1.0ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.6ms\n",
      "Speed: 1.0ms preprocess, 73.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 126.6ms\n",
      "Speed: 0.9ms preprocess, 126.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.7ms\n",
      "Speed: 1.1ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 25.6ms preprocess, 62.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 1.0ms preprocess, 67.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 0.9ms preprocess, 62.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.9ms preprocess, 77.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 1.3ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 1.3ms preprocess, 77.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.7ms\n",
      "Speed: 1.2ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "Speed: 1.1ms preprocess, 76.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 182.7ms\n",
      "Speed: 1.2ms preprocess, 182.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.2ms\n",
      "Speed: 1.0ms preprocess, 64.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.3ms\n",
      "Speed: 0.9ms preprocess, 67.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.5ms\n",
      "Speed: 1.0ms preprocess, 89.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.0ms\n",
      "Speed: 1.0ms preprocess, 84.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.4ms\n",
      "Speed: 1.0ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.2ms\n",
      "Speed: 1.1ms preprocess, 81.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.4ms\n",
      "Speed: 0.8ms preprocess, 88.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 11.2ms preprocess, 75.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 1.0ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.5ms\n",
      "Speed: 2.5ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.9ms preprocess, 70.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.3ms\n",
      "Speed: 1.0ms preprocess, 88.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 1.0ms preprocess, 80.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.1ms\n",
      "Speed: 6.8ms preprocess, 103.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Average Score for the Video: 5.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define the scoring model\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"athlete_score_predictor.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_004494.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.2065 - mae: 2.1478 - val_loss: 6.1974 - val_mae: 2.2377\n",
      "Epoch 2/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7406 - mae: 1.0779 - val_loss: 4.8670 - val_mae: 1.9424\n",
      "Epoch 3/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6025 - mae: 1.0354 - val_loss: 3.6157 - val_mae: 1.6734\n",
      "Epoch 4/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3611 - mae: 0.9642 - val_loss: 2.7431 - val_mae: 1.4355\n",
      "Epoch 5/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3141 - mae: 0.9415 - val_loss: 1.9962 - val_mae: 1.2233\n",
      "Epoch 6/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2341 - mae: 0.9140 - val_loss: 1.7296 - val_mae: 1.1471\n",
      "Epoch 7/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0764 - mae: 0.8355 - val_loss: 1.7342 - val_mae: 1.1506\n",
      "Epoch 8/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1400 - mae: 0.8750 - val_loss: 1.6379 - val_mae: 1.0956\n",
      "Epoch 9/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1165 - mae: 0.8543 - val_loss: 1.3833 - val_mae: 1.0185\n",
      "Epoch 10/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0768 - mae: 0.8533 - val_loss: 1.4136 - val_mae: 1.0321\n",
      "Epoch 11/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0869 - mae: 0.8579 - val_loss: 1.1974 - val_mae: 0.9191\n",
      "Epoch 12/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0641 - mae: 0.8396 - val_loss: 1.1901 - val_mae: 0.9297\n",
      "Epoch 13/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0330 - mae: 0.8217 - val_loss: 1.3592 - val_mae: 0.9668\n",
      "Epoch 14/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9672 - mae: 0.7884 - val_loss: 1.1436 - val_mae: 0.8409\n",
      "Epoch 15/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0129 - mae: 0.8096 - val_loss: 1.3816 - val_mae: 0.9134\n",
      "Epoch 16/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0048 - mae: 0.7964 - val_loss: 1.6925 - val_mae: 0.9587\n",
      "Epoch 17/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8817 - mae: 0.7552 - val_loss: 1.0906 - val_mae: 0.8566\n",
      "Epoch 18/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9498 - mae: 0.7966 - val_loss: 1.0529 - val_mae: 0.8383\n",
      "Epoch 19/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9037 - mae: 0.7587 - val_loss: 1.2140 - val_mae: 0.8713\n",
      "Epoch 20/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8991 - mae: 0.7431 - val_loss: 1.0109 - val_mae: 0.8189\n",
      "Epoch 21/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8317 - mae: 0.7388 - val_loss: 1.3332 - val_mae: 0.9420\n",
      "Epoch 22/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8447 - mae: 0.7370 - val_loss: 1.1472 - val_mae: 0.8455\n",
      "Epoch 23/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9089 - mae: 0.7616 - val_loss: 1.2152 - val_mae: 0.8594\n",
      "Epoch 24/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8345 - mae: 0.7191 - val_loss: 1.1413 - val_mae: 0.8515\n",
      "Epoch 25/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8238 - mae: 0.7220 - val_loss: 1.1349 - val_mae: 0.8474\n",
      "Epoch 26/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8170 - mae: 0.7183 - val_loss: 0.9642 - val_mae: 0.7723\n",
      "Epoch 27/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7996 - mae: 0.6984 - val_loss: 0.8984 - val_mae: 0.7557\n",
      "Epoch 28/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7885 - mae: 0.7070 - val_loss: 0.9788 - val_mae: 0.7724\n",
      "Epoch 29/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7928 - mae: 0.6959 - val_loss: 1.1242 - val_mae: 0.8399\n",
      "Epoch 30/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7781 - mae: 0.6893 - val_loss: 1.1958 - val_mae: 0.8758\n",
      "Epoch 31/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9301 - mae: 0.7695 - val_loss: 0.8844 - val_mae: 0.7472\n",
      "Epoch 32/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6553 - mae: 0.6253 - val_loss: 1.2031 - val_mae: 0.8279\n",
      "Epoch 33/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7586 - mae: 0.6861 - val_loss: 0.9049 - val_mae: 0.7268\n",
      "Epoch 34/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7198 - mae: 0.6710 - val_loss: 0.9675 - val_mae: 0.7843\n",
      "Epoch 35/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7321 - mae: 0.6739 - val_loss: 0.8394 - val_mae: 0.7101\n",
      "Epoch 36/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7028 - mae: 0.6499 - val_loss: 0.8976 - val_mae: 0.7509\n",
      "Epoch 37/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7485 - mae: 0.6640 - val_loss: 1.2447 - val_mae: 0.8497\n",
      "Epoch 38/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7093 - mae: 0.6577 - val_loss: 1.3763 - val_mae: 0.8544\n",
      "Epoch 39/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7490 - mae: 0.6541 - val_loss: 1.3221 - val_mae: 0.8602\n",
      "Epoch 40/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6973 - mae: 0.6302 - val_loss: 0.8411 - val_mae: 0.7017\n",
      "Epoch 41/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6479 - mae: 0.6226 - val_loss: 0.7852 - val_mae: 0.6805\n",
      "Epoch 42/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6323 - mae: 0.6122 - val_loss: 0.8825 - val_mae: 0.7129\n",
      "Epoch 43/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6238 - mae: 0.6049 - val_loss: 0.8671 - val_mae: 0.6984\n",
      "Epoch 44/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6303 - mae: 0.6021 - val_loss: 0.7258 - val_mae: 0.6162\n",
      "Epoch 45/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6164 - mae: 0.5927 - val_loss: 0.9331 - val_mae: 0.7523\n",
      "Epoch 46/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5955 - mae: 0.5884 - val_loss: 1.6278 - val_mae: 1.0226\n",
      "Epoch 47/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6687 - mae: 0.6179 - val_loss: 1.0843 - val_mae: 0.7506\n",
      "Epoch 48/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6123 - mae: 0.5844 - val_loss: 0.8437 - val_mae: 0.6966\n",
      "Epoch 49/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6085 - mae: 0.6036 - val_loss: 0.7884 - val_mae: 0.6506\n",
      "Epoch 50/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6055 - mae: 0.5893 - val_loss: 0.8453 - val_mae: 0.6715\n",
      "Epoch 51/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6596 - mae: 0.6105 - val_loss: 0.7717 - val_mae: 0.6874\n",
      "Epoch 52/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5941 - mae: 0.5721 - val_loss: 0.8326 - val_mae: 0.6810\n",
      "Epoch 53/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6543 - mae: 0.6122 - val_loss: 1.0004 - val_mae: 0.7381\n",
      "Epoch 54/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6233 - mae: 0.5898 - val_loss: 0.7679 - val_mae: 0.6749\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7589 - mae: 0.6791 \n",
      "Test Loss: 0.7679, Test MAE: 0.6749\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5075\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 2.6ms preprocess, 50.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 12.1ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 1.4ms preprocess, 63.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.5ms\n",
      "Speed: 1.6ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 105.4ms\n",
      "Speed: 1.3ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\n",
      "0: 384x640 1 person, 108.2ms\n",
      "Speed: 7.7ms preprocess, 108.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.6ms\n",
      "Speed: 1.2ms preprocess, 56.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.2ms\n",
      "Speed: 1.1ms preprocess, 55.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.2ms preprocess, 69.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.1ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.3ms\n",
      "Speed: 1.1ms preprocess, 66.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 1.3ms preprocess, 69.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.1ms\n",
      "Speed: 1.3ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 1.3ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.8ms\n",
      "Speed: 9.4ms preprocess, 100.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 1.2ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.2ms\n",
      "Speed: 1.3ms preprocess, 174.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.7ms\n",
      "Speed: 1.1ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.7ms\n",
      "Speed: 1.5ms preprocess, 78.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 1.3ms preprocess, 71.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.4ms\n",
      "Speed: 1.4ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.5ms\n",
      "Speed: 1.2ms preprocess, 87.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 181.7ms\n",
      "Speed: 1.2ms preprocess, 181.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 1.1ms preprocess, 54.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.1ms\n",
      "Speed: 1.4ms preprocess, 61.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 120.3ms\n",
      "Speed: 10.4ms preprocess, 120.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 1.2ms preprocess, 66.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.7ms\n",
      "Speed: 1.4ms preprocess, 66.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 1.1ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.1ms\n",
      "Speed: 1.3ms preprocess, 76.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 1.1ms preprocess, 77.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 196.2ms\n",
      "Speed: 1.6ms preprocess, 196.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 175.7ms\n",
      "Speed: 1.2ms preprocess, 175.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.3ms\n",
      "Speed: 1.2ms preprocess, 55.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 26.8ms preprocess, 79.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.5ms\n",
      "Speed: 1.2ms preprocess, 85.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.6ms\n",
      "Speed: 1.4ms preprocess, 86.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 1.3ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 152.0ms\n",
      "Speed: 11.9ms preprocess, 152.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 1.1ms preprocess, 77.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.0ms\n",
      "Speed: 2.5ms preprocess, 65.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.3ms\n",
      "Speed: 1.3ms preprocess, 58.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.9ms\n",
      "Speed: 1.1ms preprocess, 62.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 193.8ms\n",
      "Speed: 1.3ms preprocess, 193.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.4ms\n",
      "Speed: 1.2ms preprocess, 59.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 1.3ms preprocess, 82.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 1.1ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.0ms\n",
      "Speed: 1.3ms preprocess, 87.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.0ms\n",
      "Speed: 1.2ms preprocess, 85.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 1.5ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.7ms\n",
      "Speed: 1.3ms preprocess, 80.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.6ms\n",
      "Speed: 1.3ms preprocess, 91.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 2.7ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 1.1ms preprocess, 73.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 188.3ms\n",
      "Speed: 1.3ms preprocess, 188.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 180.4ms\n",
      "Speed: 1.3ms preprocess, 180.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.7ms\n",
      "Speed: 1.3ms preprocess, 67.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.8ms\n",
      "Speed: 1.1ms preprocess, 76.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.6ms\n",
      "Speed: 1.6ms preprocess, 59.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.1ms\n",
      "Speed: 1.1ms preprocess, 90.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.2ms\n",
      "Speed: 1.3ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 87.9ms\n",
      "Speed: 1.3ms preprocess, 87.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 1.3ms preprocess, 74.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.3ms\n",
      "Speed: 1.3ms preprocess, 80.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.2ms\n",
      "Speed: 1.3ms preprocess, 84.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 2.5ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 1.2ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 1.4ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 2.3ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.9ms\n",
      "Speed: 1.2ms preprocess, 65.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.9ms\n",
      "Speed: 1.4ms preprocess, 85.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.4ms\n",
      "Speed: 1.3ms preprocess, 92.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.7ms\n",
      "Speed: 3.0ms preprocess, 99.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.5ms\n",
      "Speed: 1.2ms preprocess, 97.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.0ms\n",
      "Speed: 20.9ms preprocess, 82.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.0ms\n",
      "Speed: 1.1ms preprocess, 68.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.0ms\n",
      "Speed: 1.1ms preprocess, 64.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.3ms\n",
      "Speed: 1.4ms preprocess, 86.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.4ms\n",
      "Speed: 1.9ms preprocess, 98.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.4ms\n",
      "Speed: 1.4ms preprocess, 100.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.2ms\n",
      "Speed: 1.3ms preprocess, 87.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.3ms\n",
      "Speed: 20.7ms preprocess, 104.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.7ms\n",
      "Speed: 1.3ms preprocess, 72.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.5ms\n",
      "Speed: 1.2ms preprocess, 86.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.7ms\n",
      "Speed: 1.2ms preprocess, 101.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.5ms\n",
      "Speed: 1.2ms preprocess, 55.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.3ms\n",
      "Speed: 1.2ms preprocess, 91.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 253.6ms\n",
      "Speed: 22.0ms preprocess, 253.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 1.2ms preprocess, 75.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.8ms\n",
      "Speed: 1.8ms preprocess, 86.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.2ms\n",
      "Speed: 1.3ms preprocess, 85.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 180.2ms\n",
      "Speed: 1.3ms preprocess, 180.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 1.3ms preprocess, 77.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.6ms\n",
      "Speed: 1.2ms preprocess, 59.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.4ms\n",
      "Speed: 1.2ms preprocess, 66.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.3ms\n",
      "Speed: 1.2ms preprocess, 87.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.4ms\n",
      "Speed: 1.7ms preprocess, 81.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.5ms\n",
      "Speed: 1.4ms preprocess, 85.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.2ms\n",
      "Speed: 1.2ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 1.4ms preprocess, 68.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 177.5ms\n",
      "Speed: 1.4ms preprocess, 177.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.0ms\n",
      "Speed: 1.4ms preprocess, 56.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "0: 384x640 2 persons, 94.1ms\n",
      "Speed: 3.1ms preprocess, 94.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 (no detections), 175.8ms\n",
      "Speed: 1.2ms preprocess, 175.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Average Score for the Video: 3.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discuswerper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Discurweper.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_010422.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 7.2341 - mae: 2.3330 - val_loss: 7.7276 - val_mae: 2.6143\n",
      "Epoch 2/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7877 - mae: 1.0635 - val_loss: 6.5183 - val_mae: 2.4208\n",
      "Epoch 3/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4443 - mae: 0.9234 - val_loss: 5.5765 - val_mae: 2.2533\n",
      "Epoch 4/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2861 - mae: 0.8676 - val_loss: 4.4046 - val_mae: 2.0182\n",
      "Epoch 5/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3309 - mae: 0.8758 - val_loss: 3.4764 - val_mae: 1.7910\n",
      "Epoch 6/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1396 - mae: 0.8088 - val_loss: 2.8468 - val_mae: 1.6088\n",
      "Epoch 7/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1689 - mae: 0.7962 - val_loss: 2.2606 - val_mae: 1.4046\n",
      "Epoch 8/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1562 - mae: 0.8159 - val_loss: 1.7502 - val_mae: 1.1887\n",
      "Epoch 9/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9772 - mae: 0.7323 - val_loss: 1.5604 - val_mae: 1.0440\n",
      "Epoch 10/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0511 - mae: 0.7355 - val_loss: 1.3507 - val_mae: 0.9223\n",
      "Epoch 11/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9468 - mae: 0.7151 - val_loss: 1.2189 - val_mae: 0.8496\n",
      "Epoch 12/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9520 - mae: 0.7050 - val_loss: 0.9925 - val_mae: 0.7321\n",
      "Epoch 13/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9603 - mae: 0.7256 - val_loss: 0.9122 - val_mae: 0.7057\n",
      "Epoch 14/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8926 - mae: 0.6721 - val_loss: 0.9858 - val_mae: 0.6919\n",
      "Epoch 15/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8936 - mae: 0.6694 - val_loss: 0.9026 - val_mae: 0.7090\n",
      "Epoch 16/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8782 - mae: 0.6757 - val_loss: 0.9049 - val_mae: 0.6769\n",
      "Epoch 17/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8138 - mae: 0.6745 - val_loss: 0.8365 - val_mae: 0.5819\n",
      "Epoch 18/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8541 - mae: 0.6577 - val_loss: 0.7512 - val_mae: 0.5551\n",
      "Epoch 19/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8105 - mae: 0.6387 - val_loss: 0.7590 - val_mae: 0.5739\n",
      "Epoch 20/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8601 - mae: 0.6566 - val_loss: 1.0022 - val_mae: 0.6772\n",
      "Epoch 21/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9106 - mae: 0.6765 - val_loss: 0.7846 - val_mae: 0.5677\n",
      "Epoch 22/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8275 - mae: 0.6465 - val_loss: 1.0625 - val_mae: 0.6629\n",
      "Epoch 23/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8346 - mae: 0.6341 - val_loss: 0.7621 - val_mae: 0.5852\n",
      "Epoch 24/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7052 - mae: 0.5902 - val_loss: 0.8538 - val_mae: 0.6471\n",
      "Epoch 25/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8033 - mae: 0.6111 - val_loss: 0.8376 - val_mae: 0.6019\n",
      "Epoch 26/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7888 - mae: 0.6076 - val_loss: 0.7371 - val_mae: 0.5861\n",
      "Epoch 27/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7364 - mae: 0.6019 - val_loss: 0.7714 - val_mae: 0.5724\n",
      "Epoch 28/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7052 - mae: 0.5985 - val_loss: 0.8150 - val_mae: 0.5954\n",
      "Epoch 29/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6514 - mae: 0.5649 - val_loss: 0.6952 - val_mae: 0.5548\n",
      "Epoch 30/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7094 - mae: 0.5892 - val_loss: 0.7879 - val_mae: 0.5614\n",
      "Epoch 31/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7286 - mae: 0.5820 - val_loss: 0.8534 - val_mae: 0.6021\n",
      "Epoch 32/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6078 - mae: 0.5573 - val_loss: 0.6664 - val_mae: 0.5286\n",
      "Epoch 33/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6590 - mae: 0.5717 - val_loss: 0.7392 - val_mae: 0.5583\n",
      "Epoch 34/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6222 - mae: 0.5449 - val_loss: 0.7458 - val_mae: 0.5740\n",
      "Epoch 35/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6982 - mae: 0.5854 - val_loss: 0.7399 - val_mae: 0.5634\n",
      "Epoch 36/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6087 - mae: 0.5432 - val_loss: 0.7091 - val_mae: 0.5395\n",
      "Epoch 37/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6037 - mae: 0.5335 - val_loss: 0.9365 - val_mae: 0.6625\n",
      "Epoch 38/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6508 - mae: 0.5688 - val_loss: 0.7613 - val_mae: 0.5778\n",
      "Epoch 39/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6366 - mae: 0.5589 - val_loss: 0.6692 - val_mae: 0.5077\n",
      "Epoch 40/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6255 - mae: 0.5493 - val_loss: 0.8274 - val_mae: 0.5858\n",
      "Epoch 41/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6278 - mae: 0.5329 - val_loss: 0.7145 - val_mae: 0.5508\n",
      "Epoch 42/100\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6274 - mae: 0.5397 - val_loss: 0.8189 - val_mae: 0.5919\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8363 - mae: 0.5895 \n",
      "Test Loss: 0.8189, Test MAE: 0.5919\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.4873\n",
      "\n",
      "0: 640x640 1 person, 203.9ms\n",
      "Speed: 2.1ms preprocess, 203.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 (no detections), 102.4ms\n",
      "Speed: 1.7ms preprocess, 102.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 104.3ms\n",
      "Speed: 2.1ms preprocess, 104.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 220.2ms\n",
      "Speed: 1.7ms preprocess, 220.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 85.6ms\n",
      "Speed: 1.6ms preprocess, 85.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 207.5ms\n",
      "Speed: 1.5ms preprocess, 207.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Average Score for the Video: 3.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Estafette.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Estafette.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Estafette/segment_002126.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 6.7765 - mae: 2.1790 - val_loss: 7.3646 - val_mae: 2.4064\n",
      "Epoch 2/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0913 - mae: 1.1564 - val_loss: 5.5091 - val_mae: 2.0402\n",
      "Epoch 3/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7146 - mae: 1.0196 - val_loss: 4.6824 - val_mae: 1.8596\n",
      "Epoch 4/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7520 - mae: 1.0430 - val_loss: 3.5479 - val_mae: 1.5660\n",
      "Epoch 5/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6559 - mae: 0.9895 - val_loss: 3.0527 - val_mae: 1.4245\n",
      "Epoch 6/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4300 - mae: 0.9360 - val_loss: 2.6898 - val_mae: 1.3432\n",
      "Epoch 7/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4835 - mae: 0.9435 - val_loss: 2.1999 - val_mae: 1.1797\n",
      "Epoch 8/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3873 - mae: 0.9032 - val_loss: 2.2704 - val_mae: 1.2103\n",
      "Epoch 9/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3848 - mae: 0.9131 - val_loss: 1.7245 - val_mae: 1.0071\n",
      "Epoch 10/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2256 - mae: 0.8447 - val_loss: 1.6052 - val_mae: 0.9768\n",
      "Epoch 11/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2165 - mae: 0.8407 - val_loss: 1.5135 - val_mae: 0.9783\n",
      "Epoch 12/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2125 - mae: 0.8483 - val_loss: 1.2426 - val_mae: 0.8542\n",
      "Epoch 13/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2833 - mae: 0.8716 - val_loss: 1.3388 - val_mae: 0.8899\n",
      "Epoch 14/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2298 - mae: 0.8502 - val_loss: 1.2921 - val_mae: 0.8581\n",
      "Epoch 15/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2313 - mae: 0.8683 - val_loss: 1.1210 - val_mae: 0.7957\n",
      "Epoch 16/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1470 - mae: 0.8123 - val_loss: 1.2015 - val_mae: 0.8710\n",
      "Epoch 17/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2524 - mae: 0.8549 - val_loss: 1.1634 - val_mae: 0.8398\n",
      "Epoch 18/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0540 - mae: 0.7839 - val_loss: 1.1975 - val_mae: 0.8309\n",
      "Epoch 19/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1829 - mae: 0.8381 - val_loss: 1.0624 - val_mae: 0.7395\n",
      "Epoch 20/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0770 - mae: 0.7812 - val_loss: 1.0992 - val_mae: 0.7576\n",
      "Epoch 21/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1362 - mae: 0.7962 - val_loss: 1.2011 - val_mae: 0.8277\n",
      "Epoch 22/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0771 - mae: 0.7855 - val_loss: 1.2712 - val_mae: 0.8809\n",
      "Epoch 23/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0977 - mae: 0.7876 - val_loss: 1.2083 - val_mae: 0.7925\n",
      "Epoch 24/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9669 - mae: 0.7565 - val_loss: 1.3932 - val_mae: 0.9434\n",
      "Epoch 25/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0212 - mae: 0.7669 - val_loss: 1.0031 - val_mae: 0.7088\n",
      "Epoch 26/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0020 - mae: 0.7507 - val_loss: 1.1608 - val_mae: 0.7687\n",
      "Epoch 27/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9223 - mae: 0.7289 - val_loss: 1.0443 - val_mae: 0.7248\n",
      "Epoch 28/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9553 - mae: 0.7179 - val_loss: 1.4569 - val_mae: 0.8455\n",
      "Epoch 29/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9529 - mae: 0.7233 - val_loss: 1.1439 - val_mae: 0.7294\n",
      "Epoch 30/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9550 - mae: 0.7153 - val_loss: 0.9921 - val_mae: 0.6895\n",
      "Epoch 31/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9175 - mae: 0.7245 - val_loss: 1.0692 - val_mae: 0.7509\n",
      "Epoch 32/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9860 - mae: 0.7417 - val_loss: 0.9271 - val_mae: 0.6873\n",
      "Epoch 33/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9025 - mae: 0.6950 - val_loss: 1.0588 - val_mae: 0.7333\n",
      "Epoch 34/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9116 - mae: 0.7062 - val_loss: 1.0426 - val_mae: 0.7333\n",
      "Epoch 35/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8775 - mae: 0.7072 - val_loss: 1.0827 - val_mae: 0.7209\n",
      "Epoch 36/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8818 - mae: 0.6903 - val_loss: 1.0520 - val_mae: 0.6988\n",
      "Epoch 37/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9562 - mae: 0.7300 - val_loss: 1.1217 - val_mae: 0.7618\n",
      "Epoch 38/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0168 - mae: 0.7446 - val_loss: 1.0785 - val_mae: 0.7126\n",
      "Epoch 39/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8272 - mae: 0.6647 - val_loss: 1.0867 - val_mae: 0.7515\n",
      "Epoch 40/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8502 - mae: 0.6746 - val_loss: 1.0245 - val_mae: 0.7315\n",
      "Epoch 41/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9182 - mae: 0.7069 - val_loss: 1.2545 - val_mae: 0.7793\n",
      "Epoch 42/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9308 - mae: 0.6946 - val_loss: 0.9637 - val_mae: 0.7355\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9039 - mae: 0.7162 \n",
      "Test Loss: 0.9637, Test MAE: 0.7355\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.3993\n",
      "\n",
      "0: 640x640 1 person, 94.4ms\n",
      "Speed: 3.0ms preprocess, 94.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 640x640 1 person, 201.6ms\n",
      "Speed: 4.8ms preprocess, 201.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.0ms\n",
      "Speed: 1.7ms preprocess, 104.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.4ms\n",
      "Speed: 29.0ms preprocess, 99.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.4ms\n",
      "Speed: 2.7ms preprocess, 136.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 184.6ms\n",
      "Speed: 1.5ms preprocess, 184.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 139.3ms\n",
      "Speed: 2.4ms preprocess, 139.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 211.9ms\n",
      "Speed: 1.6ms preprocess, 211.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 207.2ms\n",
      "Speed: 1.6ms preprocess, 207.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.2ms\n",
      "Speed: 1.8ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.4ms\n",
      "Speed: 1.8ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.0ms\n",
      "Speed: 1.9ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.8ms\n",
      "Speed: 15.5ms preprocess, 125.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 10.9ms preprocess, 116.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 143.9ms\n",
      "Speed: 24.7ms preprocess, 143.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.4ms\n",
      "Speed: 1.6ms preprocess, 100.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.4ms\n",
      "Speed: 1.8ms preprocess, 107.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.1ms\n",
      "Speed: 1.9ms preprocess, 111.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.0ms\n",
      "Speed: 1.7ms preprocess, 102.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.6ms\n",
      "Speed: 9.2ms preprocess, 136.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.0ms\n",
      "Speed: 2.4ms preprocess, 97.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.7ms\n",
      "Speed: 17.3ms preprocess, 100.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.4ms\n",
      "Speed: 11.8ms preprocess, 125.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.6ms\n",
      "Speed: 1.6ms preprocess, 97.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.4ms\n",
      "Speed: 1.7ms preprocess, 92.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 640x640 1 person, 130.7ms\n",
      "Speed: 11.3ms preprocess, 130.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 160.3ms\n",
      "Speed: 16.5ms preprocess, 160.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 198.6ms\n",
      "Speed: 20.1ms preprocess, 198.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.6ms\n",
      "Speed: 15.2ms preprocess, 68.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.2ms\n",
      "Speed: 1.7ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 150.5ms\n",
      "Speed: 22.3ms preprocess, 150.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 239.8ms\n",
      "Speed: 2.0ms preprocess, 239.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.7ms\n",
      "Speed: 2.0ms preprocess, 111.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.0ms\n",
      "Speed: 1.8ms preprocess, 120.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.1ms\n",
      "Speed: 1.7ms preprocess, 96.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.1ms\n",
      "Speed: 12.1ms preprocess, 120.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 172.1ms\n",
      "Speed: 5.0ms preprocess, 172.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.7ms\n",
      "Speed: 1.6ms preprocess, 123.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.0ms\n",
      "Speed: 17.2ms preprocess, 93.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.4ms\n",
      "Speed: 5.2ms preprocess, 146.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.7ms\n",
      "Speed: 1.7ms preprocess, 122.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 152.5ms\n",
      "Speed: 18.1ms preprocess, 152.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.7ms\n",
      "Speed: 1.9ms preprocess, 93.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.7ms\n",
      "Speed: 5.4ms preprocess, 136.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.6ms\n",
      "Speed: 6.6ms preprocess, 126.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.1ms\n",
      "Speed: 1.7ms preprocess, 121.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.1ms\n",
      "Speed: 1.5ms preprocess, 95.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 214.3ms\n",
      "Speed: 1.8ms preprocess, 214.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.1ms\n",
      "Speed: 1.7ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 161.9ms\n",
      "Speed: 2.9ms preprocess, 161.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 108.5ms\n",
      "Speed: 1.5ms preprocess, 108.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.4ms\n",
      "Speed: 2.6ms preprocess, 112.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.2ms\n",
      "Speed: 23.1ms preprocess, 149.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 202.0ms\n",
      "Speed: 9.1ms preprocess, 202.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.7ms\n",
      "Speed: 1.9ms preprocess, 99.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.2ms\n",
      "Speed: 2.2ms preprocess, 113.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 90.1ms\n",
      "Speed: 1.9ms preprocess, 90.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.6ms\n",
      "Speed: 1.9ms preprocess, 114.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.6ms\n",
      "Speed: 8.7ms preprocess, 149.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.6ms\n",
      "Speed: 20.6ms preprocess, 147.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.5ms\n",
      "Speed: 1.9ms preprocess, 92.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.1ms\n",
      "Speed: 2.2ms preprocess, 133.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.9ms\n",
      "Speed: 1.8ms preprocess, 98.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 245.7ms\n",
      "Speed: 2.8ms preprocess, 245.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.5ms\n",
      "Speed: 1.6ms preprocess, 95.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.0ms\n",
      "Speed: 1.8ms preprocess, 121.0ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 252.0ms\n",
      "Speed: 2.2ms preprocess, 252.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.5ms\n",
      "Speed: 1.6ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Average Score for the Video: 2.95\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Hoogspringen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Hoogspringen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hoogspringen/segment_001127.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 7.5322 - mae: 2.3303 - val_loss: 9.8180 - val_mae: 2.9259\n",
      "Epoch 2/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1834 - mae: 1.1821 - val_loss: 7.5410 - val_mae: 2.5450\n",
      "Epoch 3/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7079 - mae: 1.0284 - val_loss: 5.7474 - val_mae: 2.1752\n",
      "Epoch 4/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6637 - mae: 1.0120 - val_loss: 4.0918 - val_mae: 1.7783\n",
      "Epoch 5/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4182 - mae: 0.9416 - val_loss: 3.3264 - val_mae: 1.5735\n",
      "Epoch 6/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2811 - mae: 0.8961 - val_loss: 3.1601 - val_mae: 1.5325\n",
      "Epoch 7/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3129 - mae: 0.8811 - val_loss: 2.1896 - val_mae: 1.2155\n",
      "Epoch 8/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2797 - mae: 0.8832 - val_loss: 1.9107 - val_mae: 1.1241\n",
      "Epoch 9/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1746 - mae: 0.8154 - val_loss: 1.5261 - val_mae: 0.9777\n",
      "Epoch 10/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1968 - mae: 0.8541 - val_loss: 1.4344 - val_mae: 0.9540\n",
      "Epoch 11/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2048 - mae: 0.8432 - val_loss: 1.2092 - val_mae: 0.8680\n",
      "Epoch 12/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1800 - mae: 0.8364 - val_loss: 1.0689 - val_mae: 0.7959\n",
      "Epoch 13/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1529 - mae: 0.8004 - val_loss: 1.1791 - val_mae: 0.8620\n",
      "Epoch 14/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1657 - mae: 0.8162 - val_loss: 1.0408 - val_mae: 0.7748\n",
      "Epoch 15/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1061 - mae: 0.7928 - val_loss: 0.9821 - val_mae: 0.7121\n",
      "Epoch 16/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1058 - mae: 0.7989 - val_loss: 0.9463 - val_mae: 0.7142\n",
      "Epoch 17/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0154 - mae: 0.7589 - val_loss: 1.0067 - val_mae: 0.7305\n",
      "Epoch 18/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9976 - mae: 0.7466 - val_loss: 1.1766 - val_mae: 0.8398\n",
      "Epoch 19/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0430 - mae: 0.7696 - val_loss: 0.9869 - val_mae: 0.7044\n",
      "Epoch 20/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0000 - mae: 0.7515 - val_loss: 0.9756 - val_mae: 0.7225\n",
      "Epoch 21/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0830 - mae: 0.7872 - val_loss: 1.2476 - val_mae: 0.8662\n",
      "Epoch 22/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9866 - mae: 0.7423 - val_loss: 1.1791 - val_mae: 0.7862\n",
      "Epoch 23/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0698 - mae: 0.7599 - val_loss: 1.0195 - val_mae: 0.7672\n",
      "Epoch 24/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9521 - mae: 0.7423 - val_loss: 1.0942 - val_mae: 0.8097\n",
      "Epoch 25/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9412 - mae: 0.7223 - val_loss: 0.9573 - val_mae: 0.7325\n",
      "Epoch 26/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9439 - mae: 0.7198 - val_loss: 1.0235 - val_mae: 0.7454\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1135 - mae: 0.7714\n",
      "Test Loss: 1.0235, Test MAE: 0.7454\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.3519\n",
      "\n",
      "0: 384x640 1 person, 178.5ms\n",
      "Speed: 2.0ms preprocess, 178.5ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.4ms\n",
      "Speed: 0.9ms preprocess, 55.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 148.0ms\n",
      "Speed: 11.6ms preprocess, 148.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 1.2ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.6ms\n",
      "Speed: 0.8ms preprocess, 174.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.7ms\n",
      "Speed: 0.8ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 107.8ms\n",
      "Speed: 19.0ms preprocess, 107.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 143.1ms\n",
      "Speed: 0.8ms preprocess, 143.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.9ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.3ms\n",
      "Speed: 1.6ms preprocess, 62.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 143.9ms\n",
      "Speed: 20.8ms preprocess, 143.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.4ms\n",
      "Speed: 0.8ms preprocess, 61.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 0.9ms preprocess, 67.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 0.8ms preprocess, 60.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.1ms\n",
      "Speed: 0.9ms preprocess, 80.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 1.0ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 0.9ms preprocess, 72.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.4ms\n",
      "Speed: 0.8ms preprocess, 63.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.8ms preprocess, 75.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.1ms\n",
      "Speed: 1.0ms preprocess, 80.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.4ms\n",
      "Speed: 0.9ms preprocess, 83.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.0ms\n",
      "Speed: 0.9ms preprocess, 68.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 0.9ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.3ms\n",
      "Speed: 1.0ms preprocess, 92.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 1.0ms preprocess, 81.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 159.1ms\n",
      "Speed: 10.8ms preprocess, 159.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.2ms\n",
      "Speed: 1.0ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.6ms\n",
      "Speed: 1.0ms preprocess, 68.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 1.2ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 1.0ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.3ms\n",
      "Speed: 0.9ms preprocess, 90.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.8ms\n",
      "Speed: 1.2ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.1ms\n",
      "Speed: 1.0ms preprocess, 67.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.7ms\n",
      "Speed: 1.5ms preprocess, 70.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 1.2ms preprocess, 74.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.7ms\n",
      "Speed: 2.7ms preprocess, 65.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.9ms\n",
      "Speed: 1.1ms preprocess, 89.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.3ms\n",
      "Speed: 1.2ms preprocess, 78.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 135.2ms\n",
      "Speed: 24.5ms preprocess, 135.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 1.2ms preprocess, 61.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.0ms\n",
      "Speed: 1.2ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.9ms\n",
      "Speed: 0.9ms preprocess, 76.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.9ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.7ms\n",
      "Speed: 0.9ms preprocess, 63.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.5ms\n",
      "Speed: 1.1ms preprocess, 186.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.9ms preprocess, 68.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 1.0ms preprocess, 79.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.5ms\n",
      "Speed: 0.9ms preprocess, 64.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.5ms\n",
      "Speed: 1.0ms preprocess, 79.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.4ms\n",
      "Speed: 1.0ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.3ms\n",
      "Speed: 1.0ms preprocess, 65.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.9ms\n",
      "Speed: 0.8ms preprocess, 58.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.4ms\n",
      "Speed: 1.0ms preprocess, 80.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.3ms\n",
      "Speed: 1.0ms preprocess, 81.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.6ms\n",
      "Speed: 1.0ms preprocess, 186.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.1ms\n",
      "Speed: 0.9ms preprocess, 56.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.6ms\n",
      "Speed: 17.8ms preprocess, 85.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.5ms\n",
      "Speed: 1.1ms preprocess, 63.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.6ms\n",
      "Speed: 0.8ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.9ms preprocess, 69.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 1.5ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 0.9ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.9ms\n",
      "Speed: 1.6ms preprocess, 65.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.9ms\n",
      "Speed: 0.8ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.4ms\n",
      "Speed: 0.9ms preprocess, 76.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 1.0ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 1.0ms preprocess, 74.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 1.3ms preprocess, 83.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 1.0ms preprocess, 67.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.5ms\n",
      "Speed: 0.8ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.5ms\n",
      "Speed: 1.0ms preprocess, 87.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 210.4ms\n",
      "Speed: 1.7ms preprocess, 210.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.9ms\n",
      "Speed: 0.8ms preprocess, 62.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.5ms\n",
      "Speed: 0.9ms preprocess, 81.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 0.8ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 300.3ms\n",
      "Speed: 1.0ms preprocess, 300.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.4ms\n",
      "Speed: 0.7ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 29.1ms preprocess, 67.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.2ms\n",
      "Speed: 0.7ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.3ms\n",
      "Speed: 0.9ms preprocess, 53.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.2ms\n",
      "Speed: 1.5ms preprocess, 55.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.4ms\n",
      "Speed: 0.9ms preprocess, 64.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 154.6ms\n",
      "Speed: 10.1ms preprocess, 154.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 0.9ms preprocess, 83.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.4ms\n",
      "Speed: 0.8ms preprocess, 81.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.6ms\n",
      "Speed: 0.9ms preprocess, 78.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 6.3ms preprocess, 61.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.6ms\n",
      "Speed: 0.8ms preprocess, 73.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.4ms\n",
      "Speed: 0.9ms preprocess, 63.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.9ms\n",
      "Speed: 1.0ms preprocess, 82.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.1ms\n",
      "Speed: 1.0ms preprocess, 87.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.4ms\n",
      "Speed: 1.0ms preprocess, 81.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 140.0ms\n",
      "Speed: 17.2ms preprocess, 140.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 1.0ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 0.9ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.8ms\n",
      "Speed: 0.9ms preprocess, 89.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.7ms\n",
      "Speed: 0.8ms preprocess, 58.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.3ms\n",
      "Speed: 0.8ms preprocess, 73.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.1ms\n",
      "Speed: 1.0ms preprocess, 85.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.8ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.8ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 (no detections), 148.5ms\n",
      "Speed: 19.9ms preprocess, 148.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 64.3ms\n",
      "Speed: 1.0ms preprocess, 64.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.2ms\n",
      "Speed: 4.9ms preprocess, 101.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 0.9ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 1.0ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.2ms\n",
      "Speed: 0.9ms preprocess, 81.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.3ms\n",
      "Speed: 0.9ms preprocess, 82.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 1.1ms preprocess, 75.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.9ms preprocess, 74.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 0.9ms preprocess, 74.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.6ms\n",
      "Speed: 1.1ms preprocess, 78.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 157.2ms\n",
      "Speed: 13.2ms preprocess, 157.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 0.9ms preprocess, 68.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.5ms\n",
      "Speed: 0.8ms preprocess, 68.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 1.0ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 14.6ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 1.2ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 0.9ms preprocess, 74.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.8ms\n",
      "Speed: 1.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 105.6ms\n",
      "Speed: 1.2ms preprocess, 105.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 127.5ms\n",
      "Speed: 1.0ms preprocess, 127.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 149.8ms\n",
      "Speed: 6.1ms preprocess, 149.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.1ms\n",
      "Speed: 0.9ms preprocess, 64.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.5ms\n",
      "Speed: 1.1ms preprocess, 103.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.5ms\n",
      "Speed: 0.9ms preprocess, 52.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 119.0ms\n",
      "Speed: 1.9ms preprocess, 119.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 0.9ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 0.9ms preprocess, 74.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 1.3ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.2ms\n",
      "Speed: 2.1ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.9ms\n",
      "Speed: 0.9ms preprocess, 53.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.3ms\n",
      "Speed: 1.1ms preprocess, 51.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.8ms\n",
      "Speed: 1.4ms preprocess, 83.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.5ms\n",
      "Speed: 1.3ms preprocess, 94.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.7ms\n",
      "Speed: 9.0ms preprocess, 96.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.5ms\n",
      "Speed: 1.0ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 1.2ms preprocess, 62.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.8ms\n",
      "Speed: 1.0ms preprocess, 59.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.7ms\n",
      "Speed: 0.8ms preprocess, 55.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.2ms\n",
      "Speed: 0.9ms preprocess, 57.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.0ms preprocess, 69.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.5ms\n",
      "Speed: 1.2ms preprocess, 86.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.4ms\n",
      "Speed: 1.0ms preprocess, 66.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 1.1ms preprocess, 63.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 1.1ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.3ms\n",
      "Speed: 1.0ms preprocess, 76.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.8ms\n",
      "Speed: 1.0ms preprocess, 65.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 130.2ms\n",
      "Speed: 1.3ms preprocess, 130.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.2ms\n",
      "Speed: 1.1ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 0.9ms preprocess, 62.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.4ms\n",
      "Speed: 1.4ms preprocess, 77.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.4ms\n",
      "Speed: 1.1ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 1.0ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.3ms\n",
      "Speed: 0.9ms preprocess, 54.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.9ms preprocess, 52.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 0.9ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.8ms preprocess, 52.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 0.9ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.8ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.9ms\n",
      "Speed: 1.1ms preprocess, 54.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.3ms\n",
      "Speed: 0.9ms preprocess, 53.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.3ms\n",
      "Speed: 0.9ms preprocess, 54.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.6ms\n",
      "Speed: 0.8ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 1.0ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.5ms\n",
      "Speed: 1.0ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.4ms\n",
      "Speed: 11.0ms preprocess, 62.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.3ms\n",
      "Speed: 0.8ms preprocess, 52.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.3ms\n",
      "Speed: 0.8ms preprocess, 53.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.7ms\n",
      "Speed: 0.9ms preprocess, 56.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.5ms\n",
      "Speed: 0.9ms preprocess, 52.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.5ms\n",
      "Speed: 1.0ms preprocess, 51.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.5ms\n",
      "Speed: 1.1ms preprocess, 52.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.4ms\n",
      "Speed: 0.8ms preprocess, 53.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.6ms\n",
      "Speed: 1.0ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 0.9ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.3ms\n",
      "Speed: 0.9ms preprocess, 53.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 0.8ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.7ms\n",
      "Speed: 0.8ms preprocess, 51.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.5ms\n",
      "Speed: 1.1ms preprocess, 56.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.4ms\n",
      "Speed: 0.9ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.2ms\n",
      "Speed: 0.9ms preprocess, 54.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.7ms\n",
      "Speed: 1.1ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 1.1ms preprocess, 53.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.7ms\n",
      "Speed: 0.9ms preprocess, 58.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 1.0ms preprocess, 54.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 10.7ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.9ms\n",
      "Speed: 0.9ms preprocess, 53.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.3ms\n",
      "Speed: 1.0ms preprocess, 51.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.8ms\n",
      "Speed: 1.3ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 1.1ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 1.0ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.6ms\n",
      "Speed: 0.8ms preprocess, 51.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 0.8ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.9ms\n",
      "Speed: 1.2ms preprocess, 54.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.1ms\n",
      "Speed: 1.0ms preprocess, 56.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.6ms\n",
      "Speed: 0.9ms preprocess, 54.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.8ms\n",
      "Speed: 0.8ms preprocess, 52.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 0.8ms preprocess, 52.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 0.8ms preprocess, 50.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 1.1ms preprocess, 50.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 0.9ms preprocess, 49.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.4ms\n",
      "Speed: 1.0ms preprocess, 51.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 1.0ms preprocess, 54.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.4ms\n",
      "Speed: 1.4ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 1.0ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.6ms\n",
      "Speed: 1.0ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 1.1ms preprocess, 49.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.3ms\n",
      "Speed: 0.8ms preprocess, 56.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 0.9ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.1ms\n",
      "Speed: 0.8ms preprocess, 56.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 1.0ms preprocess, 61.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.8ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.6ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.4ms\n",
      "Speed: 2.6ms preprocess, 57.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 0.6ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 0.7ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.4ms\n",
      "Speed: 5.1ms preprocess, 51.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.9ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.7ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.6ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.6ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 3.0ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.6ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.6ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.6ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.2ms\n",
      "Speed: 0.7ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.6ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.6ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.7ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.6ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 3.4ms preprocess, 46.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.6ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.6ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.5ms\n",
      "Speed: 0.6ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.6ms\n",
      "Speed: 0.9ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.5ms\n",
      "Speed: 0.6ms preprocess, 54.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.6ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.8ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.6ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.6ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 2.9ms preprocess, 47.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.6ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.6ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.6ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.6ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.6ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.6ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.6ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 4.0ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.6ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.9ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.6ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.6ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.4ms\n",
      "Speed: 3.4ms preprocess, 63.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.8ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.6ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.9ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 3.3ms preprocess, 49.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.6ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.6ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.6ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.9ms\n",
      "Speed: 3.4ms preprocess, 89.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.9ms\n",
      "Speed: 4.2ms preprocess, 56.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.6ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.6ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 2.5ms preprocess, 50.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.9ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 2.2ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.9ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.6ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.9ms\n",
      "Speed: 0.8ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.7ms\n",
      "Speed: 1.3ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 5.5ms preprocess, 72.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 0.8ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 0.7ms preprocess, 43.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.6ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.8ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 0.9ms preprocess, 47.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.6ms\n",
      "Speed: 1.5ms preprocess, 48.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.5ms\n",
      "Speed: 5.5ms preprocess, 51.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.6ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 0.8ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.9ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.8ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 0.6ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 5.5ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 0.7ms preprocess, 46.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 1.0ms preprocess, 47.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.7ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.7ms\n",
      "Speed: 0.8ms preprocess, 53.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 5.4ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.6ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.1ms\n",
      "Speed: 0.9ms preprocess, 59.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.1ms\n",
      "Speed: 1.0ms preprocess, 51.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.3ms\n",
      "Speed: 4.8ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.0ms\n",
      "Speed: 0.9ms preprocess, 90.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.4ms\n",
      "Speed: 0.7ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.9ms\n",
      "Speed: 7.6ms preprocess, 80.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 1.4ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.2ms\n",
      "Speed: 5.3ms preprocess, 52.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.7ms preprocess, 74.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.5ms\n",
      "Speed: 2.7ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.9ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Average Score for the Video: 4.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Hordelopen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Hordelopen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hordenlopen/segment_001695.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.9281 - mae: 2.4646 - val_loss: 6.9331 - val_mae: 2.4701\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7024 - mae: 1.0307 - val_loss: 5.7266 - val_mae: 2.2298\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1657 - mae: 0.8475 - val_loss: 4.2442 - val_mae: 1.9067\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9887 - mae: 0.7796 - val_loss: 3.3658 - val_mae: 1.6843\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8428 - mae: 0.7315 - val_loss: 2.6030 - val_mae: 1.4702\n",
      "Epoch 6/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7213 - mae: 0.6568 - val_loss: 2.3963 - val_mae: 1.4081\n",
      "Epoch 7/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6570 - mae: 0.6303 - val_loss: 1.9346 - val_mae: 1.2538\n",
      "Epoch 8/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5547 - mae: 0.5757 - val_loss: 1.3652 - val_mae: 1.0013\n",
      "Epoch 9/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6163 - mae: 0.6070 - val_loss: 1.2637 - val_mae: 0.9803\n",
      "Epoch 10/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5374 - mae: 0.5741 - val_loss: 0.8456 - val_mae: 0.7588\n",
      "Epoch 11/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5823 - mae: 0.5919 - val_loss: 0.7646 - val_mae: 0.7143\n",
      "Epoch 12/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5430 - mae: 0.5637 - val_loss: 0.8186 - val_mae: 0.7745\n",
      "Epoch 13/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4783 - mae: 0.5351 - val_loss: 0.3500 - val_mae: 0.4290\n",
      "Epoch 14/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4888 - mae: 0.5324 - val_loss: 0.5812 - val_mae: 0.6319\n",
      "Epoch 15/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4623 - mae: 0.5153 - val_loss: 0.3361 - val_mae: 0.4358\n",
      "Epoch 16/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4326 - mae: 0.4987 - val_loss: 0.6753 - val_mae: 0.7031\n",
      "Epoch 17/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4939 - mae: 0.5364 - val_loss: 0.4286 - val_mae: 0.5070\n",
      "Epoch 18/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4222 - mae: 0.4936 - val_loss: 0.4040 - val_mae: 0.4948\n",
      "Epoch 19/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4208 - mae: 0.4990 - val_loss: 0.3320 - val_mae: 0.4244\n",
      "Epoch 20/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4272 - mae: 0.4989 - val_loss: 0.4756 - val_mae: 0.5458\n",
      "Epoch 21/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4703 - mae: 0.5097 - val_loss: 0.3199 - val_mae: 0.4179\n",
      "Epoch 22/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3625 - mae: 0.4676 - val_loss: 0.3182 - val_mae: 0.4166\n",
      "Epoch 23/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4171 - mae: 0.4899 - val_loss: 0.3373 - val_mae: 0.4361\n",
      "Epoch 24/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3702 - mae: 0.4533 - val_loss: 0.5260 - val_mae: 0.5746\n",
      "Epoch 25/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4532 - mae: 0.5146 - val_loss: 0.2973 - val_mae: 0.4082\n",
      "Epoch 26/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4168 - mae: 0.4865 - val_loss: 0.2731 - val_mae: 0.3820\n",
      "Epoch 27/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3745 - mae: 0.4594 - val_loss: 0.4074 - val_mae: 0.5189\n",
      "Epoch 28/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4221 - mae: 0.4850 - val_loss: 0.3048 - val_mae: 0.4009\n",
      "Epoch 29/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3518 - mae: 0.4503 - val_loss: 0.2900 - val_mae: 0.3901\n",
      "Epoch 30/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3613 - mae: 0.4477 - val_loss: 0.5071 - val_mae: 0.5569\n",
      "Epoch 31/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3505 - mae: 0.4357 - val_loss: 0.3231 - val_mae: 0.4248\n",
      "Epoch 32/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3509 - mae: 0.4420 - val_loss: 0.3491 - val_mae: 0.4322\n",
      "Epoch 33/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4098 - mae: 0.4732 - val_loss: 0.2859 - val_mae: 0.3811\n",
      "Epoch 34/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3582 - mae: 0.4489 - val_loss: 0.2968 - val_mae: 0.4002\n",
      "Epoch 35/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3704 - mae: 0.4584 - val_loss: 0.3349 - val_mae: 0.4268\n",
      "Epoch 36/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3402 - mae: 0.4343 - val_loss: 0.3113 - val_mae: 0.4211\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3299 - mae: 0.4136 \n",
      "Test Loss: 0.3113, Test MAE: 0.4211\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.6781\n",
      "\n",
      "0: 384x640 3 persons, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.9ms\n",
      "Speed: 1.6ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 55.0ms\n",
      "Speed: 3.5ms preprocess, 55.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.8ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.4ms\n",
      "Speed: 0.9ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 0.9ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.9ms\n",
      "Speed: 0.9ms preprocess, 47.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.4ms\n",
      "Speed: 0.6ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.4ms\n",
      "Speed: 0.6ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.0ms\n",
      "Speed: 0.6ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.6ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 1.0ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.9ms\n",
      "Speed: 0.6ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 68.0ms\n",
      "Speed: 2.3ms preprocess, 68.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.3ms\n",
      "Speed: 0.9ms preprocess, 58.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.5ms\n",
      "Speed: 0.7ms preprocess, 50.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.0ms\n",
      "Speed: 1.1ms preprocess, 58.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.7ms\n",
      "Speed: 1.4ms preprocess, 49.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.2ms\n",
      "Speed: 5.7ms preprocess, 54.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.8ms\n",
      "Speed: 0.9ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.9ms\n",
      "Speed: 0.9ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.9ms\n",
      "Speed: 0.7ms preprocess, 49.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 59.5ms\n",
      "Speed: 1.1ms preprocess, 59.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 67.8ms\n",
      "Speed: 0.9ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 60.4ms\n",
      "Speed: 6.5ms preprocess, 60.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.7ms\n",
      "Speed: 0.8ms preprocess, 66.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 69.0ms\n",
      "Speed: 0.8ms preprocess, 69.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.1ms\n",
      "Speed: 0.8ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.7ms\n",
      "Speed: 1.0ms preprocess, 66.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.2ms\n",
      "Speed: 0.9ms preprocess, 57.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.6ms\n",
      "Speed: 0.9ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.5ms\n",
      "Speed: 0.9ms preprocess, 51.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.8ms\n",
      "Speed: 0.9ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.0ms\n",
      "Speed: 0.8ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 65.6ms\n",
      "Speed: 0.8ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.2ms\n",
      "Speed: 0.9ms preprocess, 57.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.4ms\n",
      "Speed: 0.8ms preprocess, 56.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.9ms\n",
      "Speed: 0.8ms preprocess, 50.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.0ms\n",
      "Speed: 0.9ms preprocess, 57.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.7ms\n",
      "Speed: 1.6ms preprocess, 51.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 55.3ms\n",
      "Speed: 0.9ms preprocess, 55.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 59.7ms\n",
      "Speed: 0.9ms preprocess, 59.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.5ms\n",
      "Speed: 0.8ms preprocess, 49.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.9ms\n",
      "Speed: 1.1ms preprocess, 54.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.8ms\n",
      "Speed: 0.9ms preprocess, 51.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.3ms\n",
      "Speed: 1.3ms preprocess, 54.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.4ms\n",
      "Speed: 0.8ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.1ms\n",
      "Speed: 5.7ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 55.8ms\n",
      "Speed: 1.8ms preprocess, 55.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 68.3ms\n",
      "Speed: 0.9ms preprocess, 68.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 93.4ms\n",
      "Speed: 0.9ms preprocess, 93.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.8ms\n",
      "Speed: 1.4ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 78.5ms\n",
      "Speed: 0.9ms preprocess, 78.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 67.6ms\n",
      "Speed: 0.8ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.3ms\n",
      "Speed: 0.8ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 64.7ms\n",
      "Speed: 1.1ms preprocess, 64.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.8ms\n",
      "Speed: 0.9ms preprocess, 58.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.7ms\n",
      "Speed: 0.7ms preprocess, 56.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.7ms\n",
      "Speed: 1.0ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.2ms\n",
      "Speed: 1.0ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.1ms\n",
      "Speed: 0.9ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 64.4ms\n",
      "Speed: 0.9ms preprocess, 64.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.4ms\n",
      "Speed: 6.5ms preprocess, 58.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.0ms\n",
      "Speed: 0.7ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 55.2ms\n",
      "Speed: 0.9ms preprocess, 55.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.1ms\n",
      "Speed: 0.9ms preprocess, 54.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 64.8ms\n",
      "Speed: 0.7ms preprocess, 64.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.6ms\n",
      "Speed: 0.8ms preprocess, 54.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.5ms\n",
      "Speed: 1.0ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 67.6ms\n",
      "Speed: 1.0ms preprocess, 67.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 96.4ms\n",
      "Speed: 1.2ms preprocess, 96.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.7ms\n",
      "Speed: 0.9ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 68.1ms\n",
      "Speed: 0.9ms preprocess, 68.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.5ms\n",
      "Speed: 0.9ms preprocess, 54.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.2ms\n",
      "Speed: 0.7ms preprocess, 51.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 133.4ms\n",
      "Speed: 0.8ms preprocess, 133.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.8ms\n",
      "Speed: 1.0ms preprocess, 54.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 60.3ms\n",
      "Speed: 1.3ms preprocess, 60.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 83.7ms\n",
      "Speed: 2.3ms preprocess, 83.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 64.9ms\n",
      "Speed: 1.1ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.9ms\n",
      "Speed: 0.8ms preprocess, 51.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.1ms\n",
      "Speed: 0.7ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.0ms\n",
      "Speed: 0.8ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.2ms\n",
      "Speed: 0.8ms preprocess, 57.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 55.7ms\n",
      "Speed: 1.0ms preprocess, 55.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.6ms\n",
      "Speed: 0.8ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.3ms\n",
      "Speed: 0.7ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.6ms\n",
      "Speed: 0.8ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.9ms\n",
      "Speed: 0.8ms preprocess, 56.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.3ms\n",
      "Speed: 0.8ms preprocess, 52.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.1ms\n",
      "Speed: 1.0ms preprocess, 56.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 104.0ms\n",
      "Speed: 1.5ms preprocess, 104.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.8ms\n",
      "Speed: 0.9ms preprocess, 51.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.7ms\n",
      "Speed: 0.7ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.7ms\n",
      "Speed: 0.7ms preprocess, 53.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 63.7ms\n",
      "Speed: 0.7ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 93.0ms\n",
      "Speed: 2.6ms preprocess, 93.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.0ms\n",
      "Speed: 0.6ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.7ms\n",
      "Speed: 10.4ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.9ms\n",
      "Speed: 1.1ms preprocess, 58.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.3ms\n",
      "Speed: 0.8ms preprocess, 54.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.4ms\n",
      "Speed: 0.7ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.8ms\n",
      "Speed: 0.9ms preprocess, 53.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.4ms\n",
      "Speed: 0.7ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.5ms\n",
      "Speed: 0.7ms preprocess, 48.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.7ms\n",
      "Speed: 1.1ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.9ms\n",
      "Speed: 1.1ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.0ms\n",
      "Speed: 1.5ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 60.2ms\n",
      "Speed: 0.9ms preprocess, 60.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.6ms\n",
      "Speed: 0.8ms preprocess, 52.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.8ms\n",
      "Speed: 0.8ms preprocess, 50.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.7ms\n",
      "Speed: 0.9ms preprocess, 47.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.4ms\n",
      "Speed: 0.8ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.1ms\n",
      "Speed: 0.8ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.9ms\n",
      "Speed: 1.1ms preprocess, 52.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.4ms\n",
      "Speed: 0.9ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 70.4ms\n",
      "Speed: 2.8ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 58.0ms\n",
      "Speed: 6.7ms preprocess, 58.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.8ms\n",
      "Speed: 3.5ms preprocess, 57.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.5ms\n",
      "Speed: 0.7ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.0ms\n",
      "Speed: 0.9ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.2ms\n",
      "Speed: 1.0ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.2ms\n",
      "Speed: 0.9ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.2ms\n",
      "Speed: 0.8ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.4ms\n",
      "Speed: 1.2ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.0ms\n",
      "Speed: 0.9ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.0ms\n",
      "Speed: 0.9ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.7ms\n",
      "Speed: 0.9ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.1ms\n",
      "Speed: 0.8ms preprocess, 53.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.1ms\n",
      "Speed: 0.9ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.9ms\n",
      "Speed: 1.0ms preprocess, 46.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.0ms\n",
      "Speed: 0.9ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 66.8ms\n",
      "Speed: 4.4ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.0ms\n",
      "Speed: 0.9ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.2ms\n",
      "Speed: 0.8ms preprocess, 48.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 75.0ms\n",
      "Speed: 0.7ms preprocess, 75.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.1ms\n",
      "Speed: 0.8ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.7ms\n",
      "Speed: 1.2ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 1.1ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.9ms\n",
      "Speed: 0.7ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.4ms\n",
      "Speed: 0.8ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.4ms\n",
      "Speed: 0.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 0.9ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.5ms\n",
      "Speed: 0.9ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.0ms\n",
      "Speed: 0.7ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 57.6ms\n",
      "Speed: 4.0ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.8ms\n",
      "Speed: 0.8ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.9ms\n",
      "Speed: 0.8ms preprocess, 53.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.8ms\n",
      "Speed: 0.9ms preprocess, 48.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 101.0ms\n",
      "Speed: 1.1ms preprocess, 101.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.1ms\n",
      "Speed: 0.9ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 50.1ms\n",
      "Speed: 1.0ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.4ms\n",
      "Speed: 0.7ms preprocess, 47.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 50.4ms\n",
      "Speed: 0.7ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.9ms\n",
      "Speed: 0.8ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.5ms\n",
      "Speed: 0.8ms preprocess, 48.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.0ms\n",
      "Speed: 2.6ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.2ms\n",
      "Speed: 0.9ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.1ms\n",
      "Speed: 0.9ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.0ms\n",
      "Speed: 0.9ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.0ms\n",
      "Speed: 0.8ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 65.4ms\n",
      "Speed: 3.6ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.0ms\n",
      "Speed: 0.6ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.7ms\n",
      "Speed: 1.0ms preprocess, 49.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.8ms\n",
      "Speed: 0.9ms preprocess, 52.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 57.6ms\n",
      "Speed: 0.9ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 4 persons, 65.5ms\n",
      "Speed: 0.8ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 384x640 4 persons, 97.7ms\n",
      "Speed: 2.2ms preprocess, 97.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 101.1ms\n",
      "Speed: 1.1ms preprocess, 101.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 51.3ms\n",
      "Speed: 0.9ms preprocess, 51.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 4 persons, 56.5ms\n",
      "Speed: 1.0ms preprocess, 56.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 62.1ms\n",
      "Speed: 0.8ms preprocess, 62.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 72.6ms\n",
      "Speed: 1.1ms preprocess, 72.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 141.8ms\n",
      "Speed: 1.2ms preprocess, 141.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 64.9ms\n",
      "Speed: 1.4ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 67.1ms\n",
      "Speed: 1.2ms preprocess, 67.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 78.6ms\n",
      "Speed: 2.5ms preprocess, 78.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 74.9ms\n",
      "Speed: 7.2ms preprocess, 74.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.0ms\n",
      "Speed: 1.2ms preprocess, 52.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.7ms\n",
      "Speed: 0.9ms preprocess, 52.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 51.8ms\n",
      "Speed: 1.3ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.9ms\n",
      "Speed: 0.9ms preprocess, 52.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 50.6ms\n",
      "Speed: 0.7ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.0ms\n",
      "Speed: 0.8ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 54.4ms\n",
      "Speed: 0.9ms preprocess, 54.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.2ms\n",
      "Speed: 0.9ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 55.0ms\n",
      "Speed: 0.9ms preprocess, 55.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.2ms\n",
      "Speed: 1.0ms preprocess, 49.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.0ms\n",
      "Speed: 0.7ms preprocess, 48.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.7ms\n",
      "Speed: 0.9ms preprocess, 49.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 53.6ms\n",
      "Speed: 1.0ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.8ms\n",
      "Speed: 0.7ms preprocess, 46.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 59.9ms\n",
      "Speed: 6.3ms preprocess, 59.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 50.5ms\n",
      "Speed: 0.9ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.7ms\n",
      "Speed: 0.9ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.4ms\n",
      "Speed: 0.8ms preprocess, 47.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.5ms\n",
      "Speed: 0.7ms preprocess, 47.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.2ms\n",
      "Speed: 0.8ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.0ms\n",
      "Speed: 0.8ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.3ms\n",
      "Speed: 0.8ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.1ms\n",
      "Speed: 0.7ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 51.2ms\n",
      "Speed: 0.8ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.7ms\n",
      "Speed: 0.8ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 54.4ms\n",
      "Speed: 0.8ms preprocess, 54.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 62.5ms\n",
      "Speed: 3.8ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 183.6ms\n",
      "Speed: 1.6ms preprocess, 183.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 116.6ms\n",
      "Speed: 31.3ms preprocess, 116.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 4 persons, 53.5ms\n",
      "Speed: 0.9ms preprocess, 53.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 56.1ms\n",
      "Speed: 0.9ms preprocess, 56.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 65.4ms\n",
      "Speed: 6.4ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.2ms\n",
      "Speed: 0.8ms preprocess, 48.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.3ms\n",
      "Speed: 0.7ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 81.1ms\n",
      "Speed: 1.0ms preprocess, 81.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 62.1ms\n",
      "Speed: 0.9ms preprocess, 62.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 58.1ms\n",
      "Speed: 1.2ms preprocess, 58.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 54.6ms\n",
      "Speed: 1.0ms preprocess, 54.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.5ms\n",
      "Speed: 1.2ms preprocess, 49.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.2ms\n",
      "Speed: 0.9ms preprocess, 47.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 51.5ms\n",
      "Speed: 0.8ms preprocess, 51.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.9ms\n",
      "Speed: 0.7ms preprocess, 49.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.9ms\n",
      "Speed: 0.7ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 53.0ms\n",
      "Speed: 1.0ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.3ms\n",
      "Speed: 0.8ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 69.6ms\n",
      "Speed: 3.0ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.3ms\n",
      "Speed: 0.7ms preprocess, 52.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 51.7ms\n",
      "Speed: 2.3ms preprocess, 51.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.3ms\n",
      "Speed: 0.7ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 52.6ms\n",
      "Speed: 0.7ms preprocess, 52.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 54.1ms\n",
      "Speed: 0.8ms preprocess, 54.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.3ms\n",
      "Speed: 0.9ms preprocess, 47.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.6ms\n",
      "Speed: 0.6ms preprocess, 47.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.7ms\n",
      "Speed: 0.9ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.1ms\n",
      "Speed: 1.3ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 50.8ms\n",
      "Speed: 0.7ms preprocess, 50.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 4 persons, 64.0ms\n",
      "Speed: 8.7ms preprocess, 64.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.6ms\n",
      "Speed: 0.7ms preprocess, 46.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.1ms\n",
      "Speed: 0.9ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 56.6ms\n",
      "Speed: 0.8ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.2ms\n",
      "Speed: 0.7ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 55.5ms\n",
      "Speed: 0.7ms preprocess, 55.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 47.3ms\n",
      "Speed: 0.8ms preprocess, 47.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.4ms\n",
      "Speed: 0.8ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.4ms\n",
      "Speed: 0.9ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.9ms\n",
      "Speed: 0.7ms preprocess, 48.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 58.5ms\n",
      "Speed: 2.8ms preprocess, 58.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 46.4ms\n",
      "Speed: 1.0ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.5ms\n",
      "Speed: 1.2ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 43.7ms\n",
      "Speed: 0.8ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 49.6ms\n",
      "Speed: 0.7ms preprocess, 49.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.4ms\n",
      "Speed: 5.1ms preprocess, 56.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.7ms\n",
      "Speed: 0.9ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.2ms\n",
      "Speed: 0.7ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.9ms\n",
      "Speed: 0.6ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.0ms\n",
      "Speed: 5.9ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.6ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.6ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.6ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 2.1ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Average Score for the Video: 5.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Kogelstoten.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Kogelstoten.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Kogelstoten/segment_000265.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.5406 - mae: 2.4022 - val_loss: 8.2534 - val_mae: 2.6719\n",
      "Epoch 2/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3931 - mae: 0.9420 - val_loss: 6.9330 - val_mae: 2.4303\n",
      "Epoch 3/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3629 - mae: 0.9547 - val_loss: 4.5341 - val_mae: 1.9712\n",
      "Epoch 4/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1609 - mae: 0.8723 - val_loss: 3.4644 - val_mae: 1.7254\n",
      "Epoch 5/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9518 - mae: 0.7743 - val_loss: 2.5485 - val_mae: 1.4907\n",
      "Epoch 6/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9454 - mae: 0.7774 - val_loss: 1.9388 - val_mae: 1.3008\n",
      "Epoch 7/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8222 - mae: 0.7070 - val_loss: 1.5772 - val_mae: 1.1459\n",
      "Epoch 8/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7854 - mae: 0.6980 - val_loss: 1.4682 - val_mae: 1.0950\n",
      "Epoch 9/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7976 - mae: 0.7045 - val_loss: 1.2359 - val_mae: 1.0208\n",
      "Epoch 10/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7524 - mae: 0.6841 - val_loss: 0.9967 - val_mae: 0.8444\n",
      "Epoch 11/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7332 - mae: 0.6803 - val_loss: 0.7770 - val_mae: 0.7392\n",
      "Epoch 12/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7788 - mae: 0.6849 - val_loss: 0.6929 - val_mae: 0.7032\n",
      "Epoch 13/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6881 - mae: 0.6539 - val_loss: 0.5204 - val_mae: 0.5585\n",
      "Epoch 14/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6580 - mae: 0.6302 - val_loss: 0.5769 - val_mae: 0.5413\n",
      "Epoch 15/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7511 - mae: 0.6790 - val_loss: 0.6606 - val_mae: 0.5851\n",
      "Epoch 16/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6526 - mae: 0.6185 - val_loss: 0.8635 - val_mae: 0.7136\n",
      "Epoch 17/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7131 - mae: 0.6446 - val_loss: 0.6273 - val_mae: 0.5441\n",
      "Epoch 18/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5969 - mae: 0.5895 - val_loss: 0.5605 - val_mae: 0.5676\n",
      "Epoch 19/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6524 - mae: 0.6143 - val_loss: 0.3940 - val_mae: 0.4817\n",
      "Epoch 20/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5921 - mae: 0.5885 - val_loss: 0.5022 - val_mae: 0.5143\n",
      "Epoch 21/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6060 - mae: 0.5966 - val_loss: 0.4337 - val_mae: 0.4732\n",
      "Epoch 22/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6187 - mae: 0.5905 - val_loss: 0.4863 - val_mae: 0.5033\n",
      "Epoch 23/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5504 - mae: 0.5599 - val_loss: 0.3740 - val_mae: 0.4554\n",
      "Epoch 24/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5845 - mae: 0.5881 - val_loss: 0.4041 - val_mae: 0.4651\n",
      "Epoch 25/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5740 - mae: 0.5621 - val_loss: 0.3703 - val_mae: 0.4418\n",
      "Epoch 26/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5689 - mae: 0.5757 - val_loss: 0.4352 - val_mae: 0.4790\n",
      "Epoch 27/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5749 - mae: 0.5795 - val_loss: 0.5122 - val_mae: 0.5287\n",
      "Epoch 28/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5706 - mae: 0.5659 - val_loss: 0.6081 - val_mae: 0.5464\n",
      "Epoch 29/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6286 - mae: 0.5909 - val_loss: 0.7064 - val_mae: 0.6168\n",
      "Epoch 30/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5201 - mae: 0.5437 - val_loss: 0.4242 - val_mae: 0.4727\n",
      "Epoch 31/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5311 - mae: 0.5467 - val_loss: 0.5928 - val_mae: 0.5369\n",
      "Epoch 32/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5623 - mae: 0.5511 - val_loss: 0.6607 - val_mae: 0.5580\n",
      "Epoch 33/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5479 - mae: 0.5435 - val_loss: 0.5281 - val_mae: 0.5323\n",
      "Epoch 34/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5396 - mae: 0.5540 - val_loss: 0.4376 - val_mae: 0.4746\n",
      "Epoch 35/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4553 - mae: 0.4948 - val_loss: 0.3566 - val_mae: 0.4297\n",
      "Epoch 36/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4585 - mae: 0.5044 - val_loss: 0.3554 - val_mae: 0.4370\n",
      "Epoch 37/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5198 - mae: 0.5274 - val_loss: 0.6068 - val_mae: 0.5699\n",
      "Epoch 38/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4998 - mae: 0.5192 - val_loss: 0.4067 - val_mae: 0.4651\n",
      "Epoch 39/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4681 - mae: 0.5072 - val_loss: 0.3379 - val_mae: 0.4287\n",
      "Epoch 40/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4851 - mae: 0.5165 - val_loss: 0.3209 - val_mae: 0.4296\n",
      "Epoch 41/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4839 - mae: 0.5058 - val_loss: 0.2897 - val_mae: 0.3942\n",
      "Epoch 42/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4436 - mae: 0.4794 - val_loss: 0.3533 - val_mae: 0.4461\n",
      "Epoch 43/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4729 - mae: 0.4921 - val_loss: 0.2840 - val_mae: 0.3623\n",
      "Epoch 44/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4472 - mae: 0.4814 - val_loss: 0.4042 - val_mae: 0.4382\n",
      "Epoch 45/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4275 - mae: 0.4676 - val_loss: 0.5679 - val_mae: 0.5311\n",
      "Epoch 46/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4443 - mae: 0.4915 - val_loss: 0.3116 - val_mae: 0.3992\n",
      "Epoch 47/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4266 - mae: 0.4697 - val_loss: 0.5714 - val_mae: 0.5565\n",
      "Epoch 48/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4462 - mae: 0.4991 - val_loss: 0.3218 - val_mae: 0.3984\n",
      "Epoch 49/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4546 - mae: 0.4875 - val_loss: 0.8555 - val_mae: 0.6686\n",
      "Epoch 50/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4794 - mae: 0.5020 - val_loss: 0.3384 - val_mae: 0.4270\n",
      "Epoch 51/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4602 - mae: 0.4762 - val_loss: 0.3744 - val_mae: 0.4383\n",
      "Epoch 52/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4209 - mae: 0.4639 - val_loss: 0.3103 - val_mae: 0.4166\n",
      "Epoch 53/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4613 - mae: 0.4910 - val_loss: 0.3795 - val_mae: 0.4146\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3749 - mae: 0.4051\n",
      "Test Loss: 0.3795, Test MAE: 0.4146\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.6698\n",
      "\n",
      "0: 384x640 2 persons, 49.8ms\n",
      "Speed: 1.1ms preprocess, 49.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 53.6ms\n",
      "Speed: 1.3ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 0.8ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.0ms\n",
      "Speed: 0.8ms preprocess, 34.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.9ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.8ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.6ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.6ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.8ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.8ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.8ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.6ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 0.8ms preprocess, 58.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 0.7ms preprocess, 34.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.6ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.7ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.9ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.6ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.6ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.6ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.7ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.6ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.9ms\n",
      "Speed: 0.6ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.0ms\n",
      "Speed: 0.8ms preprocess, 87.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 2.1ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.8ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.6ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.9ms\n",
      "Speed: 2.7ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.6ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.6ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.6ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.8ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.9ms\n",
      "Speed: 4.1ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.9ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.8ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 0.7ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 4.0ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.6ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.9ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.7ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.8ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.4ms\n",
      "Speed: 0.7ms preprocess, 49.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.7ms\n",
      "Speed: 1.2ms preprocess, 47.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 1.0ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.6ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.8ms\n",
      "Speed: 12.3ms preprocess, 95.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 0.9ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.6ms\n",
      "Speed: 1.0ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.3ms\n",
      "Speed: 0.9ms preprocess, 51.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.8ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.6ms\n",
      "Speed: 1.2ms preprocess, 95.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.8ms\n",
      "Speed: 3.1ms preprocess, 101.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 1.3ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.7ms\n",
      "Speed: 0.8ms preprocess, 55.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.2ms\n",
      "Speed: 1.2ms preprocess, 79.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.2ms\n",
      "Speed: 1.6ms preprocess, 57.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.9ms\n",
      "Speed: 2.3ms preprocess, 53.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.7ms\n",
      "Speed: 1.1ms preprocess, 62.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 0.9ms preprocess, 61.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 1.1ms preprocess, 83.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.4ms\n",
      "Speed: 0.9ms preprocess, 55.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.4ms\n",
      "Speed: 0.8ms preprocess, 58.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.5ms\n",
      "Speed: 0.8ms preprocess, 56.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.8ms\n",
      "Speed: 0.7ms preprocess, 57.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.0ms\n",
      "Speed: 1.0ms preprocess, 56.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.6ms\n",
      "Speed: 0.8ms preprocess, 61.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 0.7ms preprocess, 55.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 1.0ms preprocess, 54.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.2ms\n",
      "Speed: 1.0ms preprocess, 55.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 177.3ms\n",
      "Speed: 2.0ms preprocess, 177.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.8ms\n",
      "Speed: 1.0ms preprocess, 55.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.2ms\n",
      "Speed: 0.8ms preprocess, 59.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 0.8ms preprocess, 78.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 1.2ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 1.1ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.2ms\n",
      "Speed: 0.8ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.4ms\n",
      "Speed: 1.3ms preprocess, 96.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 0.9ms preprocess, 76.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 1.1ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.6ms\n",
      "Speed: 0.8ms preprocess, 55.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 1.2ms preprocess, 60.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.1ms\n",
      "Speed: 0.9ms preprocess, 59.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 1.1ms preprocess, 72.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 122.4ms\n",
      "Speed: 1.2ms preprocess, 122.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.9ms\n",
      "Speed: 1.0ms preprocess, 62.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.4ms\n",
      "Speed: 0.9ms preprocess, 57.4ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.1ms\n",
      "Speed: 0.9ms preprocess, 59.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 0.9ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.7ms\n",
      "Speed: 0.8ms preprocess, 60.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.5ms\n",
      "Speed: 1.2ms preprocess, 55.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 1.1ms preprocess, 60.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 0.8ms preprocess, 54.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.9ms\n",
      "Speed: 0.9ms preprocess, 64.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.7ms\n",
      "Speed: 0.8ms preprocess, 52.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.0ms\n",
      "Speed: 0.9ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.7ms\n",
      "Speed: 0.9ms preprocess, 65.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.1ms\n",
      "Speed: 0.8ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 0.8ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 0.9ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 3.4ms preprocess, 77.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.8ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.1ms\n",
      "Speed: 0.9ms preprocess, 56.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.1ms\n",
      "Speed: 1.3ms preprocess, 55.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.1ms\n",
      "Speed: 0.8ms preprocess, 48.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 0.8ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.5ms\n",
      "Speed: 0.9ms preprocess, 62.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.6ms\n",
      "Speed: 0.8ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.7ms\n",
      "Speed: 0.8ms preprocess, 51.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 0.8ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 0.8ms preprocess, 51.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 0.8ms preprocess, 50.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 0.7ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 0.7ms preprocess, 48.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.8ms\n",
      "Speed: 0.8ms preprocess, 48.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.4ms\n",
      "Speed: 0.8ms preprocess, 52.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.6ms\n",
      "Speed: 0.8ms preprocess, 49.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 0.8ms preprocess, 51.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.0ms\n",
      "Speed: 3.4ms preprocess, 60.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.7ms\n",
      "Speed: 0.8ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 0.8ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.9ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 0.8ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 0.7ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.4ms\n",
      "Speed: 0.7ms preprocess, 59.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.7ms preprocess, 52.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.4ms\n",
      "Speed: 0.8ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.7ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.5ms\n",
      "Speed: 3.4ms preprocess, 59.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.8ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 0.9ms preprocess, 58.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.5ms\n",
      "Speed: 0.8ms preprocess, 94.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.3ms\n",
      "Speed: 0.7ms preprocess, 67.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 2.3ms preprocess, 61.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.6ms\n",
      "Speed: 0.8ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 3.6ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.7ms\n",
      "Speed: 3.1ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 1.2ms preprocess, 69.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.8ms\n",
      "Speed: 0.8ms preprocess, 63.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.7ms\n",
      "Speed: 1.0ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 0.9ms preprocess, 67.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 0.9ms preprocess, 60.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 1.2ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 0.8ms preprocess, 51.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.3ms\n",
      "Speed: 0.9ms preprocess, 55.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 121.9ms\n",
      "Speed: 3.0ms preprocess, 121.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.5ms\n",
      "Speed: 1.1ms preprocess, 54.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.8ms\n",
      "Speed: 1.4ms preprocess, 56.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.1ms\n",
      "Speed: 0.8ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 0.9ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 0.8ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 2.2ms preprocess, 62.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 133.0ms\n",
      "Speed: 1.5ms preprocess, 133.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.4ms\n",
      "Speed: 0.9ms preprocess, 54.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 1.0ms preprocess, 72.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 0.8ms preprocess, 55.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.3ms\n",
      "Speed: 0.9ms preprocess, 67.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 1.4ms preprocess, 75.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 0.9ms preprocess, 63.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.5ms\n",
      "Speed: 1.9ms preprocess, 92.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.4ms\n",
      "Speed: 0.9ms preprocess, 56.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 1.6ms preprocess, 54.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 1.2ms preprocess, 60.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.0ms\n",
      "Speed: 0.8ms preprocess, 60.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.7ms\n",
      "Speed: 0.8ms preprocess, 60.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.9ms\n",
      "Speed: 0.9ms preprocess, 57.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 0.9ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.7ms\n",
      "Speed: 0.8ms preprocess, 64.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.5ms\n",
      "Speed: 0.9ms preprocess, 93.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.2ms\n",
      "Speed: 1.6ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.1ms\n",
      "Speed: 0.8ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 0.9ms preprocess, 67.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.5ms\n",
      "Speed: 1.8ms preprocess, 62.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 0.9ms preprocess, 63.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.7ms\n",
      "Speed: 0.8ms preprocess, 56.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 1.6ms preprocess, 57.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.5ms\n",
      "Speed: 0.9ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.8ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.4ms\n",
      "Speed: 0.9ms preprocess, 89.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 1.1ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.1ms\n",
      "Speed: 0.9ms preprocess, 57.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 0.7ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 1.0ms preprocess, 61.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 1.0ms preprocess, 74.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 0.8ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.6ms\n",
      "Speed: 1.1ms preprocess, 61.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.2ms\n",
      "Speed: 3.8ms preprocess, 65.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 0.9ms preprocess, 66.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.1ms\n",
      "Speed: 3.9ms preprocess, 67.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.0ms\n",
      "Speed: 0.8ms preprocess, 60.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.7ms\n",
      "Speed: 1.2ms preprocess, 77.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 0.8ms preprocess, 55.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 108.1ms\n",
      "Speed: 0.9ms preprocess, 108.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 1.0ms preprocess, 79.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.0ms\n",
      "Speed: 0.8ms preprocess, 60.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 4.0ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.4ms\n",
      "Speed: 0.8ms preprocess, 63.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.1ms\n",
      "Speed: 1.1ms preprocess, 53.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 1.3ms preprocess, 68.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.1ms\n",
      "Speed: 5.7ms preprocess, 89.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.3ms\n",
      "Speed: 0.9ms preprocess, 56.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.6ms\n",
      "Speed: 0.9ms preprocess, 59.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 4.9ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 1.1ms preprocess, 53.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 0.9ms preprocess, 58.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.0ms\n",
      "Speed: 0.8ms preprocess, 56.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 1.2ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 1.1ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 0.8ms preprocess, 54.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.1ms\n",
      "Speed: 0.7ms preprocess, 88.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 0.9ms preprocess, 53.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.7ms\n",
      "Speed: 1.2ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 0.8ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 0.8ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.7ms preprocess, 52.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 0.8ms preprocess, 50.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.5ms\n",
      "Speed: 0.8ms preprocess, 49.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 0.9ms preprocess, 49.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.9ms\n",
      "Speed: 0.8ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.3ms\n",
      "Speed: 0.9ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.6ms\n",
      "Speed: 3.5ms preprocess, 63.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.3ms\n",
      "Speed: 1.2ms preprocess, 51.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.4ms\n",
      "Speed: 0.8ms preprocess, 51.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 0.9ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.6ms\n",
      "Speed: 0.8ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.3ms\n",
      "Speed: 0.9ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 0.7ms preprocess, 49.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.8ms\n",
      "Speed: 0.8ms preprocess, 59.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.3ms\n",
      "Speed: 0.8ms preprocess, 51.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 7.1ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 0.8ms preprocess, 51.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 0.8ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.5ms\n",
      "Speed: 0.7ms preprocess, 49.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.5ms\n",
      "Speed: 0.7ms preprocess, 49.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 0.7ms preprocess, 62.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.8ms\n",
      "Speed: 0.9ms preprocess, 56.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.5ms\n",
      "Speed: 1.1ms preprocess, 52.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.5ms\n",
      "Speed: 1.0ms preprocess, 56.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.5ms\n",
      "Speed: 21.3ms preprocess, 57.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 0.8ms preprocess, 69.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.4ms\n",
      "Speed: 0.8ms preprocess, 66.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 1.2ms preprocess, 74.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.0ms\n",
      "Speed: 1.1ms preprocess, 74.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 0.9ms preprocess, 80.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.7ms\n",
      "Speed: 1.5ms preprocess, 87.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 142.1ms\n",
      "Speed: 0.9ms preprocess, 142.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.0ms\n",
      "Speed: 0.7ms preprocess, 71.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.5ms\n",
      "Speed: 1.0ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 0.7ms preprocess, 71.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 0.8ms preprocess, 60.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.3ms\n",
      "Speed: 0.9ms preprocess, 98.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.9ms\n",
      "Speed: 0.8ms preprocess, 67.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 0.9ms preprocess, 63.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.6ms preprocess, 69.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.5ms\n",
      "Speed: 1.3ms preprocess, 88.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 105.7ms\n",
      "Speed: 1.3ms preprocess, 105.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 1.2ms preprocess, 74.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 0.9ms preprocess, 62.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.7ms\n",
      "Speed: 0.9ms preprocess, 64.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.8ms\n",
      "Speed: 1.3ms preprocess, 59.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.3ms\n",
      "Speed: 1.0ms preprocess, 52.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.4ms\n",
      "Speed: 0.7ms preprocess, 59.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.9ms\n",
      "Speed: 0.8ms preprocess, 57.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 2.7ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 1.3ms preprocess, 67.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.2ms\n",
      "Speed: 0.9ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.7ms\n",
      "Speed: 0.8ms preprocess, 62.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 63.5ms\n",
      "Speed: 0.7ms preprocess, 63.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 55.9ms\n",
      "Speed: 1.0ms preprocess, 55.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 105.6ms\n",
      "Speed: 0.9ms preprocess, 105.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.6ms\n",
      "Speed: 1.0ms preprocess, 54.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 60.1ms\n",
      "Speed: 0.9ms preprocess, 60.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.6ms\n",
      "Speed: 1.0ms preprocess, 59.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.9ms\n",
      "Speed: 0.9ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.3ms\n",
      "Speed: 0.8ms preprocess, 53.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.1ms\n",
      "Speed: 0.8ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.8ms\n",
      "Speed: 0.8ms preprocess, 51.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.2ms\n",
      "Speed: 0.7ms preprocess, 50.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.9ms\n",
      "Speed: 3.6ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.9ms\n",
      "Speed: 0.8ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 0.8ms preprocess, 50.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 1.2ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 0.9ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.7ms\n",
      "Speed: 0.7ms preprocess, 49.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.7ms\n",
      "Speed: 0.7ms preprocess, 50.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 1.0ms preprocess, 47.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.7ms\n",
      "Speed: 1.2ms preprocess, 49.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.6ms\n",
      "Speed: 0.8ms preprocess, 49.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.6ms\n",
      "Speed: 0.8ms preprocess, 51.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 0.7ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 0.9ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 0.8ms preprocess, 49.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.3ms\n",
      "Speed: 0.8ms preprocess, 52.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.6ms\n",
      "Speed: 0.7ms preprocess, 48.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.5ms\n",
      "Speed: 0.7ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.6ms\n",
      "Speed: 0.9ms preprocess, 48.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.6ms\n",
      "Speed: 1.3ms preprocess, 48.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.8ms preprocess, 52.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.4ms\n",
      "Speed: 1.2ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.7ms\n",
      "Speed: 0.7ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.5ms\n",
      "Speed: 0.9ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 0.7ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.4ms\n",
      "Speed: 0.7ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.4ms\n",
      "Speed: 0.8ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.8ms\n",
      "Speed: 1.1ms preprocess, 56.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.8ms\n",
      "Speed: 0.8ms preprocess, 53.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.0ms\n",
      "Speed: 0.9ms preprocess, 52.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 0.7ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.3ms\n",
      "Speed: 0.8ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.5ms\n",
      "Speed: 0.7ms preprocess, 51.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.1ms\n",
      "Speed: 1.0ms preprocess, 57.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 96.1ms\n",
      "Speed: 0.9ms preprocess, 96.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.0ms\n",
      "Speed: 1.2ms preprocess, 58.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.2ms\n",
      "Speed: 0.8ms preprocess, 52.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.0ms\n",
      "Speed: 0.9ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 0.8ms preprocess, 62.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.0ms\n",
      "Speed: 0.9ms preprocess, 62.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.5ms\n",
      "Speed: 0.9ms preprocess, 53.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.7ms preprocess, 52.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 0.8ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 127.4ms\n",
      "Speed: 1.5ms preprocess, 127.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.3ms\n",
      "Speed: 1.4ms preprocess, 59.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.2ms\n",
      "Speed: 1.1ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 0.9ms preprocess, 74.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.6ms\n",
      "Speed: 11.1ms preprocess, 64.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.6ms\n",
      "Speed: 1.4ms preprocess, 54.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 57.2ms\n",
      "Speed: 1.4ms preprocess, 57.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.8ms\n",
      "Speed: 1.5ms preprocess, 69.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 5.1ms preprocess, 69.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.8ms\n",
      "Speed: 8.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 1.5ms preprocess, 61.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.3ms\n",
      "Speed: 0.9ms preprocess, 61.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.2ms\n",
      "Speed: 0.8ms preprocess, 63.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.9ms\n",
      "Speed: 0.8ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.6ms\n",
      "Speed: 1.0ms preprocess, 64.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.5ms\n",
      "Speed: 0.8ms preprocess, 70.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 176.0ms\n",
      "Speed: 0.8ms preprocess, 176.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 1.8ms preprocess, 68.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.0ms\n",
      "Speed: 1.0ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.2ms\n",
      "Speed: 1.1ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 136.9ms\n",
      "Speed: 19.5ms preprocess, 136.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.7ms\n",
      "Speed: 1.0ms preprocess, 65.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 185.9ms\n",
      "Speed: 2.0ms preprocess, 185.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.8ms\n",
      "Speed: 0.9ms preprocess, 174.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 174.8ms\n",
      "Speed: 0.8ms preprocess, 174.8ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.2ms\n",
      "Speed: 0.8ms preprocess, 91.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.6ms\n",
      "Speed: 0.8ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 0.8ms preprocess, 82.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.9ms\n",
      "Speed: 0.8ms preprocess, 58.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.1ms\n",
      "Speed: 0.9ms preprocess, 79.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.0ms\n",
      "Speed: 1.0ms preprocess, 76.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 0.9ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.6ms\n",
      "Speed: 1.3ms preprocess, 88.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.9ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 9.7ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.4ms\n",
      "Speed: 1.1ms preprocess, 72.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.8ms\n",
      "Speed: 1.0ms preprocess, 55.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.1ms preprocess, 77.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 1.1ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.0ms\n",
      "Speed: 0.9ms preprocess, 79.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 156.3ms\n",
      "Speed: 10.0ms preprocess, 156.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.2ms\n",
      "Speed: 2.9ms preprocess, 83.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.1ms\n",
      "Speed: 1.2ms preprocess, 64.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 131.8ms\n",
      "Speed: 1.2ms preprocess, 131.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 0.9ms preprocess, 60.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 1.1ms preprocess, 75.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.5ms\n",
      "Speed: 0.9ms preprocess, 92.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.4ms\n",
      "Speed: 32.0ms preprocess, 89.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.9ms\n",
      "Speed: 1.1ms preprocess, 65.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.6ms\n",
      "Speed: 1.4ms preprocess, 70.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.2ms\n",
      "Speed: 1.1ms preprocess, 63.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.3ms preprocess, 65.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 0.9ms preprocess, 68.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.9ms preprocess, 73.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 0.9ms preprocess, 75.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.2ms\n",
      "Speed: 0.9ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.7ms\n",
      "Speed: 14.1ms preprocess, 80.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.7ms\n",
      "Speed: 1.0ms preprocess, 65.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.8ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 0.9ms preprocess, 66.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.6ms\n",
      "Speed: 1.0ms preprocess, 65.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.8ms\n",
      "Speed: 0.9ms preprocess, 64.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 1.2ms preprocess, 75.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.1ms\n",
      "Speed: 0.9ms preprocess, 69.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.7ms\n",
      "Speed: 3.3ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 1.0ms preprocess, 67.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 0.9ms preprocess, 75.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 1.1ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.9ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 1.0ms preprocess, 74.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 1.3ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.4ms\n",
      "Speed: 1.0ms preprocess, 66.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.3ms\n",
      "Speed: 0.9ms preprocess, 65.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.7ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 0.9ms preprocess, 61.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.1ms\n",
      "Speed: 0.8ms preprocess, 63.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.4ms\n",
      "Speed: 1.1ms preprocess, 66.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.8ms\n",
      "Speed: 18.6ms preprocess, 103.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.2ms\n",
      "Speed: 0.9ms preprocess, 70.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 0.9ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 3.7ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 1.1ms preprocess, 73.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 1.4ms preprocess, 77.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.5ms\n",
      "Speed: 0.9ms preprocess, 69.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.4ms\n",
      "Speed: 1.0ms preprocess, 110.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.8ms\n",
      "Speed: 0.8ms preprocess, 66.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 129.2ms\n",
      "Speed: 16.9ms preprocess, 129.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.9ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.0ms\n",
      "Speed: 0.9ms preprocess, 91.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.9ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 162.2ms\n",
      "Speed: 12.8ms preprocess, 162.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.9ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.0ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.7ms\n",
      "Speed: 1.0ms preprocess, 80.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.5ms\n",
      "Speed: 1.2ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 1.4ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.3ms\n",
      "Speed: 1.0ms preprocess, 65.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.4ms\n",
      "Speed: 2.3ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 1.1ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 0.8ms preprocess, 82.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.9ms\n",
      "Speed: 1.0ms preprocess, 85.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 1.2ms preprocess, 71.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.2ms\n",
      "Speed: 1.0ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 0.8ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.8ms\n",
      "Speed: 1.3ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 0.9ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.4ms\n",
      "Speed: 18.3ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 1.8ms preprocess, 83.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 131.5ms\n",
      "Speed: 20.5ms preprocess, 131.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 186.8ms\n",
      "Speed: 0.9ms preprocess, 186.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 0.9ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.7ms\n",
      "Speed: 1.0ms preprocess, 68.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.9ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.7ms\n",
      "Speed: 0.9ms preprocess, 72.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.2ms\n",
      "Speed: 5.6ms preprocess, 86.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.1ms\n",
      "Speed: 2.0ms preprocess, 86.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 0.9ms preprocess, 71.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.1ms\n",
      "Speed: 0.8ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 108.6ms\n",
      "Speed: 1.0ms preprocess, 108.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.5ms\n",
      "Speed: 28.8ms preprocess, 87.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 0.9ms preprocess, 83.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 1.8ms preprocess, 61.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.1ms\n",
      "Speed: 1.0ms preprocess, 65.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.2ms\n",
      "Speed: 0.8ms preprocess, 64.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 183.6ms\n",
      "Speed: 1.1ms preprocess, 183.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.4ms\n",
      "Speed: 0.9ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.7ms\n",
      "Speed: 0.9ms preprocess, 91.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 7.5ms preprocess, 79.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.1ms\n",
      "Speed: 0.9ms preprocess, 79.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "Speed: 1.2ms preprocess, 76.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.9ms\n",
      "Speed: 1.0ms preprocess, 69.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.0ms\n",
      "Speed: 1.0ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 118.5ms\n",
      "Speed: 12.3ms preprocess, 118.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.2ms\n",
      "Speed: 0.8ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.2ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 0.8ms preprocess, 83.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.0ms\n",
      "Speed: 7.3ms preprocess, 78.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.9ms\n",
      "Speed: 0.9ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 1.1ms preprocess, 79.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 136.2ms\n",
      "Speed: 19.9ms preprocess, 136.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 0.9ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.1ms\n",
      "Speed: 0.8ms preprocess, 98.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 147.0ms\n",
      "Speed: 16.4ms preprocess, 147.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.0ms\n",
      "Speed: 1.0ms preprocess, 63.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.5ms\n",
      "Speed: 6.6ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.6ms\n",
      "Speed: 1.3ms preprocess, 72.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.5ms\n",
      "Speed: 0.8ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 155.8ms\n",
      "Speed: 15.1ms preprocess, 155.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.3ms\n",
      "Speed: 1.0ms preprocess, 75.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 189.2ms\n",
      "Speed: 1.0ms preprocess, 189.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.1ms\n",
      "Speed: 0.9ms preprocess, 77.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 17.8ms preprocess, 82.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 1.0ms preprocess, 71.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.7ms\n",
      "Speed: 0.9ms preprocess, 73.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.8ms\n",
      "Speed: 1.0ms preprocess, 71.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 0.8ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.8ms\n",
      "Speed: 1.1ms preprocess, 81.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.4ms\n",
      "Speed: 0.9ms preprocess, 70.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 111.0ms\n",
      "Speed: 1.8ms preprocess, 111.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 1.7ms preprocess, 72.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.6ms\n",
      "Speed: 1.0ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 191.8ms\n",
      "Speed: 1.0ms preprocess, 191.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.2ms\n",
      "Speed: 1.1ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.0ms\n",
      "Speed: 22.0ms preprocess, 101.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.7ms\n",
      "Speed: 0.8ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.2ms\n",
      "Speed: 2.4ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.4ms\n",
      "Speed: 1.0ms preprocess, 62.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 89.1ms\n",
      "Speed: 1.4ms preprocess, 89.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 139.1ms\n",
      "Speed: 12.3ms preprocess, 139.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 179.2ms\n",
      "Speed: 1.1ms preprocess, 179.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.5ms\n",
      "Speed: 0.9ms preprocess, 64.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.5ms\n",
      "Speed: 0.9ms preprocess, 49.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.5ms\n",
      "Speed: 12.6ms preprocess, 100.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 1.2ms preprocess, 83.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 125.0ms\n",
      "Speed: 23.9ms preprocess, 125.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.8ms\n",
      "Speed: 0.9ms preprocess, 62.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 0.8ms preprocess, 75.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.5ms\n",
      "Speed: 1.1ms preprocess, 184.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.0ms\n",
      "Speed: 23.3ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.9ms\n",
      "Speed: 1.0ms preprocess, 71.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.2ms\n",
      "Speed: 1.0ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 0.8ms preprocess, 61.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.8ms\n",
      "Speed: 1.2ms preprocess, 77.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.9ms\n",
      "Speed: 1.0ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.3ms\n",
      "Speed: 0.8ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.9ms\n",
      "Speed: 4.0ms preprocess, 86.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 0.9ms preprocess, 75.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.2ms\n",
      "Speed: 1.1ms preprocess, 85.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.8ms\n",
      "Speed: 1.0ms preprocess, 73.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Average Score for the Video: 4.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Speerwerpen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Speerwerpen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Speerwerpen/segment_000734.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 8.5637 - mae: 2.5160 - val_loss: 10.9512 - val_mae: 3.2415\n",
      "Epoch 2/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5748 - mae: 0.9449 - val_loss: 7.8523 - val_mae: 2.7253\n",
      "Epoch 3/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0421 - mae: 0.7942 - val_loss: 5.3425 - val_mae: 2.2262\n",
      "Epoch 4/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7823 - mae: 0.6992 - val_loss: 3.5010 - val_mae: 1.7719\n",
      "Epoch 5/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6266 - mae: 0.6299 - val_loss: 2.1267 - val_mae: 1.3173\n",
      "Epoch 6/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5583 - mae: 0.6030 - val_loss: 1.5467 - val_mae: 1.1055\n",
      "Epoch 7/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5515 - mae: 0.5894 - val_loss: 0.8707 - val_mae: 0.8122\n",
      "Epoch 8/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4670 - mae: 0.5447 - val_loss: 0.6864 - val_mae: 0.7028\n",
      "Epoch 9/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4600 - mae: 0.5385 - val_loss: 0.5142 - val_mae: 0.6053\n",
      "Epoch 10/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4782 - mae: 0.5440 - val_loss: 0.3681 - val_mae: 0.4790\n",
      "Epoch 11/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4536 - mae: 0.5378 - val_loss: 0.3205 - val_mae: 0.4513\n",
      "Epoch 12/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4184 - mae: 0.5078 - val_loss: 0.3091 - val_mae: 0.4319\n",
      "Epoch 13/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3946 - mae: 0.4972 - val_loss: 0.2566 - val_mae: 0.3937\n",
      "Epoch 14/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3444 - mae: 0.4635 - val_loss: 0.2774 - val_mae: 0.4045\n",
      "Epoch 15/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3640 - mae: 0.4705 - val_loss: 0.3584 - val_mae: 0.4705\n",
      "Epoch 16/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3754 - mae: 0.4836 - val_loss: 0.2796 - val_mae: 0.4017\n",
      "Epoch 17/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3286 - mae: 0.4507 - val_loss: 0.2551 - val_mae: 0.3769\n",
      "Epoch 18/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3249 - mae: 0.4459 - val_loss: 0.2816 - val_mae: 0.4070\n",
      "Epoch 19/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3074 - mae: 0.4345 - val_loss: 0.2358 - val_mae: 0.3668\n",
      "Epoch 20/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2920 - mae: 0.4227 - val_loss: 0.2570 - val_mae: 0.3841\n",
      "Epoch 21/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3265 - mae: 0.4441 - val_loss: 0.2564 - val_mae: 0.3728\n",
      "Epoch 22/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2796 - mae: 0.4100 - val_loss: 0.2495 - val_mae: 0.3686\n",
      "Epoch 23/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2837 - mae: 0.4187 - val_loss: 0.2329 - val_mae: 0.3322\n",
      "Epoch 24/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2919 - mae: 0.4202 - val_loss: 0.3087 - val_mae: 0.4176\n",
      "Epoch 25/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2963 - mae: 0.4176 - val_loss: 0.2389 - val_mae: 0.3752\n",
      "Epoch 26/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2676 - mae: 0.4009 - val_loss: 0.2442 - val_mae: 0.3603\n",
      "Epoch 27/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2588 - mae: 0.3916 - val_loss: 0.2364 - val_mae: 0.3300\n",
      "Epoch 28/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2729 - mae: 0.4030 - val_loss: 0.2172 - val_mae: 0.3290\n",
      "Epoch 29/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2484 - mae: 0.3805 - val_loss: 0.2465 - val_mae: 0.3597\n",
      "Epoch 30/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2334 - mae: 0.3728 - val_loss: 0.2247 - val_mae: 0.3421\n",
      "Epoch 31/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2309 - mae: 0.3701 - val_loss: 0.2083 - val_mae: 0.3333\n",
      "Epoch 32/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2370 - mae: 0.3670 - val_loss: 0.2176 - val_mae: 0.3161\n",
      "Epoch 33/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2331 - mae: 0.3621 - val_loss: 0.2297 - val_mae: 0.3377\n",
      "Epoch 34/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2291 - mae: 0.3607 - val_loss: 0.2162 - val_mae: 0.3208\n",
      "Epoch 35/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2083 - mae: 0.3457 - val_loss: 0.2132 - val_mae: 0.3265\n",
      "Epoch 36/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1981 - mae: 0.3359 - val_loss: 0.2180 - val_mae: 0.3285\n",
      "Epoch 37/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2241 - mae: 0.3547 - val_loss: 0.2383 - val_mae: 0.3817\n",
      "Epoch 38/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1952 - mae: 0.3351 - val_loss: 0.2190 - val_mae: 0.3576\n",
      "Epoch 39/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2154 - mae: 0.3459 - val_loss: 0.2189 - val_mae: 0.3077\n",
      "Epoch 40/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2005 - mae: 0.3343 - val_loss: 0.2109 - val_mae: 0.3290\n",
      "Epoch 41/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1828 - mae: 0.3188 - val_loss: 0.2018 - val_mae: 0.3153\n",
      "Epoch 42/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2120 - mae: 0.3388 - val_loss: 0.2768 - val_mae: 0.3684\n",
      "Epoch 43/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1940 - mae: 0.3353 - val_loss: 0.2242 - val_mae: 0.3250\n",
      "Epoch 44/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1898 - mae: 0.3325 - val_loss: 0.2287 - val_mae: 0.3292\n",
      "Epoch 45/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1883 - mae: 0.3212 - val_loss: 0.2068 - val_mae: 0.3089\n",
      "Epoch 46/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1838 - mae: 0.3157 - val_loss: 0.2320 - val_mae: 0.3452\n",
      "Epoch 47/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1766 - mae: 0.3102 - val_loss: 0.2006 - val_mae: 0.3216\n",
      "Epoch 48/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1737 - mae: 0.3045 - val_loss: 0.2134 - val_mae: 0.3093\n",
      "Epoch 49/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1751 - mae: 0.3039 - val_loss: 0.2285 - val_mae: 0.3558\n",
      "Epoch 50/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1767 - mae: 0.3169 - val_loss: 0.2431 - val_mae: 0.3712\n",
      "Epoch 51/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1742 - mae: 0.3093 - val_loss: 0.1926 - val_mae: 0.3000\n",
      "Epoch 52/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1732 - mae: 0.3054 - val_loss: 0.2045 - val_mae: 0.3029\n",
      "Epoch 53/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1739 - mae: 0.3024 - val_loss: 0.2364 - val_mae: 0.3223\n",
      "Epoch 54/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1746 - mae: 0.3121 - val_loss: 0.2381 - val_mae: 0.3290\n",
      "Epoch 55/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1631 - mae: 0.2916 - val_loss: 0.2373 - val_mae: 0.3421\n",
      "Epoch 56/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1618 - mae: 0.2954 - val_loss: 0.2208 - val_mae: 0.3394\n",
      "Epoch 57/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1625 - mae: 0.2921 - val_loss: 0.2011 - val_mae: 0.3093\n",
      "Epoch 58/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1546 - mae: 0.2841 - val_loss: 0.2103 - val_mae: 0.3129\n",
      "Epoch 59/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1714 - mae: 0.2981 - val_loss: 0.2024 - val_mae: 0.3125\n",
      "Epoch 60/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1728 - mae: 0.3062 - val_loss: 0.1935 - val_mae: 0.2965\n",
      "Epoch 61/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1761 - mae: 0.3033 - val_loss: 0.1908 - val_mae: 0.2737\n",
      "Epoch 62/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1692 - mae: 0.2942 - val_loss: 0.2016 - val_mae: 0.2984\n",
      "Epoch 63/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1690 - mae: 0.2900 - val_loss: 0.1901 - val_mae: 0.2869\n",
      "Epoch 64/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1360 - mae: 0.2638 - val_loss: 0.1845 - val_mae: 0.2823\n",
      "Epoch 65/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1450 - mae: 0.2688 - val_loss: 0.1826 - val_mae: 0.2716\n",
      "Epoch 66/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1526 - mae: 0.2725 - val_loss: 0.2171 - val_mae: 0.3009\n",
      "Epoch 67/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1362 - mae: 0.2637 - val_loss: 0.2152 - val_mae: 0.3218\n",
      "Epoch 68/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1517 - mae: 0.2718 - val_loss: 0.1734 - val_mae: 0.2675\n",
      "Epoch 69/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1527 - mae: 0.2714 - val_loss: 0.1901 - val_mae: 0.2947\n",
      "Epoch 70/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1433 - mae: 0.2697 - val_loss: 0.1947 - val_mae: 0.2753\n",
      "Epoch 71/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1504 - mae: 0.2756 - val_loss: 0.1891 - val_mae: 0.2872\n",
      "Epoch 72/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1579 - mae: 0.2757 - val_loss: 0.1947 - val_mae: 0.2889\n",
      "Epoch 73/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1408 - mae: 0.2614 - val_loss: 0.1785 - val_mae: 0.2714\n",
      "Epoch 74/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1377 - mae: 0.2555 - val_loss: 0.1954 - val_mae: 0.2817\n",
      "Epoch 75/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1529 - mae: 0.2655 - val_loss: 0.1897 - val_mae: 0.2941\n",
      "Epoch 76/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1434 - mae: 0.2584 - val_loss: 0.2096 - val_mae: 0.3112\n",
      "Epoch 77/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1385 - mae: 0.2539 - val_loss: 0.1788 - val_mae: 0.2779\n",
      "Epoch 78/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1381 - mae: 0.2530 - val_loss: 0.2090 - val_mae: 0.3239\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2130 - mae: 0.3245 \n",
      "Test Loss: 0.2090, Test MAE: 0.3239\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5520\n",
      "\n",
      "0: 640x640 1 person, 97.4ms\n",
      "Speed: 3.1ms preprocess, 97.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 2.4ms preprocess, 117.9ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 82.4ms\n",
      "Speed: 1.9ms preprocess, 82.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 130.6ms\n",
      "Speed: 3.6ms preprocess, 130.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 640x640 1 person, 253.9ms\n",
      "Speed: 3.5ms preprocess, 253.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.1ms\n",
      "Speed: 2.7ms preprocess, 124.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 272.4ms\n",
      "Speed: 2.3ms preprocess, 272.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.9ms\n",
      "Speed: 3.1ms preprocess, 147.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.0ms\n",
      "Speed: 3.5ms preprocess, 105.0ms inference, 12.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\n",
      "0: 640x640 1 person, 234.2ms\n",
      "Speed: 1.9ms preprocess, 234.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.2ms\n",
      "Speed: 2.4ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.3ms\n",
      "Speed: 2.3ms preprocess, 111.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.3ms\n",
      "Speed: 1.5ms preprocess, 97.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 88.1ms\n",
      "Speed: 2.2ms preprocess, 88.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.7ms\n",
      "Speed: 19.7ms preprocess, 102.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.7ms\n",
      "Speed: 28.1ms preprocess, 147.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 164.6ms\n",
      "Speed: 12.5ms preprocess, 164.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 171.2ms\n",
      "Speed: 16.9ms preprocess, 171.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 192.2ms\n",
      "Speed: 16.2ms preprocess, 192.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 156.5ms\n",
      "Speed: 21.4ms preprocess, 156.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 168.1ms\n",
      "Speed: 14.1ms preprocess, 168.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "0: 640x640 1 person, 184.7ms\n",
      "Speed: 10.6ms preprocess, 184.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 181.8ms\n",
      "Speed: 11.9ms preprocess, 181.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 173.4ms\n",
      "Speed: 6.3ms preprocess, 173.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.2ms\n",
      "Speed: 1.8ms preprocess, 111.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 215.4ms\n",
      "Speed: 4.0ms preprocess, 215.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.3ms\n",
      "Speed: 1.8ms preprocess, 122.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.6ms\n",
      "Speed: 2.3ms preprocess, 98.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 159.5ms\n",
      "Speed: 13.7ms preprocess, 159.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 193.9ms\n",
      "Speed: 12.3ms preprocess, 193.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.5ms\n",
      "Speed: 3.0ms preprocess, 149.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 1.7ms preprocess, 116.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.2ms\n",
      "Speed: 2.0ms preprocess, 118.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.1ms\n",
      "Speed: 2.4ms preprocess, 125.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 2.7ms preprocess, 113.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 140.7ms\n",
      "Speed: 2.0ms preprocess, 140.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 167.8ms\n",
      "Speed: 2.0ms preprocess, 167.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.9ms\n",
      "Speed: 6.3ms preprocess, 124.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 330.6ms\n",
      "Speed: 2.6ms preprocess, 330.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.1ms\n",
      "Speed: 17.0ms preprocess, 101.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.0ms\n",
      "Speed: 2.2ms preprocess, 94.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.4ms\n",
      "Speed: 4.5ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 228.2ms\n",
      "Speed: 1.8ms preprocess, 228.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.1ms\n",
      "Speed: 1.7ms preprocess, 116.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 185.6ms\n",
      "Speed: 5.8ms preprocess, 185.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.3ms\n",
      "Speed: 4.1ms preprocess, 105.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.5ms\n",
      "Speed: 15.9ms preprocess, 138.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.5ms\n",
      "Speed: 1.9ms preprocess, 151.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 190.3ms\n",
      "Speed: 7.7ms preprocess, 190.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.0ms\n",
      "Speed: 1.6ms preprocess, 106.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.8ms\n",
      "Speed: 13.1ms preprocess, 138.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 141.8ms\n",
      "Speed: 1.9ms preprocess, 141.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 168.7ms\n",
      "Speed: 2.9ms preprocess, 168.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.7ms\n",
      "Speed: 1.9ms preprocess, 120.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 140.1ms\n",
      "Speed: 2.2ms preprocess, 140.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 135.5ms\n",
      "Speed: 6.8ms preprocess, 135.5ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.4ms\n",
      "Speed: 1.8ms preprocess, 115.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.5ms\n",
      "Speed: 16.3ms preprocess, 149.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 156.7ms\n",
      "Speed: 22.1ms preprocess, 156.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 161.7ms\n",
      "Speed: 26.2ms preprocess, 161.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 150.8ms\n",
      "Speed: 1.9ms preprocess, 150.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.5ms\n",
      "Speed: 1.8ms preprocess, 117.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 271.9ms\n",
      "Speed: 2.5ms preprocess, 271.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.8ms\n",
      "Speed: 2.1ms preprocess, 99.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 143.5ms\n",
      "Speed: 1.8ms preprocess, 143.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 231.9ms\n",
      "Speed: 1.9ms preprocess, 231.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.2ms\n",
      "Speed: 1.7ms preprocess, 127.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 256.9ms\n",
      "Speed: 21.7ms preprocess, 256.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.8ms\n",
      "Speed: 1.7ms preprocess, 104.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 144.2ms\n",
      "Speed: 6.6ms preprocess, 144.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.0ms\n",
      "Speed: 1.6ms preprocess, 97.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 165.3ms\n",
      "Speed: 5.9ms preprocess, 165.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.0ms\n",
      "Speed: 1.6ms preprocess, 114.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.9ms\n",
      "Speed: 20.9ms preprocess, 136.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 226.4ms\n",
      "Speed: 2.0ms preprocess, 226.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.1ms\n",
      "Speed: 1.7ms preprocess, 101.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.9ms\n",
      "Speed: 1.6ms preprocess, 98.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.2ms\n",
      "Speed: 2.7ms preprocess, 113.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 640x640 1 person, 131.3ms\n",
      "Speed: 2.6ms preprocess, 131.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 173.5ms\n",
      "Speed: 1.6ms preprocess, 173.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 640x640 1 person, 91.7ms\n",
      "Speed: 7.5ms preprocess, 91.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.9ms\n",
      "Speed: 1.9ms preprocess, 96.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.6ms\n",
      "Speed: 5.5ms preprocess, 146.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 317.7ms\n",
      "Speed: 4.6ms preprocess, 317.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 640x640 1 person, 172.5ms\n",
      "Speed: 9.9ms preprocess, 172.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 152.9ms\n",
      "Speed: 1.8ms preprocess, 152.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 150.6ms\n",
      "Speed: 9.1ms preprocess, 150.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 2 persons, 126.2ms\n",
      "Speed: 2.0ms preprocess, 126.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 179.0ms\n",
      "Speed: 1.7ms preprocess, 179.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.0ms\n",
      "Speed: 2.5ms preprocess, 120.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 85.3ms\n",
      "Speed: 1.9ms preprocess, 85.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 229.9ms\n",
      "Speed: 1.9ms preprocess, 229.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.7ms\n",
      "Speed: 1.9ms preprocess, 103.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.5ms\n",
      "Speed: 2.1ms preprocess, 133.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 155.0ms\n",
      "Speed: 20.7ms preprocess, 155.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 142.7ms\n",
      "Speed: 2.9ms preprocess, 142.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 222.5ms\n",
      "Speed: 1.6ms preprocess, 222.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 140.0ms\n",
      "Speed: 2.3ms preprocess, 140.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.2ms\n",
      "Speed: 4.0ms preprocess, 114.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.7ms\n",
      "Speed: 2.0ms preprocess, 138.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.5ms\n",
      "Speed: 2.3ms preprocess, 134.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 137.1ms\n",
      "Speed: 1.6ms preprocess, 137.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.0ms\n",
      "Speed: 12.5ms preprocess, 134.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 216.4ms\n",
      "Speed: 9.8ms preprocess, 216.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x640 1 person, 122.0ms\n",
      "Speed: 18.6ms preprocess, 122.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 186.9ms\n",
      "Speed: 11.2ms preprocess, 186.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.6ms\n",
      "Speed: 2.7ms preprocess, 151.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 153.8ms\n",
      "Speed: 1.8ms preprocess, 153.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.4ms\n",
      "Speed: 21.4ms preprocess, 123.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 248.2ms\n",
      "Speed: 2.3ms preprocess, 248.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.4ms\n",
      "Speed: 1.7ms preprocess, 121.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 85.4ms\n",
      "Speed: 2.0ms preprocess, 85.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 202.5ms\n",
      "Speed: 1.7ms preprocess, 202.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 158.6ms\n",
      "Speed: 1.7ms preprocess, 158.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.9ms\n",
      "Speed: 1.9ms preprocess, 111.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.6ms\n",
      "Speed: 1.8ms preprocess, 132.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 213.9ms\n",
      "Speed: 2.3ms preprocess, 213.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.0ms\n",
      "Speed: 1.9ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.2ms\n",
      "Speed: 2.3ms preprocess, 132.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 211.2ms\n",
      "Speed: 1.7ms preprocess, 211.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.5ms\n",
      "Speed: 3.6ms preprocess, 101.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.8ms\n",
      "Speed: 10.9ms preprocess, 147.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 231.4ms\n",
      "Speed: 1.9ms preprocess, 231.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 214.2ms\n",
      "Speed: 1.6ms preprocess, 214.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.3ms\n",
      "Speed: 1.8ms preprocess, 118.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 258.7ms\n",
      "Speed: 3.1ms preprocess, 258.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 108.3ms\n",
      "Speed: 1.7ms preprocess, 108.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.1ms\n",
      "Speed: 2.0ms preprocess, 132.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.9ms\n",
      "Speed: 5.0ms preprocess, 147.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.1ms\n",
      "Speed: 3.1ms preprocess, 133.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.5ms\n",
      "Speed: 19.1ms preprocess, 126.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 153.4ms\n",
      "Speed: 16.2ms preprocess, 153.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 187.8ms\n",
      "Speed: 12.0ms preprocess, 187.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 172.2ms\n",
      "Speed: 10.6ms preprocess, 172.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.2ms\n",
      "Speed: 1.9ms preprocess, 107.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.5ms\n",
      "Speed: 13.8ms preprocess, 146.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 238.8ms\n",
      "Speed: 2.0ms preprocess, 238.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.9ms\n",
      "Speed: 2.0ms preprocess, 113.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.3ms\n",
      "Speed: 2.4ms preprocess, 115.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.2ms\n",
      "Speed: 20.3ms preprocess, 124.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 135.6ms\n",
      "Speed: 1.7ms preprocess, 135.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.6ms\n",
      "Speed: 2.0ms preprocess, 134.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 274.8ms\n",
      "Speed: 53.9ms preprocess, 274.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.4ms\n",
      "Speed: 1.7ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.9ms\n",
      "Speed: 2.6ms preprocess, 117.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 160.1ms\n",
      "Speed: 2.4ms preprocess, 160.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 2 persons, 102.9ms\n",
      "Speed: 1.7ms preprocess, 102.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 199.1ms\n",
      "Speed: 1.6ms preprocess, 199.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 201.7ms\n",
      "Speed: 1.6ms preprocess, 201.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.7ms\n",
      "Speed: 2.3ms preprocess, 98.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.1ms\n",
      "Speed: 1.7ms preprocess, 129.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.0ms\n",
      "Speed: 2.4ms preprocess, 109.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.3ms\n",
      "Speed: 2.0ms preprocess, 106.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 2 persons, 135.3ms\n",
      "Speed: 21.4ms preprocess, 135.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.3ms\n",
      "Speed: 2.0ms preprocess, 95.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 640x640 1 person, 142.0ms\n",
      "Speed: 16.6ms preprocess, 142.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 2 persons, 185.8ms\n",
      "Speed: 16.8ms preprocess, 185.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 117.2ms\n",
      "Speed: 40.3ms preprocess, 117.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 263.2ms\n",
      "Speed: 2.3ms preprocess, 263.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 140.3ms\n",
      "Speed: 1.9ms preprocess, 140.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 142.9ms\n",
      "Speed: 12.1ms preprocess, 142.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.4ms\n",
      "Speed: 1.6ms preprocess, 111.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "0: 640x640 1 person, 134.4ms\n",
      "Speed: 11.5ms preprocess, 134.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 171.1ms\n",
      "Speed: 13.6ms preprocess, 171.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.3ms\n",
      "Speed: 1.9ms preprocess, 106.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.1ms\n",
      "Speed: 11.7ms preprocess, 136.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 142.8ms\n",
      "Speed: 1.9ms preprocess, 142.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 91.2ms\n",
      "Speed: 1.7ms preprocess, 91.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 186.6ms\n",
      "Speed: 1.7ms preprocess, 186.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.3ms\n",
      "Speed: 1.7ms preprocess, 110.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.2ms\n",
      "Speed: 2.2ms preprocess, 96.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.2ms\n",
      "Speed: 10.3ms preprocess, 146.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.9ms\n",
      "Speed: 4.4ms preprocess, 138.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.3ms\n",
      "Speed: 1.7ms preprocess, 95.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 184.2ms\n",
      "Speed: 12.8ms preprocess, 184.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.4ms\n",
      "Speed: 1.9ms preprocess, 98.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.8ms\n",
      "Speed: 21.0ms preprocess, 133.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.9ms\n",
      "Speed: 2.7ms preprocess, 109.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.0ms\n",
      "Speed: 2.0ms preprocess, 106.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.3ms\n",
      "Speed: 2.3ms preprocess, 107.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 82.4ms\n",
      "Speed: 2.0ms preprocess, 82.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 156.4ms\n",
      "Speed: 3.3ms preprocess, 156.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.1ms\n",
      "Speed: 1.7ms preprocess, 103.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 640x640 1 person, 91.4ms\n",
      "Speed: 1.9ms preprocess, 91.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 148.4ms\n",
      "Speed: 21.0ms preprocess, 148.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 176.0ms\n",
      "Speed: 11.6ms preprocess, 176.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.8ms\n",
      "Speed: 1.9ms preprocess, 109.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "0: 640x640 1 person, 87.1ms\n",
      "Speed: 14.8ms preprocess, 87.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 166.7ms\n",
      "Speed: 13.7ms preprocess, 166.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 156.8ms\n",
      "Speed: 10.0ms preprocess, 156.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.4ms\n",
      "Speed: 1.8ms preprocess, 112.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 640x640 1 person, 135.6ms\n",
      "Speed: 20.0ms preprocess, 135.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 168.9ms\n",
      "Speed: 11.9ms preprocess, 168.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 164.2ms\n",
      "Speed: 25.3ms preprocess, 164.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 191.7ms\n",
      "Speed: 12.0ms preprocess, 191.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 183.7ms\n",
      "Speed: 15.8ms preprocess, 183.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 168.8ms\n",
      "Speed: 7.7ms preprocess, 168.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.4ms\n",
      "Speed: 1.8ms preprocess, 109.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 152.4ms\n",
      "Speed: 4.4ms preprocess, 152.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 175.9ms\n",
      "Speed: 14.7ms preprocess, 175.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 154.7ms\n",
      "Speed: 14.3ms preprocess, 154.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.5ms\n",
      "Speed: 4.1ms preprocess, 94.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.0ms\n",
      "Speed: 12.8ms preprocess, 126.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 163.7ms\n",
      "Speed: 6.7ms preprocess, 163.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 84.0ms\n",
      "Speed: 1.7ms preprocess, 84.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 242.1ms\n",
      "Speed: 14.0ms preprocess, 242.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.0ms\n",
      "Speed: 1.6ms preprocess, 109.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.5ms\n",
      "Speed: 9.7ms preprocess, 98.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 160.9ms\n",
      "Speed: 8.5ms preprocess, 160.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 152.8ms\n",
      "Speed: 1.7ms preprocess, 152.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.4ms\n",
      "Speed: 1.6ms preprocess, 83.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 137.6ms\n",
      "Speed: 1.8ms preprocess, 137.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 145.6ms\n",
      "Speed: 3.5ms preprocess, 145.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 216.7ms\n",
      "Speed: 1.7ms preprocess, 216.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 207.2ms\n",
      "Speed: 1.4ms preprocess, 207.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.1ms\n",
      "Speed: 1.7ms preprocess, 105.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.8ms\n",
      "Speed: 2.0ms preprocess, 94.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 143.0ms\n",
      "Speed: 15.7ms preprocess, 143.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 170.6ms\n",
      "Speed: 9.6ms preprocess, 170.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.5ms\n",
      "Speed: 1.9ms preprocess, 98.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 139.7ms\n",
      "Speed: 14.0ms preprocess, 139.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 166.1ms\n",
      "Speed: 9.2ms preprocess, 166.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 185.3ms\n",
      "Speed: 12.0ms preprocess, 185.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.4ms\n",
      "Speed: 2.1ms preprocess, 83.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.9ms\n",
      "Speed: 12.4ms preprocess, 119.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.0ms\n",
      "Speed: 2.4ms preprocess, 110.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 85.8ms\n",
      "Speed: 2.5ms preprocess, 85.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.4ms\n",
      "Speed: 1.8ms preprocess, 93.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 178.4ms\n",
      "Speed: 20.9ms preprocess, 178.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.3ms\n",
      "Speed: 1.9ms preprocess, 107.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 115.3ms\n",
      "Speed: 1.8ms preprocess, 115.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 230.9ms\n",
      "Speed: 2.0ms preprocess, 230.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 88.3ms\n",
      "Speed: 1.9ms preprocess, 88.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.6ms\n",
      "Speed: 2.0ms preprocess, 100.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 143.3ms\n",
      "Speed: 11.0ms preprocess, 143.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 150.9ms\n",
      "Speed: 16.7ms preprocess, 150.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 262.1ms\n",
      "Speed: 3.7ms preprocess, 262.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.5ms\n",
      "Speed: 1.7ms preprocess, 97.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.3ms\n",
      "Speed: 1.9ms preprocess, 125.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 140.7ms\n",
      "Speed: 5.6ms preprocess, 140.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.1ms\n",
      "Speed: 1.7ms preprocess, 100.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.0ms\n",
      "Speed: 1.7ms preprocess, 99.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.3ms\n",
      "Speed: 3.7ms preprocess, 133.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 125.0ms\n",
      "Speed: 1.7ms preprocess, 125.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 163.9ms\n",
      "Speed: 16.3ms preprocess, 163.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 184.7ms\n",
      "Speed: 9.6ms preprocess, 184.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 198.5ms\n",
      "Speed: 11.6ms preprocess, 198.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.9ms\n",
      "Speed: 1.9ms preprocess, 100.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 640x640 1 person, 223.3ms\n",
      "Speed: 2.0ms preprocess, 223.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 219.5ms\n",
      "Speed: 1.9ms preprocess, 219.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.8ms\n",
      "Speed: 1.7ms preprocess, 110.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 303.5ms\n",
      "Speed: 11.8ms preprocess, 303.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 84.5ms\n",
      "Speed: 1.7ms preprocess, 84.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.8ms\n",
      "Speed: 1.9ms preprocess, 105.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x640 1 person, 139.5ms\n",
      "Speed: 22.5ms preprocess, 139.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.5ms\n",
      "Speed: 22.3ms preprocess, 138.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 166.7ms\n",
      "Speed: 14.6ms preprocess, 166.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 174.6ms\n",
      "Speed: 12.7ms preprocess, 174.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.2ms\n",
      "Speed: 2.0ms preprocess, 93.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 640x640 1 person, 150.8ms\n",
      "Speed: 16.0ms preprocess, 150.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.5ms\n",
      "Speed: 14.2ms preprocess, 151.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.6ms\n",
      "Speed: 1.9ms preprocess, 96.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 153.0ms\n",
      "Speed: 2.3ms preprocess, 153.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 231.9ms\n",
      "Speed: 5.8ms preprocess, 231.9ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.7ms\n",
      "Speed: 1.7ms preprocess, 104.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.3ms\n",
      "Speed: 1.9ms preprocess, 99.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.0ms\n",
      "Speed: 1.8ms preprocess, 104.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.4ms\n",
      "Speed: 2.0ms preprocess, 105.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "0: 640x640 1 person, 132.9ms\n",
      "Speed: 33.7ms preprocess, 132.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 100.7ms\n",
      "Speed: 1.8ms preprocess, 100.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.9ms\n",
      "Speed: 1.8ms preprocess, 106.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.1ms\n",
      "Speed: 2.0ms preprocess, 105.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 21.4ms preprocess, 113.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 88.9ms\n",
      "Speed: 1.7ms preprocess, 88.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.5ms\n",
      "Speed: 2.2ms preprocess, 98.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.7ms\n",
      "Speed: 10.3ms preprocess, 133.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.5ms\n",
      "Speed: 2.0ms preprocess, 109.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 128.4ms\n",
      "Speed: 11.0ms preprocess, 128.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.3ms\n",
      "Speed: 12.5ms preprocess, 151.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.3ms\n",
      "Speed: 11.3ms preprocess, 151.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.7ms\n",
      "Speed: 2.6ms preprocess, 92.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.9ms\n",
      "Speed: 7.2ms preprocess, 146.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 152.4ms\n",
      "Speed: 39.8ms preprocess, 152.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 255.2ms\n",
      "Speed: 3.2ms preprocess, 255.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.8ms\n",
      "Speed: 1.7ms preprocess, 95.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.3ms\n",
      "Speed: 1.8ms preprocess, 105.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 147.4ms\n",
      "Speed: 17.6ms preprocess, 147.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.8ms\n",
      "Speed: 2.9ms preprocess, 107.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 127.2ms\n",
      "Speed: 3.7ms preprocess, 127.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 214.1ms\n",
      "Speed: 1.7ms preprocess, 214.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 211.7ms\n",
      "Speed: 1.5ms preprocess, 211.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.7ms\n",
      "Speed: 1.9ms preprocess, 92.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.6ms\n",
      "Speed: 1.8ms preprocess, 97.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 124.3ms\n",
      "Speed: 2.5ms preprocess, 124.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 82.2ms\n",
      "Speed: 1.7ms preprocess, 82.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 84.0ms\n",
      "Speed: 5.1ms preprocess, 84.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.5ms\n",
      "Speed: 1.8ms preprocess, 107.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 137.7ms\n",
      "Speed: 4.4ms preprocess, 137.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.7ms\n",
      "Speed: 1.8ms preprocess, 96.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 256.5ms\n",
      "Speed: 1.9ms preprocess, 256.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.9ms\n",
      "Speed: 1.7ms preprocess, 98.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 219.5ms\n",
      "Speed: 1.9ms preprocess, 219.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 86.8ms\n",
      "Speed: 1.8ms preprocess, 86.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.2ms\n",
      "Speed: 1.7ms preprocess, 102.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 145.1ms\n",
      "Speed: 2.5ms preprocess, 145.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.5ms\n",
      "Speed: 2.0ms preprocess, 116.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.5ms\n",
      "Speed: 1.9ms preprocess, 103.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.1ms\n",
      "Speed: 18.3ms preprocess, 113.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 228.0ms\n",
      "Speed: 2.4ms preprocess, 228.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.5ms\n",
      "Speed: 2.7ms preprocess, 101.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.4ms\n",
      "Speed: 10.6ms preprocess, 151.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.8ms\n",
      "Speed: 1.9ms preprocess, 83.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 91.4ms\n",
      "Speed: 2.0ms preprocess, 91.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.7ms\n",
      "Speed: 28.0ms preprocess, 118.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 145.9ms\n",
      "Speed: 19.3ms preprocess, 145.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 89.8ms\n",
      "Speed: 1.9ms preprocess, 89.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.7ms\n",
      "Speed: 2.0ms preprocess, 94.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 216.0ms\n",
      "Speed: 1.9ms preprocess, 216.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.4ms\n",
      "Speed: 1.8ms preprocess, 101.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 283.1ms\n",
      "Speed: 6.6ms preprocess, 283.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 225.1ms\n",
      "Speed: 1.8ms preprocess, 225.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 640x640 1 person, 161.6ms\n",
      "Speed: 4.9ms preprocess, 161.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.1ms\n",
      "Speed: 1.9ms preprocess, 94.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 180.7ms\n",
      "Speed: 12.5ms preprocess, 180.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.2ms\n",
      "Speed: 2.0ms preprocess, 110.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.7ms\n",
      "Speed: 1.6ms preprocess, 110.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.5ms\n",
      "Speed: 2.4ms preprocess, 111.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.9ms\n",
      "Speed: 1.7ms preprocess, 123.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 223.7ms\n",
      "Speed: 1.7ms preprocess, 223.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 218.0ms\n",
      "Speed: 1.8ms preprocess, 218.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 137.7ms\n",
      "Speed: 2.0ms preprocess, 137.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 133.8ms\n",
      "Speed: 2.5ms preprocess, 133.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.6ms\n",
      "Speed: 1.9ms preprocess, 123.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.8ms\n",
      "Speed: 2.1ms preprocess, 138.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.6ms\n",
      "Speed: 1.8ms preprocess, 146.6ms inference, 21.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.3ms\n",
      "Speed: 2.0ms preprocess, 103.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.3ms\n",
      "Speed: 1.8ms preprocess, 104.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.1ms\n",
      "Speed: 1.9ms preprocess, 97.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.5ms\n",
      "Speed: 1.8ms preprocess, 103.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 224.5ms\n",
      "Speed: 1.8ms preprocess, 224.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.4ms\n",
      "Speed: 1.8ms preprocess, 107.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.8ms\n",
      "Speed: 2.1ms preprocess, 94.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 90.0ms\n",
      "Speed: 1.9ms preprocess, 90.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 222.9ms\n",
      "Speed: 2.0ms preprocess, 222.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 242.7ms\n",
      "Speed: 2.0ms preprocess, 242.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 225.8ms\n",
      "Speed: 2.3ms preprocess, 225.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.7ms\n",
      "Speed: 2.2ms preprocess, 116.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.6ms\n",
      "Speed: 2.3ms preprocess, 113.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.1ms\n",
      "Speed: 3.0ms preprocess, 114.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 2.2ms preprocess, 114.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.1ms\n",
      "Speed: 1.8ms preprocess, 120.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 118.8ms\n",
      "Speed: 15.0ms preprocess, 118.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.9ms\n",
      "Speed: 1.7ms preprocess, 94.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 108.4ms\n",
      "Speed: 2.3ms preprocess, 108.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.5ms\n",
      "Speed: 1.9ms preprocess, 105.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.9ms\n",
      "Speed: 9.5ms preprocess, 98.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 155.1ms\n",
      "Speed: 4.4ms preprocess, 155.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 2.1ms preprocess, 113.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.8ms\n",
      "Speed: 2.1ms preprocess, 136.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.7ms\n",
      "Speed: 1.7ms preprocess, 111.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.8ms\n",
      "Speed: 24.1ms preprocess, 123.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 148.6ms\n",
      "Speed: 10.7ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.4ms\n",
      "Speed: 1.9ms preprocess, 110.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 123.0ms\n",
      "Speed: 1.8ms preprocess, 123.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 217.0ms\n",
      "Speed: 1.7ms preprocess, 217.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 218.9ms\n",
      "Speed: 6.0ms preprocess, 218.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.6ms\n",
      "Speed: 7.2ms preprocess, 104.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.0ms\n",
      "Speed: 1.9ms preprocess, 111.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.7ms\n",
      "Speed: 1.8ms preprocess, 113.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.9ms\n",
      "Speed: 2.2ms preprocess, 116.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.8ms\n",
      "Speed: 2.1ms preprocess, 102.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 116.8ms\n",
      "Speed: 2.1ms preprocess, 116.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.5ms\n",
      "Speed: 2.0ms preprocess, 96.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 154.6ms\n",
      "Speed: 21.2ms preprocess, 154.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.5ms\n",
      "Speed: 2.8ms preprocess, 111.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.2ms\n",
      "Speed: 1.8ms preprocess, 107.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.7ms\n",
      "Speed: 1.9ms preprocess, 107.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.5ms\n",
      "Speed: 12.6ms preprocess, 119.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.8ms\n",
      "Speed: 1.8ms preprocess, 110.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 136.5ms\n",
      "Speed: 2.1ms preprocess, 136.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 129.0ms\n",
      "Speed: 1.8ms preprocess, 129.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 135.7ms\n",
      "Speed: 2.1ms preprocess, 135.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 109.0ms\n",
      "Speed: 2.0ms preprocess, 109.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.9ms\n",
      "Speed: 1.8ms preprocess, 105.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 114.9ms\n",
      "Speed: 1.8ms preprocess, 114.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 105.1ms\n",
      "Speed: 9.2ms preprocess, 105.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.1ms\n",
      "Speed: 1.7ms preprocess, 106.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.9ms\n",
      "Speed: 1.9ms preprocess, 104.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 302.9ms\n",
      "Speed: 3.4ms preprocess, 302.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.9ms\n",
      "Speed: 1.7ms preprocess, 101.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.6ms\n",
      "Speed: 2.6ms preprocess, 101.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.6ms\n",
      "Speed: 17.1ms preprocess, 99.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 195.0ms\n",
      "Speed: 7.6ms preprocess, 195.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 154.5ms\n",
      "Speed: 13.5ms preprocess, 154.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 158.4ms\n",
      "Speed: 25.1ms preprocess, 158.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 175.4ms\n",
      "Speed: 4.7ms preprocess, 175.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.6ms\n",
      "Speed: 1.8ms preprocess, 95.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 170.8ms\n",
      "Speed: 4.5ms preprocess, 170.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.6ms\n",
      "Speed: 1.7ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.4ms\n",
      "Speed: 1.9ms preprocess, 98.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 126.1ms\n",
      "Speed: 1.9ms preprocess, 126.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 195.6ms\n",
      "Speed: 1.8ms preprocess, 195.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.7ms\n",
      "Speed: 1.8ms preprocess, 112.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 232.3ms\n",
      "Speed: 2.1ms preprocess, 232.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.9ms\n",
      "Speed: 1.9ms preprocess, 107.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 224.3ms\n",
      "Speed: 1.8ms preprocess, 224.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 103.0ms\n",
      "Speed: 2.0ms preprocess, 103.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "0: 640x640 1 person, 144.1ms\n",
      "Speed: 18.1ms preprocess, 144.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.5ms\n",
      "Speed: 2.4ms preprocess, 99.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.6ms\n",
      "Speed: 15.4ms preprocess, 104.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.1ms\n",
      "Speed: 7.8ms preprocess, 102.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 111.4ms\n",
      "Speed: 2.0ms preprocess, 111.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 107.7ms\n",
      "Speed: 2.2ms preprocess, 107.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.3ms\n",
      "Speed: 1.6ms preprocess, 93.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 120.3ms\n",
      "Speed: 2.0ms preprocess, 120.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.0ms\n",
      "Speed: 2.1ms preprocess, 102.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Average Score for the Video: 4.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Sprint_Start.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Sprint_Start.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/sprint_start/segment_000091.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 9.4250 - mae: 2.7061 - val_loss: 9.8258 - val_mae: 2.9392\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9793 - mae: 1.1100 - val_loss: 9.1721 - val_mae: 2.8701\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6300 - mae: 1.0295 - val_loss: 7.9005 - val_mae: 2.6514\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3214 - mae: 0.8919 - val_loss: 6.5940 - val_mae: 2.4093\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2281 - mae: 0.8585 - val_loss: 5.5273 - val_mae: 2.1977\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1849 - mae: 0.8482 - val_loss: 4.3365 - val_mae: 1.9084\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1186 - mae: 0.8201 - val_loss: 3.1918 - val_mae: 1.6019\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0556 - mae: 0.8366 - val_loss: 2.3523 - val_mae: 1.3443\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8084 - mae: 0.7010 - val_loss: 2.1998 - val_mae: 1.3006\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8052 - mae: 0.7063 - val_loss: 1.4879 - val_mae: 1.0427\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7967 - mae: 0.7011 - val_loss: 1.3924 - val_mae: 1.0055\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6670 - mae: 0.6465 - val_loss: 1.0141 - val_mae: 0.8393\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5889 - mae: 0.5939 - val_loss: 0.9004 - val_mae: 0.7718\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5513 - mae: 0.5755 - val_loss: 0.8815 - val_mae: 0.7407\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6046 - mae: 0.6030 - val_loss: 0.9277 - val_mae: 0.8184\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5804 - mae: 0.5866 - val_loss: 0.5222 - val_mae: 0.5924\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5030 - mae: 0.5534 - val_loss: 0.4518 - val_mae: 0.5345\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5117 - mae: 0.5619 - val_loss: 0.4547 - val_mae: 0.5151\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4659 - mae: 0.5452 - val_loss: 0.4281 - val_mae: 0.5006\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5296 - mae: 0.5809 - val_loss: 0.5172 - val_mae: 0.5479\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5212 - mae: 0.5693 - val_loss: 0.3949 - val_mae: 0.4778\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5597 - mae: 0.5683 - val_loss: 0.4848 - val_mae: 0.5030\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3855 - mae: 0.4653 - val_loss: 0.3475 - val_mae: 0.4288\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4573 - mae: 0.5168 - val_loss: 0.4067 - val_mae: 0.4497\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4014 - mae: 0.4909 - val_loss: 0.3568 - val_mae: 0.4259\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4137 - mae: 0.4946 - val_loss: 0.4740 - val_mae: 0.5066\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4068 - mae: 0.4932 - val_loss: 0.3343 - val_mae: 0.3929\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4083 - mae: 0.4897 - val_loss: 0.3470 - val_mae: 0.4141\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3592 - mae: 0.4568 - val_loss: 0.3391 - val_mae: 0.4198\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4123 - mae: 0.4832 - val_loss: 0.3230 - val_mae: 0.4125\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3861 - mae: 0.4788 - val_loss: 0.2998 - val_mae: 0.3959\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3588 - mae: 0.4633 - val_loss: 0.3109 - val_mae: 0.3708\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4368 - mae: 0.5006 - val_loss: 0.3040 - val_mae: 0.3979\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3716 - mae: 0.4704 - val_loss: 0.3175 - val_mae: 0.3722\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3620 - mae: 0.4489 - val_loss: 0.3877 - val_mae: 0.4314\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3673 - mae: 0.4636 - val_loss: 0.2968 - val_mae: 0.3790\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3160 - mae: 0.4284 - val_loss: 0.3793 - val_mae: 0.4200\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3740 - mae: 0.4544 - val_loss: 0.4002 - val_mae: 0.4265\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4285 - mae: 0.4923 - val_loss: 0.4100 - val_mae: 0.4704\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3454 - mae: 0.4442 - val_loss: 0.3667 - val_mae: 0.4599\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3859 - mae: 0.4680 - val_loss: 0.5653 - val_mae: 0.6166\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3734 - mae: 0.4546 - val_loss: 0.3497 - val_mae: 0.4190\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3112 - mae: 0.4284 - val_loss: 0.3518 - val_mae: 0.4121\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2965 - mae: 0.4144 - val_loss: 0.2964 - val_mae: 0.3553\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2816 - mae: 0.4110 - val_loss: 0.3144 - val_mae: 0.3553\n",
      "Epoch 46/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3524 - mae: 0.4501 - val_loss: 0.3223 - val_mae: 0.3734\n",
      "Epoch 47/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3944 - mae: 0.4489 - val_loss: 0.3267 - val_mae: 0.3737\n",
      "Epoch 48/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3598 - mae: 0.4445 - val_loss: 0.4418 - val_mae: 0.4746\n",
      "Epoch 49/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3388 - mae: 0.4401 - val_loss: 0.2766 - val_mae: 0.3087\n",
      "Epoch 50/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3272 - mae: 0.4185 - val_loss: 0.3021 - val_mae: 0.3396\n",
      "Epoch 51/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3204 - mae: 0.4100 - val_loss: 0.3375 - val_mae: 0.3962\n",
      "Epoch 52/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3148 - mae: 0.4177 - val_loss: 0.2760 - val_mae: 0.3505\n",
      "Epoch 53/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3374 - mae: 0.4288 - val_loss: 0.2806 - val_mae: 0.3230\n",
      "Epoch 54/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2713 - mae: 0.3822 - val_loss: 0.2752 - val_mae: 0.3355\n",
      "Epoch 55/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3260 - mae: 0.4392 - val_loss: 0.3307 - val_mae: 0.3851\n",
      "Epoch 56/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3007 - mae: 0.4107 - val_loss: 0.3297 - val_mae: 0.4022\n",
      "Epoch 57/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2990 - mae: 0.4049 - val_loss: 0.3118 - val_mae: 0.3825\n",
      "Epoch 58/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2616 - mae: 0.3746 - val_loss: 0.2765 - val_mae: 0.3303\n",
      "Epoch 59/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3045 - mae: 0.4050 - val_loss: 0.2580 - val_mae: 0.3400\n",
      "Epoch 60/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2774 - mae: 0.3783 - val_loss: 0.3214 - val_mae: 0.3660\n",
      "Epoch 61/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2708 - mae: 0.3934 - val_loss: 0.3354 - val_mae: 0.3726\n",
      "Epoch 62/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2646 - mae: 0.3766 - val_loss: 0.2660 - val_mae: 0.3111\n",
      "Epoch 63/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2850 - mae: 0.3991 - val_loss: 0.2667 - val_mae: 0.3286\n",
      "Epoch 64/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2875 - mae: 0.3851 - val_loss: 0.2550 - val_mae: 0.3032\n",
      "Epoch 65/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2884 - mae: 0.3794 - val_loss: 0.2730 - val_mae: 0.3412\n",
      "Epoch 66/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3115 - mae: 0.4114 - val_loss: 0.3190 - val_mae: 0.3890\n",
      "Epoch 67/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2804 - mae: 0.3921 - val_loss: 0.2771 - val_mae: 0.3244\n",
      "Epoch 68/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2596 - mae: 0.3780 - val_loss: 0.2657 - val_mae: 0.3253\n",
      "Epoch 69/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2469 - mae: 0.3615 - val_loss: 0.2904 - val_mae: 0.3238\n",
      "Epoch 70/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2567 - mae: 0.3757 - val_loss: 0.2585 - val_mae: 0.2925\n",
      "Epoch 71/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2565 - mae: 0.3630 - val_loss: 0.2820 - val_mae: 0.3519\n",
      "Epoch 72/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2977 - mae: 0.4017 - val_loss: 0.2387 - val_mae: 0.3027\n",
      "Epoch 73/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2127 - mae: 0.3376 - val_loss: 0.3109 - val_mae: 0.3461\n",
      "Epoch 74/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2319 - mae: 0.3565 - val_loss: 0.3406 - val_mae: 0.3698\n",
      "Epoch 75/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2623 - mae: 0.3671 - val_loss: 0.2650 - val_mae: 0.2906\n",
      "Epoch 76/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2375 - mae: 0.3521 - val_loss: 0.2788 - val_mae: 0.3272\n",
      "Epoch 77/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2940 - mae: 0.4011 - val_loss: 0.3378 - val_mae: 0.3860\n",
      "Epoch 78/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2422 - mae: 0.3528 - val_loss: 0.3017 - val_mae: 0.3276\n",
      "Epoch 79/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2725 - mae: 0.3686 - val_loss: 0.2875 - val_mae: 0.3365\n",
      "Epoch 80/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2511 - mae: 0.3578 - val_loss: 0.2609 - val_mae: 0.3323\n",
      "Epoch 81/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2759 - mae: 0.3889 - val_loss: 0.3074 - val_mae: 0.3815\n",
      "Epoch 82/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2415 - mae: 0.3615 - val_loss: 0.3679 - val_mae: 0.3800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3273 - mae: 0.3565\n",
      "Test Loss: 0.3679, Test MAE: 0.3800\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5562\n",
      "\n",
      "0: 384x640 1 person, 120.7ms\n",
      "Speed: 2.5ms preprocess, 120.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 1.7ms preprocess, 66.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Average Score for the Video: 5.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Sprint.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Sprint.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/sprint/segment_000671.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 7.6056 - mae: 2.4432 - val_loss: 10.7096 - val_mae: 3.1804\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5249 - mae: 0.9396 - val_loss: 8.8968 - val_mae: 2.8887\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3316 - mae: 0.8976 - val_loss: 7.8673 - val_mae: 2.7110\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9470 - mae: 0.7790 - val_loss: 6.2346 - val_mae: 2.3970\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9079 - mae: 0.7616 - val_loss: 4.5779 - val_mae: 2.0208\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8352 - mae: 0.7126 - val_loss: 3.4964 - val_mae: 1.7343\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6363 - mae: 0.6080 - val_loss: 2.1828 - val_mae: 1.3104\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7099 - mae: 0.6629 - val_loss: 1.7309 - val_mae: 1.1175\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6054 - mae: 0.6103 - val_loss: 1.1423 - val_mae: 0.8765\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6068 - mae: 0.6071 - val_loss: 0.8937 - val_mae: 0.7845\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5202 - mae: 0.5721 - val_loss: 0.9473 - val_mae: 0.7989\n",
      "Epoch 12/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5127 - mae: 0.5541 - val_loss: 1.0082 - val_mae: 0.8295\n",
      "Epoch 13/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4706 - mae: 0.5267 - val_loss: 0.7981 - val_mae: 0.7339\n",
      "Epoch 14/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4786 - mae: 0.5452 - val_loss: 0.7689 - val_mae: 0.7250\n",
      "Epoch 15/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5163 - mae: 0.5662 - val_loss: 0.5659 - val_mae: 0.6197\n",
      "Epoch 16/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4069 - mae: 0.4895 - val_loss: 0.4231 - val_mae: 0.5118\n",
      "Epoch 17/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4700 - mae: 0.5365 - val_loss: 0.4768 - val_mae: 0.5657\n",
      "Epoch 18/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4793 - mae: 0.5286 - val_loss: 0.5399 - val_mae: 0.5981\n",
      "Epoch 19/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4113 - mae: 0.4880 - val_loss: 0.4066 - val_mae: 0.4917\n",
      "Epoch 20/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4255 - mae: 0.4977 - val_loss: 0.4246 - val_mae: 0.5071\n",
      "Epoch 21/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4003 - mae: 0.4864 - val_loss: 0.3875 - val_mae: 0.4653\n",
      "Epoch 22/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4707 - mae: 0.5181 - val_loss: 0.3374 - val_mae: 0.4300\n",
      "Epoch 23/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4251 - mae: 0.5080 - val_loss: 0.4716 - val_mae: 0.5313\n",
      "Epoch 24/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3692 - mae: 0.4656 - val_loss: 0.3680 - val_mae: 0.4567\n",
      "Epoch 25/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3971 - mae: 0.4793 - val_loss: 0.3783 - val_mae: 0.4464\n",
      "Epoch 26/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4471 - mae: 0.5056 - val_loss: 0.3309 - val_mae: 0.4211\n",
      "Epoch 27/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3868 - mae: 0.4781 - val_loss: 0.4001 - val_mae: 0.4954\n",
      "Epoch 28/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3859 - mae: 0.4733 - val_loss: 0.3490 - val_mae: 0.4168\n",
      "Epoch 29/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4052 - mae: 0.4808 - val_loss: 0.3485 - val_mae: 0.4267\n",
      "Epoch 30/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4330 - mae: 0.5049 - val_loss: 0.3676 - val_mae: 0.4509\n",
      "Epoch 31/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3984 - mae: 0.4761 - val_loss: 0.3620 - val_mae: 0.4463\n",
      "Epoch 32/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3818 - mae: 0.4618 - val_loss: 0.4561 - val_mae: 0.5211\n",
      "Epoch 33/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4258 - mae: 0.5010 - val_loss: 0.3859 - val_mae: 0.4541\n",
      "Epoch 34/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3206 - mae: 0.4333 - val_loss: 0.3622 - val_mae: 0.4455\n",
      "Epoch 35/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3800 - mae: 0.4710 - val_loss: 0.4913 - val_mae: 0.5177\n",
      "Epoch 36/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3683 - mae: 0.4515 - val_loss: 0.3769 - val_mae: 0.4523\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4170 - mae: 0.4669\n",
      "Test Loss: 0.3769, Test MAE: 0.4523\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.3358\n",
      "\n",
      "0: 640x640 1 person, 222.2ms\n",
      "Speed: 2.2ms preprocess, 222.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 169.6ms\n",
      "Speed: 12.5ms preprocess, 169.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 190.8ms\n",
      "Speed: 10.7ms preprocess, 190.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 166.6ms\n",
      "Speed: 6.7ms preprocess, 166.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 112.8ms\n",
      "Speed: 2.8ms preprocess, 112.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 137.2ms\n",
      "Speed: 6.6ms preprocess, 137.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 106.0ms\n",
      "Speed: 1.7ms preprocess, 106.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 2 persons, 112.8ms\n",
      "Speed: 2.2ms preprocess, 112.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.7ms\n",
      "Speed: 2.5ms preprocess, 149.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 177.6ms\n",
      "Speed: 11.0ms preprocess, 177.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 161.0ms\n",
      "Speed: 24.7ms preprocess, 161.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 217.9ms\n",
      "Speed: 3.0ms preprocess, 217.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 226.8ms\n",
      "Speed: 2.1ms preprocess, 226.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 151.1ms\n",
      "Speed: 2.6ms preprocess, 151.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 108.0ms\n",
      "Speed: 1.8ms preprocess, 108.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.2ms\n",
      "Speed: 1.7ms preprocess, 102.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 121.7ms\n",
      "Speed: 1.5ms preprocess, 121.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 233.0ms\n",
      "Speed: 1.8ms preprocess, 233.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 (no detections), 127.8ms\n",
      "Speed: 2.0ms preprocess, 127.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 113.5ms\n",
      "Speed: 1.7ms preprocess, 113.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.1ms\n",
      "Speed: 1.7ms preprocess, 110.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 234.3ms\n",
      "Speed: 3.2ms preprocess, 234.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Average Score for the Video: 4.52\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Verspringen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Verspringen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Verspringen/segment_001406.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athletes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
