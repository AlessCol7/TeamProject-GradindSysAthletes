{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2075, 51), Shape of y: (2075,)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/52\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 736ms/step - loss: 9.2963 - mae: 2.7614"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node adam/Mul_39 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/base_events.py\", line 640, in run_forever\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/base_events.py\", line 1992, in _run_once\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/var/folders/yh/61qhjdf50_74bpxh_zh688bm0000gn/T/ipykernel_3384/182883040.py\", line 64, in <module>\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 80, in train_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 383, in apply_gradients\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 448, in apply\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 511, in _backend_apply_gradients\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/adam.py\", line 148, in update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/ops/numpy.py\", line 6016, in multiply\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 627, in sparse_wrapper\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\", line 523, in multiply\n\nIncompatible shapes: [64] vs. [0]\n\t [[{{node adam/Mul_39}}]] [Op:__inference_multi_step_on_iterator_2721]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/Testing/perexercise.ipynb#W0sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m loss, mae \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node adam/Mul_39 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/base_events.py\", line 640, in run_forever\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/base_events.py\", line 1992, in _run_once\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/var/folders/yh/61qhjdf50_74bpxh_zh688bm0000gn/T/ipykernel_3384/182883040.py\", line 64, in <module>\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 80, in train_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 383, in apply_gradients\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 448, in apply\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 511, in _backend_apply_gradients\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/optimizers/adam.py\", line 148, in update_step\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/ops/numpy.py\", line 6016, in multiply\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 627, in sparse_wrapper\n\n  File \"/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\", line 523, in multiply\n\nIncompatible shapes: [64] vs. [0]\n\t [[{{node adam/Mul_39}}]] [Op:__inference_multi_step_on_iterator_2721]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load JSON data\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discurweper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']\n",
    "        bbox = annotation['bbox']\n",
    "\n",
    "        if len(keypoints) % 3 != 0:\n",
    "            # Skip invalid keypoints\n",
    "            print(f\"Skipping invalid keypoints in segment: {segment}\")\n",
    "            continue\n",
    "\n",
    "        # Normalize keypoints using bbox dimensions\n",
    "        normalized_keypoints = []\n",
    "        for i in range(0, len(keypoints), 3):\n",
    "            x = keypoints[i] / bbox[2]  # Normalize x by width\n",
    "            y = keypoints[i + 1] / bbox[3]  # Normalize y by height\n",
    "            visibility = keypoints[i + 2]  # Visibility\n",
    "            normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "        if len(normalized_keypoints) == 51:  # Ensure all entries are of the same length\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Check consistency\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"athlete_score_predictor.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.0191 - mae: 1.4239 - val_loss: 5.5599 - val_mae: 2.1272\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5189 - mae: 1.0041 - val_loss: 3.8408 - val_mae: 1.7279\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3721 - mae: 0.9597 - val_loss: 2.8792 - val_mae: 1.4619\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2794 - mae: 0.9312 - val_loss: 1.8681 - val_mae: 1.1625\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2599 - mae: 0.8957 - val_loss: 1.5366 - val_mae: 1.0355\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1541 - mae: 0.8727 - val_loss: 1.1899 - val_mae: 0.9219\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2758 - mae: 0.9139 - val_loss: 1.2230 - val_mae: 0.9000\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1725 - mae: 0.8773 - val_loss: 1.0286 - val_mae: 0.8341\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1381 - mae: 0.8645 - val_loss: 1.0199 - val_mae: 0.8307\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2255 - mae: 0.9083 - val_loss: 0.9946 - val_mae: 0.8056\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1650 - mae: 0.8746 - val_loss: 1.0595 - val_mae: 0.8488\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1151 - mae: 0.8742 - val_loss: 1.0703 - val_mae: 0.8444\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1193 - mae: 0.8524 - val_loss: 0.9586 - val_mae: 0.7931\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1067 - mae: 0.8557 - val_loss: 0.9485 - val_mae: 0.7795\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0850 - mae: 0.8395 - val_loss: 0.8839 - val_mae: 0.7316\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1057 - mae: 0.8523 - val_loss: 0.9687 - val_mae: 0.7834\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0480 - mae: 0.8355 - val_loss: 0.9476 - val_mae: 0.7682\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0592 - mae: 0.8386 - val_loss: 0.9121 - val_mae: 0.7483\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9706 - mae: 0.8008 - val_loss: 0.9583 - val_mae: 0.7973\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0424 - mae: 0.8395 - val_loss: 0.8481 - val_mae: 0.7244\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0848 - mae: 0.8490 - val_loss: 0.9712 - val_mae: 0.8028\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0814 - mae: 0.8470 - val_loss: 1.0105 - val_mae: 0.7523\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9994 - mae: 0.8096 - val_loss: 0.9024 - val_mae: 0.7497\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1360 - mae: 0.8694 - val_loss: 0.9466 - val_mae: 0.7737\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9858 - mae: 0.8030 - val_loss: 0.9222 - val_mae: 0.7256\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9877 - mae: 0.8006 - val_loss: 1.0626 - val_mae: 0.8190\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0396 - mae: 0.8225 - val_loss: 1.0474 - val_mae: 0.8315\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0356 - mae: 0.8314 - val_loss: 0.9824 - val_mae: 0.7990\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9861 - mae: 0.7962 - val_loss: 1.1511 - val_mae: 0.9009\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0460 - mae: 0.8346 - val_loss: 0.8960 - val_mae: 0.7559\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0395 - mae: 0.8280 - val_loss: 1.0302 - val_mae: 0.8399\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0509 - mae: 0.8184 - val_loss: 0.9808 - val_mae: 0.8005\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9317 - mae: 0.7752 - val_loss: 0.9111 - val_mae: 0.7651\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0475 - mae: 0.8175 - val_loss: 0.9129 - val_mae: 0.7613\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0683 - mae: 0.8361 - val_loss: 0.8719 - val_mae: 0.7356\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9609 - mae: 0.8049 - val_loss: 0.9054 - val_mae: 0.7390\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9533 - mae: 0.7955 - val_loss: 0.8358 - val_mae: 0.7208\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9332 - mae: 0.7741 - val_loss: 0.8456 - val_mae: 0.7318\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9178 - mae: 0.7590 - val_loss: 0.7955 - val_mae: 0.7050\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9504 - mae: 0.7837 - val_loss: 0.8219 - val_mae: 0.6879\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9935 - mae: 0.7993 - val_loss: 0.7456 - val_mae: 0.6679\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9194 - mae: 0.7730 - val_loss: 0.8733 - val_mae: 0.7499\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8980 - mae: 0.7607 - val_loss: 0.7977 - val_mae: 0.6839\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9201 - mae: 0.7642 - val_loss: 0.7893 - val_mae: 0.7140\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9004 - mae: 0.7688 - val_loss: 0.9216 - val_mae: 0.7499\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8914 - mae: 0.7520 - val_loss: 0.9011 - val_mae: 0.7352\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9186 - mae: 0.7624 - val_loss: 0.8017 - val_mae: 0.6824\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8489 - mae: 0.7379 - val_loss: 0.7680 - val_mae: 0.6586\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8485 - mae: 0.7407 - val_loss: 0.7853 - val_mae: 0.6611\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9104 - mae: 0.7582 - val_loss: 0.7610 - val_mae: 0.6817\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 516.9ms\n",
      "Speed: 23.6ms preprocess, 516.9ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 344.4ms\n",
      "Speed: 4.8ms preprocess, 344.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.5ms\n",
      "Speed: 3.5ms preprocess, 417.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 430.4ms\n",
      "Speed: 2.9ms preprocess, 430.4ms inference, 13.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 371.2ms\n",
      "Speed: 32.4ms preprocess, 371.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 467.7ms\n",
      "Speed: 3.5ms preprocess, 467.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 470.8ms\n",
      "Speed: 3.9ms preprocess, 470.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 469.8ms\n",
      "Speed: 3.9ms preprocess, 469.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.5ms\n",
      "Speed: 9.1ms preprocess, 392.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.0ms\n",
      "Speed: 4.8ms preprocess, 374.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 465.1ms\n",
      "Speed: 4.5ms preprocess, 465.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.2ms\n",
      "Speed: 4.0ms preprocess, 386.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 349.6ms\n",
      "Speed: 3.4ms preprocess, 349.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.6ms\n",
      "Speed: 4.3ms preprocess, 406.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 407.1ms\n",
      "Speed: 4.1ms preprocess, 407.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 380.1ms\n",
      "Speed: 3.7ms preprocess, 380.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.2ms\n",
      "Speed: 4.2ms preprocess, 397.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 411.7ms\n",
      "Speed: 3.8ms preprocess, 411.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.4ms\n",
      "Speed: 4.1ms preprocess, 389.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.4ms\n",
      "Speed: 4.2ms preprocess, 392.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.2ms\n",
      "Speed: 3.8ms preprocess, 367.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 439.1ms\n",
      "Speed: 4.4ms preprocess, 439.1ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 351.4ms\n",
      "Speed: 2.3ms preprocess, 351.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.5ms\n",
      "Speed: 3.9ms preprocess, 367.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.8ms\n",
      "Speed: 3.9ms preprocess, 378.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 475.7ms\n",
      "Speed: 3.6ms preprocess, 475.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.2ms\n",
      "Speed: 6.9ms preprocess, 402.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 410.7ms\n",
      "Speed: 4.0ms preprocess, 410.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.7ms\n",
      "Speed: 3.9ms preprocess, 403.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 425.9ms\n",
      "Speed: 3.4ms preprocess, 425.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 358.0ms\n",
      "Speed: 4.6ms preprocess, 358.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.0ms\n",
      "Speed: 7.1ms preprocess, 400.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.7ms\n",
      "Speed: 3.6ms preprocess, 392.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.9ms\n",
      "Speed: 4.0ms preprocess, 388.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.0ms\n",
      "Speed: 4.0ms preprocess, 399.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.4ms\n",
      "Speed: 4.2ms preprocess, 394.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.2ms\n",
      "Speed: 3.6ms preprocess, 363.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 381.9ms\n",
      "Speed: 4.3ms preprocess, 381.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.9ms\n",
      "Speed: 3.6ms preprocess, 377.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 413.3ms\n",
      "Speed: 3.9ms preprocess, 413.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 412.7ms\n",
      "Speed: 1.9ms preprocess, 412.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.7ms\n",
      "Speed: 3.8ms preprocess, 379.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.5ms\n",
      "Speed: 3.6ms preprocess, 406.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 405.4ms\n",
      "Speed: 3.7ms preprocess, 405.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.1ms\n",
      "Speed: 3.6ms preprocess, 400.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.9ms\n",
      "Speed: 3.5ms preprocess, 382.9ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 428.8ms\n",
      "Speed: 3.3ms preprocess, 428.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 448.4ms\n",
      "Speed: 4.1ms preprocess, 448.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 385.4ms\n",
      "Speed: 4.1ms preprocess, 385.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.3ms\n",
      "Speed: 4.5ms preprocess, 378.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.8ms\n",
      "Speed: 3.4ms preprocess, 396.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 383.8ms\n",
      "Speed: 4.5ms preprocess, 383.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 460.0ms\n",
      "Speed: 3.8ms preprocess, 460.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.8ms\n",
      "Speed: 3.9ms preprocess, 397.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 357.7ms\n",
      "Speed: 3.6ms preprocess, 357.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 488.3ms\n",
      "Speed: 4.3ms preprocess, 488.3ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.9ms\n",
      "Speed: 5.1ms preprocess, 378.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 371.7ms\n",
      "Speed: 3.7ms preprocess, 371.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.9ms\n",
      "Speed: 4.0ms preprocess, 392.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 409.4ms\n",
      "Speed: 3.8ms preprocess, 409.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.0ms\n",
      "Speed: 3.6ms preprocess, 403.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.0ms\n",
      "Speed: 4.0ms preprocess, 363.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 362.0ms\n",
      "Speed: 3.3ms preprocess, 362.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.5ms\n",
      "Speed: 4.0ms preprocess, 375.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.1ms\n",
      "Speed: 3.6ms preprocess, 373.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.6ms\n",
      "Speed: 3.3ms preprocess, 382.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 366.4ms\n",
      "Speed: 3.3ms preprocess, 366.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 461.0ms\n",
      "Speed: 3.6ms preprocess, 461.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.2ms\n",
      "Speed: 1.6ms preprocess, 389.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.4ms\n",
      "Speed: 3.6ms preprocess, 374.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 350.0ms\n",
      "Speed: 3.4ms preprocess, 350.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 439.5ms\n",
      "Speed: 7.4ms preprocess, 439.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.0ms\n",
      "Speed: 9.1ms preprocess, 394.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 387.4ms\n",
      "Speed: 1.8ms preprocess, 387.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.5ms\n",
      "Speed: 3.5ms preprocess, 421.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 378.7ms\n",
      "Speed: 30.7ms preprocess, 378.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.0ms\n",
      "Speed: 3.3ms preprocess, 373.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.2ms\n",
      "Speed: 3.4ms preprocess, 403.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.7ms\n",
      "Speed: 3.7ms preprocess, 386.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 403.0ms\n",
      "Speed: 3.7ms preprocess, 403.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.6ms\n",
      "Speed: 4.0ms preprocess, 372.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.5ms\n",
      "Speed: 4.1ms preprocess, 386.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.0ms\n",
      "Speed: 3.1ms preprocess, 417.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.7ms\n",
      "Speed: 3.7ms preprocess, 372.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.4ms\n",
      "Speed: 4.1ms preprocess, 391.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.9ms\n",
      "Speed: 3.6ms preprocess, 377.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 377.2ms\n",
      "Speed: 4.1ms preprocess, 377.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 522.1ms\n",
      "Speed: 3.8ms preprocess, 522.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.0ms\n",
      "Speed: 8.3ms preprocess, 399.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.0ms\n",
      "Speed: 4.0ms preprocess, 391.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 418.9ms\n",
      "Speed: 4.0ms preprocess, 418.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 407.8ms\n",
      "Speed: 9.3ms preprocess, 407.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 368.8ms\n",
      "Speed: 3.8ms preprocess, 368.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 3.9ms preprocess, 394.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 463.3ms\n",
      "Speed: 3.4ms preprocess, 463.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 367.8ms\n",
      "Speed: 3.9ms preprocess, 367.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 417.7ms\n",
      "Speed: 2.8ms preprocess, 417.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 401.8ms\n",
      "Speed: 3.9ms preprocess, 401.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 440.3ms\n",
      "Speed: 3.7ms preprocess, 440.3ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 424.2ms\n",
      "Speed: 3.9ms preprocess, 424.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 369.3ms\n",
      "Speed: 3.7ms preprocess, 369.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 434.5ms\n",
      "Speed: 3.5ms preprocess, 434.5ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 431.7ms\n",
      "Speed: 3.4ms preprocess, 431.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 354.5ms\n",
      "Speed: 3.8ms preprocess, 354.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.9ms\n",
      "Speed: 3.7ms preprocess, 375.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.0ms\n",
      "Speed: 3.2ms preprocess, 396.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.9ms preprocess, 398.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 429.1ms\n",
      "Speed: 3.9ms preprocess, 429.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.7ms\n",
      "Speed: 8.5ms preprocess, 421.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.0ms\n",
      "Speed: 4.0ms preprocess, 375.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 405.7ms\n",
      "Speed: 3.6ms preprocess, 405.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.5ms\n",
      "Speed: 4.2ms preprocess, 391.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 408.2ms\n",
      "Speed: 3.4ms preprocess, 408.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 357.3ms\n",
      "Speed: 3.8ms preprocess, 357.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 390.2ms\n",
      "Speed: 4.2ms preprocess, 390.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.8ms\n",
      "Speed: 3.3ms preprocess, 406.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 413.4ms\n",
      "Speed: 3.9ms preprocess, 413.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 366.0ms\n",
      "Speed: 3.9ms preprocess, 366.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.3ms\n",
      "Speed: 4.2ms preprocess, 389.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 277.0ms\n",
      "Speed: 3.7ms preprocess, 277.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 393.6ms\n",
      "Speed: 3.9ms preprocess, 393.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 401.8ms\n",
      "Speed: 3.7ms preprocess, 401.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.7ms\n",
      "Speed: 4.2ms preprocess, 399.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.9ms\n",
      "Speed: 3.2ms preprocess, 370.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.3ms\n",
      "Speed: 3.6ms preprocess, 379.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.4ms\n",
      "Speed: 3.9ms preprocess, 370.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.3ms\n",
      "Speed: 4.0ms preprocess, 400.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.8ms\n",
      "Speed: 3.7ms preprocess, 389.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.8ms\n",
      "Speed: 4.0ms preprocess, 391.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.7ms\n",
      "Speed: 4.4ms preprocess, 399.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 363.8ms\n",
      "Speed: 3.9ms preprocess, 363.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 370.1ms\n",
      "Speed: 4.3ms preprocess, 370.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 416.8ms\n",
      "Speed: 3.5ms preprocess, 416.8ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 421.4ms\n",
      "Speed: 1.9ms preprocess, 421.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 384.3ms\n",
      "Speed: 4.2ms preprocess, 384.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 374.2ms\n",
      "Speed: 3.9ms preprocess, 374.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 375.7ms\n",
      "Speed: 4.9ms preprocess, 375.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.2ms\n",
      "Speed: 3.7ms preprocess, 397.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 360.1ms\n",
      "Speed: 3.6ms preprocess, 360.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 380.7ms\n",
      "Speed: 3.9ms preprocess, 380.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.3ms\n",
      "Speed: 4.4ms preprocess, 402.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 359.2ms\n",
      "Speed: 3.4ms preprocess, 359.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.9ms\n",
      "Speed: 3.5ms preprocess, 373.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 382.6ms\n",
      "Speed: 4.0ms preprocess, 382.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 383.7ms\n",
      "Speed: 3.3ms preprocess, 383.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 4.1ms preprocess, 394.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 395.8ms\n",
      "Speed: 5.0ms preprocess, 395.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 474.8ms\n",
      "Speed: 4.3ms preprocess, 474.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 430.6ms\n",
      "Speed: 4.8ms preprocess, 430.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.1ms\n",
      "Speed: 3.9ms preprocess, 388.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 399.1ms\n",
      "Speed: 3.4ms preprocess, 399.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.4ms\n",
      "Speed: 4.0ms preprocess, 400.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 356.5ms\n",
      "Speed: 3.5ms preprocess, 356.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.6ms\n",
      "Speed: 3.8ms preprocess, 372.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.7ms\n",
      "Speed: 3.9ms preprocess, 394.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 387.6ms\n",
      "Speed: 3.9ms preprocess, 387.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 423.2ms\n",
      "Speed: 3.8ms preprocess, 423.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 392.0ms\n",
      "Speed: 4.1ms preprocess, 392.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.7ms preprocess, 398.5ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 376.6ms\n",
      "Speed: 4.2ms preprocess, 376.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 360.5ms\n",
      "Speed: 3.2ms preprocess, 360.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 379.9ms\n",
      "Speed: 3.7ms preprocess, 379.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.2ms\n",
      "Speed: 4.3ms preprocess, 396.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 396.9ms\n",
      "Speed: 3.2ms preprocess, 396.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 389.2ms\n",
      "Speed: 4.0ms preprocess, 389.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 398.5ms\n",
      "Speed: 3.8ms preprocess, 398.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 472.5ms\n",
      "Speed: 3.6ms preprocess, 472.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 394.1ms\n",
      "Speed: 3.9ms preprocess, 394.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 391.0ms\n",
      "Speed: 3.6ms preprocess, 391.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 356.0ms\n",
      "Speed: 3.9ms preprocess, 356.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 388.6ms\n",
      "Speed: 3.2ms preprocess, 388.6ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.5ms\n",
      "Speed: 9.4ms preprocess, 372.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 402.0ms\n",
      "Speed: 4.0ms preprocess, 402.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 438.2ms\n",
      "Speed: 4.0ms preprocess, 438.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 386.1ms\n",
      "Speed: 3.5ms preprocess, 386.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 372.3ms\n",
      "Speed: 3.7ms preprocess, 372.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 373.4ms\n",
      "Speed: 3.5ms preprocess, 373.4ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 458.9ms\n",
      "Speed: 3.7ms preprocess, 458.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 400.7ms\n",
      "Speed: 3.3ms preprocess, 400.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 393.5ms\n",
      "Speed: 4.2ms preprocess, 393.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 458.3ms\n",
      "Speed: 6.4ms preprocess, 458.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 450.8ms\n",
      "Speed: 9.3ms preprocess, 450.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 345.9ms\n",
      "Speed: 4.3ms preprocess, 345.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 484.2ms\n",
      "Speed: 2.0ms preprocess, 484.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 539.5ms\n",
      "Speed: 1.9ms preprocess, 539.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 485.8ms\n",
      "Speed: 5.0ms preprocess, 485.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 397.5ms\n",
      "Speed: 4.6ms preprocess, 397.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 381.6ms\n",
      "Speed: 4.4ms preprocess, 381.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 406.2ms\n",
      "Speed: 3.5ms preprocess, 406.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 model for pose detection\n",
    "model = YOLO('yolov8s-pose.pt')  # Load the YOLOv8 pose model\n",
    "\n",
    "# Open video capture\n",
    "video_path = '/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_001557.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_idx = 0\n",
    "keypoints_data = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run pose detection on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Extract keypoints for each detected person\n",
    "    for result in results:  # loop over detections\n",
    "        keypoints = result.keypoints  # Extract keypoints (x, y, visibility)\n",
    "        bbox = result.boxes.xywh[0].cpu().numpy()  # Bounding box (x, y, w, h)\n",
    "\n",
    "        # Ensure keypoints are valid and normalize them\n",
    "        if len(keypoints) % 3 == 0:\n",
    "            normalized_keypoints = []\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize x by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize y by height\n",
    "                visibility = keypoints[i + 2]  # Visibility\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            keypoints_data.append(normalized_keypoints)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "# Close video capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at scoring_scaler7.pkl\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.1836 - mae: 1.6963 - val_loss: 4.5218 - val_mae: 1.9029\n",
      "Epoch 2/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7908 - mae: 1.0804 - val_loss: 2.9630 - val_mae: 1.5138\n",
      "Epoch 3/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4563 - mae: 0.9902 - val_loss: 2.1532 - val_mae: 1.2722\n",
      "Epoch 4/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3846 - mae: 0.9547 - val_loss: 1.5149 - val_mae: 1.0557\n",
      "Epoch 5/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4178 - mae: 0.9786 - val_loss: 1.3763 - val_mae: 1.0029\n",
      "Epoch 6/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3673 - mae: 0.9597 - val_loss: 1.4065 - val_mae: 0.9923\n",
      "Epoch 7/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2975 - mae: 0.9405 - val_loss: 1.1665 - val_mae: 0.9004\n",
      "Epoch 8/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2984 - mae: 0.9177 - val_loss: 1.2713 - val_mae: 0.9308\n",
      "Epoch 9/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2083 - mae: 0.8920 - val_loss: 1.2316 - val_mae: 0.9230\n",
      "Epoch 10/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2187 - mae: 0.8867 - val_loss: 1.0881 - val_mae: 0.8697\n",
      "Epoch 11/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1114 - mae: 0.8541 - val_loss: 1.2299 - val_mae: 0.8947\n",
      "Epoch 12/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1182 - mae: 0.8680 - val_loss: 1.1183 - val_mae: 0.8421\n",
      "Epoch 13/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1958 - mae: 0.8894 - val_loss: 1.2108 - val_mae: 0.9177\n",
      "Epoch 14/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1358 - mae: 0.8671 - val_loss: 1.1203 - val_mae: 0.8629\n",
      "Epoch 15/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1428 - mae: 0.8713 - val_loss: 1.2244 - val_mae: 0.8990\n",
      "Epoch 16/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0955 - mae: 0.8464 - val_loss: 1.1128 - val_mae: 0.8559\n",
      "Epoch 17/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1418 - mae: 0.8591 - val_loss: 1.1848 - val_mae: 0.9061\n",
      "Epoch 18/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1537 - mae: 0.8683 - val_loss: 1.2238 - val_mae: 0.8838\n",
      "Epoch 19/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0152 - mae: 0.8031 - val_loss: 1.0347 - val_mae: 0.8266\n",
      "Epoch 20/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0873 - mae: 0.8348 - val_loss: 1.1114 - val_mae: 0.8522\n",
      "Epoch 21/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0640 - mae: 0.8380 - val_loss: 1.0632 - val_mae: 0.8458\n",
      "Epoch 22/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0952 - mae: 0.8586 - val_loss: 1.0414 - val_mae: 0.8357\n",
      "Epoch 23/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0824 - mae: 0.8360 - val_loss: 1.0516 - val_mae: 0.8415\n",
      "Epoch 24/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0491 - mae: 0.8232 - val_loss: 0.9793 - val_mae: 0.7919\n",
      "Epoch 25/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0698 - mae: 0.8357 - val_loss: 1.0395 - val_mae: 0.8218\n",
      "Epoch 26/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0640 - mae: 0.8342 - val_loss: 1.0384 - val_mae: 0.8125\n",
      "Epoch 27/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9684 - mae: 0.8011 - val_loss: 1.0685 - val_mae: 0.8458\n",
      "Epoch 28/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0107 - mae: 0.8115 - val_loss: 1.0097 - val_mae: 0.8026\n",
      "Epoch 29/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0295 - mae: 0.8095 - val_loss: 1.0879 - val_mae: 0.8376\n",
      "Epoch 30/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9849 - mae: 0.8081 - val_loss: 1.0521 - val_mae: 0.8139\n",
      "Epoch 31/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0581 - mae: 0.8297 - val_loss: 0.9418 - val_mae: 0.7620\n",
      "Epoch 32/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8930 - mae: 0.7515 - val_loss: 1.0164 - val_mae: 0.7902\n",
      "Epoch 33/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9882 - mae: 0.7968 - val_loss: 1.1307 - val_mae: 0.8462\n",
      "Epoch 34/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9368 - mae: 0.7793 - val_loss: 0.9305 - val_mae: 0.7546\n",
      "Epoch 35/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9890 - mae: 0.8037 - val_loss: 0.8973 - val_mae: 0.7450\n",
      "Epoch 36/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0052 - mae: 0.7970 - val_loss: 0.9205 - val_mae: 0.7613\n",
      "Epoch 37/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0232 - mae: 0.8047 - val_loss: 1.0712 - val_mae: 0.7732\n",
      "Epoch 38/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8895 - mae: 0.7504 - val_loss: 1.0125 - val_mae: 0.8278\n",
      "Epoch 39/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9806 - mae: 0.7892 - val_loss: 1.0777 - val_mae: 0.8450\n",
      "Epoch 40/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9359 - mae: 0.7763 - val_loss: 0.9348 - val_mae: 0.7823\n",
      "Epoch 41/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9196 - mae: 0.7729 - val_loss: 0.9869 - val_mae: 0.8034\n",
      "Epoch 42/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9288 - mae: 0.7617 - val_loss: 0.9758 - val_mae: 0.7771\n",
      "Epoch 43/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9587 - mae: 0.7802 - val_loss: 0.9144 - val_mae: 0.7537\n",
      "Epoch 44/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9514 - mae: 0.7736 - val_loss: 0.9182 - val_mae: 0.7393\n",
      "Epoch 45/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9577 - mae: 0.7646 - val_loss: 0.8815 - val_mae: 0.7209\n",
      "Epoch 46/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9085 - mae: 0.7582 - val_loss: 0.9932 - val_mae: 0.7580\n",
      "Epoch 47/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9151 - mae: 0.7537 - val_loss: 0.9029 - val_mae: 0.7359\n",
      "Epoch 48/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9615 - mae: 0.7831 - val_loss: 0.9972 - val_mae: 0.7670\n",
      "Epoch 49/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9296 - mae: 0.7602 - val_loss: 0.9788 - val_mae: 0.7977\n",
      "Epoch 50/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8920 - mae: 0.7569 - val_loss: 1.0792 - val_mae: 0.8183\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1455 - mae: 0.8459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0792, Test MAE: 0.8183\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.52M/6.52M [00:01<00:00, 3.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 46.4ms\n",
      "Speed: 2.3ms preprocess, 46.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 62.5ms\n",
      "Speed: 0.7ms preprocess, 62.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 2 persons, 69.1ms\n",
      "Speed: 1.3ms preprocess, 69.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 09:40:37.243 python[3384:45511] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-21 09:40:37.243 python[3384:45511] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 34.4ms\n",
      "Speed: 0.7ms preprocess, 34.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 33.9ms\n",
      "Speed: 0.8ms preprocess, 33.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 33.4ms\n",
      "Speed: 0.7ms preprocess, 33.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 33.7ms\n",
      "Speed: 0.8ms preprocess, 33.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.7ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.6ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 123.4ms\n",
      "Speed: 2.5ms preprocess, 123.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 1.1ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 0.7ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.9ms\n",
      "Speed: 0.8ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 1.0ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 1.0ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.8ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.8ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.9ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.9ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.5ms\n",
      "Speed: 0.8ms preprocess, 65.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.9ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.9ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.8ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 0.7ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 0.8ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 0.7ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.9ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 51.1ms\n",
      "Speed: 0.7ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 1.0ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 1.0ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 56.0ms\n",
      "Speed: 1.8ms preprocess, 56.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 0.6ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.5ms\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.9ms\n",
      "Speed: 1.0ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.9ms\n",
      "Speed: 0.8ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.9ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 0.9ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 0.9ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.9ms preprocess, 40.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 1.0ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 46.5ms\n",
      "Speed: 1.0ms preprocess, 46.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.9ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.3ms\n",
      "Speed: 0.9ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.9ms\n",
      "Speed: 0.9ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.4ms\n",
      "Speed: 1.0ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.9ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.0ms\n",
      "Speed: 0.9ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.8ms\n",
      "Speed: 0.9ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.3ms\n",
      "Speed: 0.9ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.4ms\n",
      "Speed: 1.0ms preprocess, 64.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 46.4ms\n",
      "Speed: 0.9ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.8ms\n",
      "Speed: 1.0ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 0.8ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.8ms\n",
      "Speed: 1.0ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 0.9ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 0.9ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.5ms\n",
      "Speed: 0.8ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 49.9ms\n",
      "Speed: 0.7ms preprocess, 49.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 2 persons, 52.8ms\n",
      "Speed: 1.7ms preprocess, 52.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 0.9ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.4ms\n",
      "Speed: 0.9ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.9ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.4ms\n",
      "Speed: 0.9ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 1.9ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 34.3ms\n",
      "Speed: 0.8ms preprocess, 34.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.0ms\n",
      "Speed: 0.8ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 59.8ms\n",
      "Speed: 0.8ms preprocess, 59.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.0ms\n",
      "Speed: 0.9ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.8ms\n",
      "Speed: 0.9ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.9ms\n",
      "Speed: 0.9ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.4ms\n",
      "Speed: 0.8ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.4ms\n",
      "Speed: 0.9ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.3ms\n",
      "Speed: 2.1ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.7ms\n",
      "Speed: 0.9ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.9ms\n",
      "Speed: 0.9ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 46.0ms\n",
      "Speed: 0.8ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 0.8ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.9ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.9ms\n",
      "Speed: 1.7ms preprocess, 54.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.8ms\n",
      "Speed: 0.8ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.7ms\n",
      "Speed: 0.9ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 0.9ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.2ms\n",
      "Speed: 0.9ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.7ms\n",
      "Speed: 0.9ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.7ms\n",
      "Speed: 0.9ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.6ms\n",
      "Speed: 1.5ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 1.0ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 67.8ms\n",
      "Speed: 1.1ms preprocess, 67.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.0ms\n",
      "Speed: 0.8ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 47.3ms\n",
      "Speed: 0.7ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.0ms\n",
      "Speed: 0.7ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 83.6ms\n",
      "Speed: 0.7ms preprocess, 83.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.9ms\n",
      "Speed: 1.0ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 0.9ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 47.9ms\n",
      "Speed: 0.9ms preprocess, 47.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 108.5ms\n",
      "Speed: 0.8ms preprocess, 108.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 34.9ms\n",
      "Speed: 0.8ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.9ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 34.6ms\n",
      "Speed: 0.7ms preprocess, 34.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.9ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 54.1ms\n",
      "Speed: 2.4ms preprocess, 54.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.9ms\n",
      "Speed: 1.0ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.8ms\n",
      "Speed: 0.9ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.8ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.8ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 2.3ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 0.7ms preprocess, 34.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.9ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.2ms\n",
      "Speed: 0.7ms preprocess, 34.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 0.8ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.8ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 1.0ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 0.7ms preprocess, 34.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 32.9ms\n",
      "Speed: 0.8ms preprocess, 32.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.9ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.9ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 0.8ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.9ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.9ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.8ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.9ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 0.8ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 2.1ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.3ms\n",
      "Speed: 2.8ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 0.7ms preprocess, 34.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.2ms\n",
      "Speed: 0.7ms preprocess, 34.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.8ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.4ms\n",
      "Speed: 3.4ms preprocess, 51.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.3ms\n",
      "Speed: 0.8ms preprocess, 49.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 1.1ms preprocess, 34.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 0.8ms preprocess, 34.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.6ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.9ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.8ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.9ms\n",
      "Speed: 2.0ms preprocess, 51.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.9ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.6ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.8ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.6ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.7ms\n",
      "Speed: 2.4ms preprocess, 50.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 1.0ms preprocess, 36.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 1.5ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 1.6ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.7ms\n",
      "Speed: 1.1ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 1.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.8ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.9ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 1.4ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.9ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.6ms\n",
      "Speed: 0.7ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.5ms\n",
      "Speed: 1.3ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.9ms\n",
      "Speed: 0.8ms preprocess, 61.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.8ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 1.2ms preprocess, 47.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.9ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.8ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 1.8ms preprocess, 60.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.9ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.6ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.7ms\n",
      "Speed: 0.7ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.6ms\n",
      "Speed: 2.9ms preprocess, 63.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.8ms\n",
      "Speed: 0.8ms preprocess, 59.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.8ms\n",
      "Speed: 1.0ms preprocess, 57.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.2ms\n",
      "Speed: 1.0ms preprocess, 56.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.0ms\n",
      "Speed: 2.1ms preprocess, 55.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.3ms\n",
      "Speed: 1.1ms preprocess, 54.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.4ms\n",
      "Speed: 0.9ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.1ms\n",
      "Speed: 1.0ms preprocess, 57.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.3ms\n",
      "Speed: 1.7ms preprocess, 92.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.3ms\n",
      "Speed: 1.2ms preprocess, 55.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.1ms\n",
      "Speed: 0.9ms preprocess, 61.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 0.9ms preprocess, 53.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.4ms\n",
      "Speed: 1.1ms preprocess, 54.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.4ms\n",
      "Speed: 1.7ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.3ms\n",
      "Speed: 1.0ms preprocess, 52.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.6ms\n",
      "Speed: 1.0ms preprocess, 50.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.6ms\n",
      "Speed: 3.3ms preprocess, 63.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.8ms\n",
      "Speed: 1.0ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.5ms\n",
      "Speed: 1.0ms preprocess, 50.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 0.9ms preprocess, 50.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.0ms\n",
      "Speed: 0.9ms preprocess, 51.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.8ms\n",
      "Speed: 1.1ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.9ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.8ms preprocess, 38.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 1.0ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 5.2ms preprocess, 67.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 1.2ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 1.2ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.8ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 0.9ms preprocess, 47.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 1.1ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.3ms\n",
      "Speed: 1.6ms preprocess, 48.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.8ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.9ms preprocess, 36.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 1.1ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.6ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 0.8ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.7ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 1.0ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.8ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.0ms\n",
      "Speed: 1.8ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 0.7ms preprocess, 34.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.8ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.8ms\n",
      "Speed: 0.7ms preprocess, 34.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.7ms preprocess, 35.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.1ms\n",
      "Speed: 2.5ms preprocess, 51.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.9ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.9ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 0.7ms preprocess, 34.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.7ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.6ms\n",
      "Speed: 0.8ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.0ms\n",
      "Speed: 0.8ms preprocess, 34.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.8ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.8ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 65.9ms\n",
      "Speed: 0.9ms preprocess, 65.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.9ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.0ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.9ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.9ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.8ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 2.6ms preprocess, 46.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.9ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.7ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.9ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.7ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.3ms\n",
      "Speed: 0.9ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 0.7ms preprocess, 34.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.8ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.9ms\n",
      "Speed: 0.8ms preprocess, 34.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 3.0ms preprocess, 51.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.7ms preprocess, 35.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.9ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.2ms\n",
      "Speed: 0.7ms preprocess, 53.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.8ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.9ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 2.4ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 0.8ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.9ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 1.3ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.1ms\n",
      "Speed: 0.7ms preprocess, 58.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 1.0ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 1.4ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.2ms\n",
      "Speed: 0.9ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.4ms\n",
      "Speed: 0.7ms preprocess, 34.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.8ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.9ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.6ms\n",
      "Speed: 0.8ms preprocess, 64.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.9ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.9ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.9ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.9ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.7ms preprocess, 45.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 0.7ms preprocess, 47.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.3ms\n",
      "Speed: 2.3ms preprocess, 54.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.0ms\n",
      "Speed: 2.8ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.9ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.9ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.7ms\n",
      "Speed: 0.7ms preprocess, 61.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.8ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 0.7ms preprocess, 69.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.8ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 0.7ms preprocess, 43.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.9ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.9ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.2ms\n",
      "Speed: 3.0ms preprocess, 52.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Average Score for the Video: 1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discurweper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define the scoring model\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"athlete_score_predictor.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_004494.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 5.3875 - mae: 1.9511 - val_loss: 6.2155 - val_mae: 2.2369\n",
      "Epoch 2/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7110 - mae: 1.0674 - val_loss: 4.9363 - val_mae: 1.9622\n",
      "Epoch 3/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3115 - mae: 0.9403 - val_loss: 3.8261 - val_mae: 1.7333\n",
      "Epoch 4/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3781 - mae: 0.9514 - val_loss: 3.1125 - val_mae: 1.5539\n",
      "Epoch 5/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3576 - mae: 0.9582 - val_loss: 2.5285 - val_mae: 1.3826\n",
      "Epoch 6/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1884 - mae: 0.8927 - val_loss: 2.0929 - val_mae: 1.2511\n",
      "Epoch 7/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1636 - mae: 0.8785 - val_loss: 1.8437 - val_mae: 1.1932\n",
      "Epoch 8/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0607 - mae: 0.8395 - val_loss: 1.7581 - val_mae: 1.1599\n",
      "Epoch 9/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1923 - mae: 0.8933 - val_loss: 1.5594 - val_mae: 1.0880\n",
      "Epoch 10/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0860 - mae: 0.8447 - val_loss: 1.4640 - val_mae: 1.0089\n",
      "Epoch 11/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0789 - mae: 0.8438 - val_loss: 1.3096 - val_mae: 0.9732\n",
      "Epoch 12/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0423 - mae: 0.8150 - val_loss: 1.3170 - val_mae: 0.9017\n",
      "Epoch 13/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9723 - mae: 0.8028 - val_loss: 2.4584 - val_mae: 1.1544\n",
      "Epoch 14/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0211 - mae: 0.8221 - val_loss: 1.3701 - val_mae: 0.9927\n",
      "Epoch 15/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9962 - mae: 0.8004 - val_loss: 2.0697 - val_mae: 1.1068\n",
      "Epoch 16/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0097 - mae: 0.8288 - val_loss: 1.6359 - val_mae: 1.0118\n",
      "Epoch 17/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9828 - mae: 0.8012 - val_loss: 1.2600 - val_mae: 0.9041\n",
      "Epoch 18/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9434 - mae: 0.7774 - val_loss: 1.0880 - val_mae: 0.8326\n",
      "Epoch 19/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9888 - mae: 0.7986 - val_loss: 1.1124 - val_mae: 0.8428\n",
      "Epoch 20/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9505 - mae: 0.8015 - val_loss: 1.3763 - val_mae: 1.0034\n",
      "Epoch 21/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9493 - mae: 0.7795 - val_loss: 1.5193 - val_mae: 0.9463\n",
      "Epoch 22/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8894 - mae: 0.7375 - val_loss: 1.5099 - val_mae: 0.9308\n",
      "Epoch 23/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8427 - mae: 0.7433 - val_loss: 1.1210 - val_mae: 0.8749\n",
      "Epoch 24/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8785 - mae: 0.7651 - val_loss: 1.4093 - val_mae: 1.0081\n",
      "Epoch 25/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9643 - mae: 0.7870 - val_loss: 1.8581 - val_mae: 1.0966\n",
      "Epoch 26/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8842 - mae: 0.7500 - val_loss: 1.2211 - val_mae: 0.8503\n",
      "Epoch 27/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8646 - mae: 0.7404 - val_loss: 0.9488 - val_mae: 0.7582\n",
      "Epoch 28/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8494 - mae: 0.7352 - val_loss: 0.9955 - val_mae: 0.7930\n",
      "Epoch 29/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8640 - mae: 0.7500 - val_loss: 0.9171 - val_mae: 0.7484\n",
      "Epoch 30/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.7773 - mae: 0.7138 - val_loss: 1.1386 - val_mae: 0.8210\n",
      "Epoch 31/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8053 - mae: 0.7174 - val_loss: 1.7491 - val_mae: 1.0368\n",
      "Epoch 32/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7949 - mae: 0.7050 - val_loss: 1.1473 - val_mae: 0.8096\n",
      "Epoch 33/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6940 - mae: 0.6478 - val_loss: 0.8933 - val_mae: 0.7103\n",
      "Epoch 34/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7330 - mae: 0.6756 - val_loss: 0.8846 - val_mae: 0.7211\n",
      "Epoch 35/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7378 - mae: 0.6799 - val_loss: 0.8848 - val_mae: 0.7209\n",
      "Epoch 36/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6888 - mae: 0.6525 - val_loss: 0.8698 - val_mae: 0.7201\n",
      "Epoch 37/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7056 - mae: 0.6492 - val_loss: 0.9720 - val_mae: 0.7800\n",
      "Epoch 38/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7219 - mae: 0.6770 - val_loss: 0.8498 - val_mae: 0.6805\n",
      "Epoch 39/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7453 - mae: 0.6779 - val_loss: 2.3303 - val_mae: 1.2189\n",
      "Epoch 40/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7397 - mae: 0.6600 - val_loss: 0.9820 - val_mae: 0.7864\n",
      "Epoch 41/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7260 - mae: 0.6658 - val_loss: 0.8202 - val_mae: 0.6787\n",
      "Epoch 42/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6714 - mae: 0.6366 - val_loss: 0.7752 - val_mae: 0.6407\n",
      "Epoch 43/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6703 - mae: 0.6313 - val_loss: 0.6777 - val_mae: 0.6336\n",
      "Epoch 44/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6066 - mae: 0.6080 - val_loss: 0.8805 - val_mae: 0.7370\n",
      "Epoch 45/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6493 - mae: 0.6155 - val_loss: 0.8983 - val_mae: 0.7375\n",
      "Epoch 46/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6642 - mae: 0.6294 - val_loss: 1.0092 - val_mae: 0.8173\n",
      "Epoch 47/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6938 - mae: 0.6409 - val_loss: 1.3791 - val_mae: 0.9206\n",
      "Epoch 48/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6156 - mae: 0.6026 - val_loss: 0.7981 - val_mae: 0.6569\n",
      "Epoch 49/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6266 - mae: 0.6074 - val_loss: 0.7439 - val_mae: 0.6650\n",
      "Epoch 50/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5944 - mae: 0.5861 - val_loss: 0.7675 - val_mae: 0.6384\n",
      "Epoch 51/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6076 - mae: 0.5967 - val_loss: 1.3216 - val_mae: 0.8711\n",
      "Epoch 52/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6019 - mae: 0.5831 - val_loss: 0.7586 - val_mae: 0.7034\n",
      "Epoch 53/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5790 - mae: 0.5782 - val_loss: 1.1267 - val_mae: 0.7869\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2200 - mae: 0.8284 \n",
      "Test Loss: 1.1267, Test MAE: 0.7869\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.2774\n",
      "\n",
      "0: 384x640 1 person, 177.0ms\n",
      "Speed: 6.2ms preprocess, 177.0ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 141.5ms\n",
      "Speed: 15.9ms preprocess, 141.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.2ms\n",
      "Speed: 1.4ms preprocess, 74.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.4ms\n",
      "Speed: 1.5ms preprocess, 82.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.9ms\n",
      "Speed: 1.8ms preprocess, 84.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.6ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.0ms\n",
      "Speed: 5.9ms preprocess, 87.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.7ms\n",
      "Speed: 1.3ms preprocess, 92.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.9ms\n",
      "Speed: 1.4ms preprocess, 82.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\n",
      "0: 384x640 1 person, 100.1ms\n",
      "Speed: 2.0ms preprocess, 100.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "0: 384x640 1 person, 188.3ms\n",
      "Speed: 12.7ms preprocess, 188.3ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 194.7ms\n",
      "Speed: 1.3ms preprocess, 194.7ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.2ms\n",
      "Speed: 20.8ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.4ms\n",
      "Speed: 1.6ms preprocess, 78.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.8ms\n",
      "Speed: 19.6ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 115.9ms\n",
      "Speed: 1.6ms preprocess, 115.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.0ms\n",
      "Speed: 1.8ms preprocess, 110.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.1ms\n",
      "Speed: 1.5ms preprocess, 83.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 1.5ms preprocess, 82.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.2ms\n",
      "Speed: 1.4ms preprocess, 80.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.7ms\n",
      "Speed: 1.4ms preprocess, 83.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.8ms\n",
      "Speed: 1.4ms preprocess, 85.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.5ms\n",
      "Speed: 1.7ms preprocess, 85.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 155.5ms\n",
      "Speed: 8.6ms preprocess, 155.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 95.5ms\n",
      "Speed: 1.7ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 30.6ms preprocess, 139.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.4ms\n",
      "Speed: 3.1ms preprocess, 94.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.6ms\n",
      "Speed: 2.0ms preprocess, 84.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.5ms\n",
      "Speed: 13.1ms preprocess, 80.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 134.4ms\n",
      "Speed: 26.8ms preprocess, 134.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 151.7ms\n",
      "Speed: 18.9ms preprocess, 151.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 188.1ms\n",
      "Speed: 1.4ms preprocess, 188.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 1.4ms preprocess, 74.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.8ms\n",
      "Speed: 1.8ms preprocess, 80.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.0ms\n",
      "Speed: 1.5ms preprocess, 85.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 164.2ms\n",
      "Speed: 12.9ms preprocess, 164.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 2.2ms preprocess, 71.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 132.9ms\n",
      "Speed: 3.4ms preprocess, 132.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.8ms\n",
      "Speed: 1.4ms preprocess, 86.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.5ms\n",
      "Speed: 1.4ms preprocess, 71.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 107.4ms\n",
      "Speed: 2.1ms preprocess, 107.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.9ms\n",
      "Speed: 1.5ms preprocess, 110.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.4ms\n",
      "Speed: 1.5ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.3ms\n",
      "Speed: 1.5ms preprocess, 83.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 101.0ms\n",
      "Speed: 1.6ms preprocess, 101.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.1ms\n",
      "Speed: 1.8ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.6ms\n",
      "Speed: 2.4ms preprocess, 87.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.4ms\n",
      "Speed: 1.7ms preprocess, 81.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 2.7ms preprocess, 77.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 142.5ms\n",
      "Speed: 20.2ms preprocess, 142.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 167.4ms\n",
      "Speed: 20.4ms preprocess, 167.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 92.0ms\n",
      "Speed: 1.5ms preprocess, 92.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.9ms\n",
      "Speed: 1.5ms preprocess, 74.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 1.5ms preprocess, 75.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 159.7ms\n",
      "Speed: 10.0ms preprocess, 159.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.9ms\n",
      "Speed: 1.8ms preprocess, 87.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.8ms\n",
      "Speed: 2.2ms preprocess, 86.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.4ms\n",
      "Speed: 7.0ms preprocess, 86.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 99.4ms\n",
      "Speed: 1.7ms preprocess, 99.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 1.6ms preprocess, 77.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.4ms\n",
      "Speed: 1.9ms preprocess, 83.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 91.2ms\n",
      "Speed: 1.4ms preprocess, 91.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 91.5ms\n",
      "Speed: 1.8ms preprocess, 91.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 163.4ms\n",
      "Speed: 13.8ms preprocess, 163.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 109.3ms\n",
      "Speed: 24.0ms preprocess, 109.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 267.7ms\n",
      "Speed: 2.4ms preprocess, 267.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.9ms\n",
      "Speed: 1.4ms preprocess, 73.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 384x640 1 person, 85.4ms\n",
      "Speed: 1.5ms preprocess, 85.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.5ms\n",
      "Speed: 1.5ms preprocess, 78.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 129.2ms\n",
      "Speed: 22.2ms preprocess, 129.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 148.3ms\n",
      "Speed: 22.0ms preprocess, 148.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 167.7ms\n",
      "Speed: 19.9ms preprocess, 167.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 131.7ms\n",
      "Speed: 18.0ms preprocess, 131.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 136.8ms\n",
      "Speed: 32.6ms preprocess, 136.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 156.1ms\n",
      "Speed: 35.4ms preprocess, 156.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 149.6ms\n",
      "Speed: 6.8ms preprocess, 149.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.7ms\n",
      "Speed: 1.4ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 83.7ms\n",
      "Speed: 2.6ms preprocess, 83.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.3ms\n",
      "Speed: 2.1ms preprocess, 103.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.4ms\n",
      "Speed: 1.4ms preprocess, 71.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.3ms\n",
      "Speed: 1.4ms preprocess, 79.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 88.7ms\n",
      "Speed: 1.8ms preprocess, 88.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.3ms\n",
      "Speed: 1.4ms preprocess, 84.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 149.8ms\n",
      "Speed: 19.9ms preprocess, 149.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 2.0ms preprocess, 73.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 138.3ms\n",
      "Speed: 22.9ms preprocess, 138.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 184.7ms\n",
      "Speed: 1.7ms preprocess, 184.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 103.5ms\n",
      "Speed: 1.5ms preprocess, 103.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 1.7ms preprocess, 74.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 1.6ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 158.8ms\n",
      "Speed: 13.3ms preprocess, 158.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 158.9ms\n",
      "Speed: 20.9ms preprocess, 158.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 98.2ms\n",
      "Speed: 1.4ms preprocess, 98.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 86.9ms\n",
      "Speed: 1.5ms preprocess, 86.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.1ms\n",
      "Speed: 1.5ms preprocess, 75.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 120.6ms\n",
      "Speed: 1.5ms preprocess, 120.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 140.7ms\n",
      "Speed: 22.6ms preprocess, 140.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.2ms\n",
      "Speed: 1.7ms preprocess, 82.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 1.5ms preprocess, 72.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.8ms\n",
      "Speed: 1.9ms preprocess, 79.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 84.5ms\n",
      "Speed: 1.7ms preprocess, 84.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 2 persons, 89.8ms\n",
      "Speed: 1.7ms preprocess, 89.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 (no detections), 70.1ms\n",
      "Speed: 1.6ms preprocess, 70.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Average Score for the Video: 3.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discurweper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 3  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Discurweper.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Discurweper/segment_010422.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.3347 - mae: 1.9482 - val_loss: 6.5414 - val_mae: 2.3070\n",
      "Epoch 2/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7458 - mae: 1.0710 - val_loss: 5.2133 - val_mae: 2.0186\n",
      "Epoch 3/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4795 - mae: 1.0107 - val_loss: 4.1664 - val_mae: 1.8005\n",
      "Epoch 4/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4561 - mae: 0.9927 - val_loss: 2.8052 - val_mae: 1.4611\n",
      "Epoch 5/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2303 - mae: 0.9021 - val_loss: 2.2459 - val_mae: 1.2871\n",
      "Epoch 6/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1548 - mae: 0.8696 - val_loss: 1.8933 - val_mae: 1.1969\n",
      "Epoch 7/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1047 - mae: 0.8457 - val_loss: 1.6404 - val_mae: 1.1280\n",
      "Epoch 8/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0580 - mae: 0.8334 - val_loss: 1.4455 - val_mae: 1.0446\n",
      "Epoch 9/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0361 - mae: 0.8336 - val_loss: 1.3228 - val_mae: 0.9809\n",
      "Epoch 10/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9597 - mae: 0.8015 - val_loss: 1.2639 - val_mae: 0.9621\n",
      "Epoch 11/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0281 - mae: 0.8179 - val_loss: 1.3199 - val_mae: 0.9885\n",
      "Epoch 12/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9822 - mae: 0.8026 - val_loss: 1.2349 - val_mae: 0.9279\n",
      "Epoch 13/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9691 - mae: 0.7988 - val_loss: 1.1903 - val_mae: 0.8861\n",
      "Epoch 14/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9064 - mae: 0.7599 - val_loss: 1.2303 - val_mae: 0.9178\n",
      "Epoch 15/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9103 - mae: 0.7691 - val_loss: 1.2388 - val_mae: 0.9336\n",
      "Epoch 16/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8907 - mae: 0.7592 - val_loss: 1.3187 - val_mae: 0.9314\n",
      "Epoch 17/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9454 - mae: 0.7801 - val_loss: 1.1418 - val_mae: 0.8570\n",
      "Epoch 18/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9259 - mae: 0.7694 - val_loss: 1.1150 - val_mae: 0.8337\n",
      "Epoch 19/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8689 - mae: 0.7460 - val_loss: 1.2695 - val_mae: 0.8545\n",
      "Epoch 20/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9110 - mae: 0.7533 - val_loss: 1.0474 - val_mae: 0.8118\n",
      "Epoch 21/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9191 - mae: 0.7494 - val_loss: 0.9647 - val_mae: 0.7750\n",
      "Epoch 22/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8203 - mae: 0.7253 - val_loss: 1.1772 - val_mae: 0.8261\n",
      "Epoch 23/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8753 - mae: 0.7340 - val_loss: 1.1219 - val_mae: 0.8157\n",
      "Epoch 24/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8129 - mae: 0.7088 - val_loss: 1.0395 - val_mae: 0.7878\n",
      "Epoch 25/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7536 - mae: 0.6910 - val_loss: 0.9756 - val_mae: 0.7881\n",
      "Epoch 26/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7923 - mae: 0.6998 - val_loss: 1.1324 - val_mae: 0.7867\n",
      "Epoch 27/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8164 - mae: 0.6972 - val_loss: 0.9438 - val_mae: 0.7701\n",
      "Epoch 28/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7933 - mae: 0.7065 - val_loss: 1.0168 - val_mae: 0.7609\n",
      "Epoch 29/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7844 - mae: 0.6834 - val_loss: 0.9590 - val_mae: 0.7686\n",
      "Epoch 30/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8008 - mae: 0.6986 - val_loss: 1.0199 - val_mae: 0.7893\n",
      "Epoch 31/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7165 - mae: 0.6504 - val_loss: 1.4706 - val_mae: 0.9279\n",
      "Epoch 32/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7424 - mae: 0.6655 - val_loss: 1.1688 - val_mae: 0.8087\n",
      "Epoch 33/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7515 - mae: 0.6499 - val_loss: 0.9082 - val_mae: 0.7172\n",
      "Epoch 34/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6585 - mae: 0.6136 - val_loss: 1.2274 - val_mae: 0.8526\n",
      "Epoch 35/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6635 - mae: 0.6303 - val_loss: 0.8468 - val_mae: 0.7200\n",
      "Epoch 36/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7016 - mae: 0.6418 - val_loss: 1.0940 - val_mae: 0.8301\n",
      "Epoch 37/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7271 - mae: 0.6571 - val_loss: 0.9776 - val_mae: 0.7291\n",
      "Epoch 38/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6816 - mae: 0.6296 - val_loss: 0.8453 - val_mae: 0.6989\n",
      "Epoch 39/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6592 - mae: 0.6217 - val_loss: 0.8823 - val_mae: 0.7031\n",
      "Epoch 40/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6770 - mae: 0.6210 - val_loss: 0.8792 - val_mae: 0.6789\n",
      "Epoch 41/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6470 - mae: 0.6129 - val_loss: 0.7972 - val_mae: 0.6719\n",
      "Epoch 42/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6676 - mae: 0.6112 - val_loss: 0.9762 - val_mae: 0.7335\n",
      "Epoch 43/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6539 - mae: 0.6020 - val_loss: 0.6883 - val_mae: 0.6091\n",
      "Epoch 44/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5838 - mae: 0.5717 - val_loss: 0.9013 - val_mae: 0.7087\n",
      "Epoch 45/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5989 - mae: 0.5736 - val_loss: 0.6880 - val_mae: 0.6139\n",
      "Epoch 46/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5779 - mae: 0.5646 - val_loss: 0.8212 - val_mae: 0.6589\n",
      "Epoch 47/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6056 - mae: 0.5889 - val_loss: 0.6916 - val_mae: 0.6264\n",
      "Epoch 48/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5511 - mae: 0.5520 - val_loss: 0.8199 - val_mae: 0.6773\n",
      "Epoch 49/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5557 - mae: 0.5663 - val_loss: 0.7614 - val_mae: 0.6148\n",
      "Epoch 50/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6366 - mae: 0.5902 - val_loss: 0.7290 - val_mae: 0.6236\n",
      "Epoch 51/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5210 - mae: 0.5334 - val_loss: 1.1667 - val_mae: 0.8400\n",
      "Epoch 52/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5784 - mae: 0.5599 - val_loss: 0.6338 - val_mae: 0.5812\n",
      "Epoch 53/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5511 - mae: 0.5599 - val_loss: 0.7034 - val_mae: 0.6296\n",
      "Epoch 54/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5886 - mae: 0.5696 - val_loss: 0.7503 - val_mae: 0.6273\n",
      "Epoch 55/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5809 - mae: 0.5815 - val_loss: 0.7293 - val_mae: 0.6570\n",
      "Epoch 56/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6205 - mae: 0.5784 - val_loss: 0.6823 - val_mae: 0.6303\n",
      "Epoch 57/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5210 - mae: 0.5319 - val_loss: 0.6741 - val_mae: 0.5984\n",
      "Epoch 58/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5218 - mae: 0.5402 - val_loss: 0.7460 - val_mae: 0.6231\n",
      "Epoch 59/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5207 - mae: 0.5259 - val_loss: 0.7417 - val_mae: 0.6073\n",
      "Epoch 60/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5683 - mae: 0.5520 - val_loss: 0.6308 - val_mae: 0.5519\n",
      "Epoch 61/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4673 - mae: 0.5069 - val_loss: 0.8176 - val_mae: 0.6424\n",
      "Epoch 62/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5527 - mae: 0.5576 - val_loss: 0.6284 - val_mae: 0.5710\n",
      "Epoch 63/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5253 - mae: 0.5307 - val_loss: 0.5777 - val_mae: 0.5256\n",
      "Epoch 64/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4961 - mae: 0.5053 - val_loss: 0.6031 - val_mae: 0.5508\n",
      "Epoch 65/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5068 - mae: 0.5179 - val_loss: 0.5838 - val_mae: 0.5534\n",
      "Epoch 66/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4696 - mae: 0.4921 - val_loss: 0.6170 - val_mae: 0.5563\n",
      "Epoch 67/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4728 - mae: 0.4926 - val_loss: 0.8264 - val_mae: 0.6526\n",
      "Epoch 68/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4595 - mae: 0.4803 - val_loss: 0.7515 - val_mae: 0.6270\n",
      "Epoch 69/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4987 - mae: 0.5126 - val_loss: 0.6653 - val_mae: 0.5633\n",
      "Epoch 70/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4496 - mae: 0.4898 - val_loss: 0.6514 - val_mae: 0.5521\n",
      "Epoch 71/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4430 - mae: 0.4789 - val_loss: 0.5096 - val_mae: 0.5089\n",
      "Epoch 72/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5213 - mae: 0.5165 - val_loss: 0.5983 - val_mae: 0.5621\n",
      "Epoch 73/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5067 - mae: 0.5134 - val_loss: 0.6499 - val_mae: 0.5428\n",
      "Epoch 74/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5168 - mae: 0.5076 - val_loss: 0.5668 - val_mae: 0.5206\n",
      "Epoch 75/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4461 - mae: 0.4719 - val_loss: 0.7292 - val_mae: 0.6398\n",
      "Epoch 76/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4839 - mae: 0.5052 - val_loss: 0.5631 - val_mae: 0.5449\n",
      "Epoch 77/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4776 - mae: 0.5015 - val_loss: 0.7664 - val_mae: 0.6204\n",
      "Epoch 78/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5151 - mae: 0.5196 - val_loss: 0.6307 - val_mae: 0.5453\n",
      "Epoch 79/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4347 - mae: 0.4697 - val_loss: 0.5544 - val_mae: 0.5081\n",
      "Epoch 80/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4043 - mae: 0.4533 - val_loss: 0.6792 - val_mae: 0.5703\n",
      "Epoch 81/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4655 - mae: 0.4750 - val_loss: 0.5849 - val_mae: 0.5082\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5658 - mae: 0.5139\n",
      "Test Loss: 0.5849, Test MAE: 0.5082\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.6249\n",
      "\n",
      "0: 640x640 1 person, 88.8ms\n",
      "Speed: 5.6ms preprocess, 88.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 (no detections), 128.7ms\n",
      "Speed: 1.7ms preprocess, 128.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 79.8ms\n",
      "Speed: 1.4ms preprocess, 79.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.6ms\n",
      "Speed: 1.8ms preprocess, 70.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.4ms preprocess, 71.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.8ms\n",
      "Speed: 1.6ms preprocess, 63.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Average Score for the Video: 3.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Discurweper.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Estafette.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Estafette/segment_002126.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.7797 - mae: 2.2122 - val_loss: 7.7235 - val_mae: 2.5017\n",
      "Epoch 2/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9416 - mae: 1.1000 - val_loss: 6.4972 - val_mae: 2.2623\n",
      "Epoch 3/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7563 - mae: 1.0302 - val_loss: 5.7327 - val_mae: 2.1116\n",
      "Epoch 4/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5789 - mae: 0.9622 - val_loss: 4.9337 - val_mae: 1.9350\n",
      "Epoch 5/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5000 - mae: 0.9622 - val_loss: 3.9127 - val_mae: 1.6887\n",
      "Epoch 6/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5460 - mae: 0.9695 - val_loss: 3.6337 - val_mae: 1.6178\n",
      "Epoch 7/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4659 - mae: 0.9262 - val_loss: 2.8420 - val_mae: 1.3921\n",
      "Epoch 8/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3253 - mae: 0.8785 - val_loss: 2.1043 - val_mae: 1.1514\n",
      "Epoch 9/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3258 - mae: 0.8843 - val_loss: 2.1643 - val_mae: 1.1873\n",
      "Epoch 10/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2930 - mae: 0.8761 - val_loss: 1.8106 - val_mae: 1.0711\n",
      "Epoch 11/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1462 - mae: 0.8113 - val_loss: 1.2946 - val_mae: 0.8819\n",
      "Epoch 12/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1813 - mae: 0.8442 - val_loss: 1.3327 - val_mae: 0.8857\n",
      "Epoch 13/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2136 - mae: 0.8186 - val_loss: 1.3840 - val_mae: 0.9186\n",
      "Epoch 14/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1961 - mae: 0.8471 - val_loss: 1.4671 - val_mae: 0.9403\n",
      "Epoch 15/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1787 - mae: 0.8044 - val_loss: 1.0270 - val_mae: 0.7463\n",
      "Epoch 16/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0532 - mae: 0.7603 - val_loss: 1.0739 - val_mae: 0.7775\n",
      "Epoch 17/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1511 - mae: 0.8115 - val_loss: 1.1180 - val_mae: 0.8107\n",
      "Epoch 18/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1261 - mae: 0.8074 - val_loss: 1.0349 - val_mae: 0.7409\n",
      "Epoch 19/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1548 - mae: 0.7889 - val_loss: 0.9822 - val_mae: 0.7025\n",
      "Epoch 20/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0404 - mae: 0.7664 - val_loss: 1.1864 - val_mae: 0.7844\n",
      "Epoch 21/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0828 - mae: 0.7805 - val_loss: 1.0996 - val_mae: 0.8041\n",
      "Epoch 22/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0166 - mae: 0.7614 - val_loss: 1.0716 - val_mae: 0.7795\n",
      "Epoch 23/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0849 - mae: 0.7868 - val_loss: 1.1133 - val_mae: 0.7287\n",
      "Epoch 24/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0318 - mae: 0.7515 - val_loss: 1.1244 - val_mae: 0.7507\n",
      "Epoch 25/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0416 - mae: 0.7585 - val_loss: 1.0526 - val_mae: 0.6814\n",
      "Epoch 26/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0256 - mae: 0.7515 - val_loss: 1.1356 - val_mae: 0.7855\n",
      "Epoch 27/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0116 - mae: 0.7416 - val_loss: 1.0465 - val_mae: 0.7479\n",
      "Epoch 28/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9508 - mae: 0.7242 - val_loss: 0.9864 - val_mae: 0.6919\n",
      "Epoch 29/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8883 - mae: 0.6967 - val_loss: 0.9953 - val_mae: 0.6632\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9278 - mae: 0.6419\n",
      "Test Loss: 0.9953, Test MAE: 0.6632\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.3796\n",
      "\n",
      "0: 640x640 1 person, 95.4ms\n",
      "Speed: 2.2ms preprocess, 95.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.9ms\n",
      "Speed: 1.6ms preprocess, 95.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 640x640 1 person, 81.0ms\n",
      "Speed: 1.8ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.2ms\n",
      "Speed: 1.7ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.3ms\n",
      "Speed: 1.5ms preprocess, 73.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.6ms\n",
      "Speed: 1.5ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.5ms preprocess, 72.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.6ms\n",
      "Speed: 1.4ms preprocess, 65.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 78.4ms\n",
      "Speed: 1.4ms preprocess, 78.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.9ms preprocess, 72.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 81.2ms\n",
      "Speed: 1.5ms preprocess, 81.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.5ms\n",
      "Speed: 1.6ms preprocess, 71.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.8ms\n",
      "Speed: 1.5ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 81.6ms\n",
      "Speed: 1.6ms preprocess, 81.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.7ms\n",
      "Speed: 1.6ms preprocess, 70.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.9ms\n",
      "Speed: 1.8ms preprocess, 72.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.0ms\n",
      "Speed: 1.4ms preprocess, 66.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.0ms\n",
      "Speed: 1.6ms preprocess, 64.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.9ms\n",
      "Speed: 2.0ms preprocess, 73.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.0ms\n",
      "Speed: 1.8ms preprocess, 74.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.6ms\n",
      "Speed: 1.5ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.0ms\n",
      "Speed: 1.4ms preprocess, 66.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.3ms\n",
      "Speed: 2.5ms preprocess, 72.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.8ms\n",
      "Speed: 1.8ms preprocess, 67.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.6ms\n",
      "Speed: 1.7ms preprocess, 73.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.0ms\n",
      "Speed: 1.8ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.2ms\n",
      "Speed: 1.6ms preprocess, 76.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.3ms\n",
      "Speed: 1.4ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.8ms\n",
      "Speed: 1.5ms preprocess, 71.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.1ms\n",
      "Speed: 1.5ms preprocess, 71.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.6ms\n",
      "Speed: 1.4ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.3ms\n",
      "Speed: 3.6ms preprocess, 76.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 78.0ms\n",
      "Speed: 1.8ms preprocess, 78.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.1ms\n",
      "Speed: 1.8ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.0ms\n",
      "Speed: 2.2ms preprocess, 70.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.6ms\n",
      "Speed: 1.4ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.1ms\n",
      "Speed: 1.7ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.7ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.3ms\n",
      "Speed: 2.3ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.2ms\n",
      "Speed: 1.6ms preprocess, 68.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.9ms\n",
      "Speed: 1.6ms preprocess, 92.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.7ms\n",
      "Speed: 2.0ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 1.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.5ms\n",
      "Speed: 1.5ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.3ms\n",
      "Speed: 1.5ms preprocess, 75.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.0ms\n",
      "Speed: 1.5ms preprocess, 72.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 110.8ms\n",
      "Speed: 1.7ms preprocess, 110.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 1.6ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.6ms\n",
      "Speed: 1.6ms preprocess, 74.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.9ms\n",
      "Speed: 1.3ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.6ms\n",
      "Speed: 1.4ms preprocess, 67.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.5ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.1ms\n",
      "Speed: 1.5ms preprocess, 65.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.0ms\n",
      "Speed: 1.4ms preprocess, 66.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.5ms\n",
      "Speed: 1.4ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.2ms\n",
      "Speed: 1.6ms preprocess, 66.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.2ms\n",
      "Speed: 1.4ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.0ms\n",
      "Speed: 1.5ms preprocess, 67.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.2ms\n",
      "Speed: 1.4ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.6ms\n",
      "Speed: 1.6ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.6ms\n",
      "Speed: 1.6ms preprocess, 96.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.7ms\n",
      "Speed: 1.5ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.5ms\n",
      "Speed: 1.7ms preprocess, 72.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.3ms\n",
      "Speed: 1.5ms preprocess, 67.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.2ms\n",
      "Speed: 1.5ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.7ms\n",
      "Speed: 1.5ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Average Score for the Video: 3.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Hoogspringen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Hoogspringen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hoogspringen/segment_001127.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 10.0488 - mae: 2.7925 - val_loss: 9.2864 - val_mae: 2.8450\n",
      "Epoch 2/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3339 - mae: 1.2088 - val_loss: 7.7993 - val_mae: 2.5881\n",
      "Epoch 3/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8054 - mae: 1.0525 - val_loss: 6.3457 - val_mae: 2.3112\n",
      "Epoch 4/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7212 - mae: 1.0202 - val_loss: 5.2424 - val_mae: 2.0692\n",
      "Epoch 5/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4455 - mae: 0.9262 - val_loss: 3.9186 - val_mae: 1.7359\n",
      "Epoch 6/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5452 - mae: 0.9708 - val_loss: 3.1605 - val_mae: 1.5263\n",
      "Epoch 7/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2023 - mae: 0.8511 - val_loss: 2.7121 - val_mae: 1.3889\n",
      "Epoch 8/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1508 - mae: 0.8099 - val_loss: 2.0498 - val_mae: 1.1532\n",
      "Epoch 9/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0739 - mae: 0.7947 - val_loss: 1.5033 - val_mae: 0.9659\n",
      "Epoch 10/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2377 - mae: 0.8469 - val_loss: 1.5017 - val_mae: 0.9623\n",
      "Epoch 11/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0683 - mae: 0.7616 - val_loss: 1.2308 - val_mae: 0.8778\n",
      "Epoch 12/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0409 - mae: 0.7622 - val_loss: 1.2532 - val_mae: 0.8918\n",
      "Epoch 13/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9842 - mae: 0.7577 - val_loss: 1.0794 - val_mae: 0.8018\n",
      "Epoch 14/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0631 - mae: 0.7713 - val_loss: 0.9536 - val_mae: 0.7087\n",
      "Epoch 15/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0040 - mae: 0.7391 - val_loss: 1.0041 - val_mae: 0.7512\n",
      "Epoch 16/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9341 - mae: 0.7071 - val_loss: 1.0289 - val_mae: 0.7608\n",
      "Epoch 17/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9709 - mae: 0.7252 - val_loss: 0.9612 - val_mae: 0.7086\n",
      "Epoch 18/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9879 - mae: 0.7353 - val_loss: 1.0590 - val_mae: 0.7948\n",
      "Epoch 19/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9059 - mae: 0.7129 - val_loss: 0.9469 - val_mae: 0.7304\n",
      "Epoch 20/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0120 - mae: 0.7461 - val_loss: 0.8950 - val_mae: 0.6904\n",
      "Epoch 21/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9101 - mae: 0.7147 - val_loss: 0.9931 - val_mae: 0.7401\n",
      "Epoch 22/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8953 - mae: 0.6993 - val_loss: 1.0504 - val_mae: 0.7776\n",
      "Epoch 23/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0137 - mae: 0.7398 - val_loss: 0.9419 - val_mae: 0.6877\n",
      "Epoch 24/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9747 - mae: 0.7086 - val_loss: 0.9802 - val_mae: 0.7475\n",
      "Epoch 25/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8776 - mae: 0.6911 - val_loss: 0.9298 - val_mae: 0.7212\n",
      "Epoch 26/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9189 - mae: 0.7008 - val_loss: 0.9310 - val_mae: 0.7071\n",
      "Epoch 27/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9065 - mae: 0.6904 - val_loss: 0.8758 - val_mae: 0.6690\n",
      "Epoch 28/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8673 - mae: 0.6771 - val_loss: 0.8341 - val_mae: 0.6653\n",
      "Epoch 29/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9155 - mae: 0.6948 - val_loss: 0.9039 - val_mae: 0.6953\n",
      "Epoch 30/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9354 - mae: 0.7066 - val_loss: 0.8989 - val_mae: 0.6711\n",
      "Epoch 31/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8815 - mae: 0.6881 - val_loss: 0.8291 - val_mae: 0.6424\n",
      "Epoch 32/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8092 - mae: 0.6658 - val_loss: 0.8697 - val_mae: 0.6574\n",
      "Epoch 33/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8732 - mae: 0.6692 - val_loss: 0.9031 - val_mae: 0.6907\n",
      "Epoch 34/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8146 - mae: 0.6547 - val_loss: 1.0373 - val_mae: 0.7734\n",
      "Epoch 35/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8886 - mae: 0.6812 - val_loss: 1.0209 - val_mae: 0.7441\n",
      "Epoch 36/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8026 - mae: 0.6604 - val_loss: 0.8076 - val_mae: 0.6045\n",
      "Epoch 37/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8489 - mae: 0.6530 - val_loss: 0.8647 - val_mae: 0.6636\n",
      "Epoch 38/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8441 - mae: 0.6635 - val_loss: 0.9289 - val_mae: 0.7238\n",
      "Epoch 39/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7882 - mae: 0.6386 - val_loss: 0.8931 - val_mae: 0.6934\n",
      "Epoch 40/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8197 - mae: 0.6530 - val_loss: 1.0547 - val_mae: 0.7674\n",
      "Epoch 41/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7980 - mae: 0.6390 - val_loss: 0.8288 - val_mae: 0.6598\n",
      "Epoch 42/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8156 - mae: 0.6407 - val_loss: 0.8473 - val_mae: 0.6403\n",
      "Epoch 43/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7784 - mae: 0.6278 - val_loss: 0.9390 - val_mae: 0.6981\n",
      "Epoch 44/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8093 - mae: 0.6317 - val_loss: 0.8219 - val_mae: 0.6369\n",
      "Epoch 45/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7706 - mae: 0.6121 - val_loss: 0.8176 - val_mae: 0.6438\n",
      "Epoch 46/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7698 - mae: 0.6277 - val_loss: 0.9987 - val_mae: 0.7276\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0719 - mae: 0.7311\n",
      "Test Loss: 0.9987, Test MAE: 0.7276\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "R² Score: 0.3676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0: 384x640 1 person, 52.0ms\n",
      "Speed: 0.9ms preprocess, 52.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.7ms\n",
      "Speed: 2.0ms preprocess, 56.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.7ms\n",
      "Speed: 0.9ms preprocess, 60.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.0ms\n",
      "Speed: 0.9ms preprocess, 66.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 1.2ms preprocess, 49.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 0.7ms preprocess, 46.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.7ms preprocess, 35.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.8ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.6ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.8ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 0.7ms preprocess, 47.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 82.1ms\n",
      "Speed: 0.9ms preprocess, 82.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.5ms\n",
      "Speed: 0.9ms preprocess, 51.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 0.9ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.9ms preprocess, 43.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 66.7ms\n",
      "Speed: 1.2ms preprocess, 66.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.6ms\n",
      "Speed: 1.0ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.8ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 104.5ms\n",
      "Speed: 3.0ms preprocess, 104.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 1.0ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.9ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.6ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.8ms\n",
      "Speed: 0.8ms preprocess, 52.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.9ms\n",
      "Speed: 1.1ms preprocess, 51.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.6ms\n",
      "Speed: 1.5ms preprocess, 58.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.6ms\n",
      "Speed: 1.0ms preprocess, 60.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 0.8ms preprocess, 45.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.9ms\n",
      "Speed: 0.7ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.0ms\n",
      "Speed: 0.7ms preprocess, 50.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.4ms\n",
      "Speed: 2.4ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.2ms\n",
      "Speed: 0.7ms preprocess, 58.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 0.7ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.9ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.9ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.0ms\n",
      "Speed: 0.9ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.2ms\n",
      "Speed: 0.8ms preprocess, 54.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.0ms\n",
      "Speed: 0.7ms preprocess, 52.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.0ms\n",
      "Speed: 1.0ms preprocess, 53.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 0.9ms preprocess, 49.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.6ms\n",
      "Speed: 1.3ms preprocess, 52.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 0.7ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 0.8ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.8ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.0ms\n",
      "Speed: 0.9ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.7ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.9ms\n",
      "Speed: 0.7ms preprocess, 52.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.3ms\n",
      "Speed: 0.9ms preprocess, 87.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.9ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 0.7ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.9ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 1.1ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.5ms\n",
      "Speed: 1.1ms preprocess, 46.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.2ms\n",
      "Speed: 0.7ms preprocess, 48.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.7ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 (no detections), 44.1ms\n",
      "Speed: 1.4ms preprocess, 44.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.7ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.8ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.8ms\n",
      "Speed: 0.8ms preprocess, 48.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.6ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 0.7ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.7ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.8ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.6ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 1.2ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.0ms\n",
      "Speed: 0.8ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.9ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 94.9ms\n",
      "Speed: 0.9ms preprocess, 94.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.8ms\n",
      "Speed: 0.7ms preprocess, 50.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.7ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 0.7ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.6ms\n",
      "Speed: 0.7ms preprocess, 46.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 1.2ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 1.1ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.7ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 0.7ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 1.4ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.1ms\n",
      "Speed: 0.6ms preprocess, 47.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.6ms\n",
      "Speed: 1.0ms preprocess, 52.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 1.0ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.3ms\n",
      "Speed: 0.8ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 0.7ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.1ms\n",
      "Speed: 1.3ms preprocess, 49.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.9ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.9ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.8ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.7ms\n",
      "Speed: 0.8ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.6ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.9ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.9ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 51.2ms\n",
      "Speed: 0.7ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.8ms\n",
      "Speed: 0.9ms preprocess, 54.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 0.8ms preprocess, 48.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 107.0ms\n",
      "Speed: 0.8ms preprocess, 107.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.1ms\n",
      "Speed: 0.9ms preprocess, 60.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.8ms\n",
      "Speed: 0.9ms preprocess, 60.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 63.7ms\n",
      "Speed: 1.0ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.0ms\n",
      "Speed: 0.9ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.6ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.8ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.9ms\n",
      "Speed: 0.8ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.7ms\n",
      "Speed: 0.7ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.7ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 62.2ms\n",
      "Speed: 8.2ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.7ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.6ms\n",
      "Speed: 0.8ms preprocess, 47.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.7ms preprocess, 38.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.4ms\n",
      "Speed: 1.1ms preprocess, 61.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 60.7ms\n",
      "Speed: 0.8ms preprocess, 60.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.9ms\n",
      "Speed: 0.8ms preprocess, 53.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 0.7ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.2ms\n",
      "Speed: 0.7ms preprocess, 93.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 0.9ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 1.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.7ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 0.7ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.6ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.9ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.9ms\n",
      "Speed: 0.8ms preprocess, 81.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.6ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 1.0ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.7ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 51.3ms\n",
      "Speed: 0.9ms preprocess, 51.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.1ms\n",
      "Speed: 1.0ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 2 persons, 66.0ms\n",
      "Speed: 1.1ms preprocess, 66.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 2 persons, 50.9ms\n",
      "Speed: 2.2ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 0.7ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.1ms\n",
      "Speed: 2.3ms preprocess, 90.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.9ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.7ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 0.7ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.5ms\n",
      "Speed: 0.9ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.8ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.8ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.8ms\n",
      "Speed: 0.7ms preprocess, 48.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 1.5ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.6ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.9ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.8ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.8ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.6ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.2ms\n",
      "Speed: 0.8ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 2.5ms preprocess, 79.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 1.0ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.9ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.9ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 1.0ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 97.5ms\n",
      "Speed: 1.1ms preprocess, 97.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.8ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.9ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 53.1ms\n",
      "Speed: 1.2ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.4ms\n",
      "Speed: 0.9ms preprocess, 57.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.8ms preprocess, 44.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.6ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.7ms\n",
      "Speed: 0.7ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 61.3ms\n",
      "Speed: 0.9ms preprocess, 61.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 64.9ms\n",
      "Speed: 2.2ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.3ms\n",
      "Speed: 0.8ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.6ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.6ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.6ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.7ms\n",
      "Speed: 0.7ms preprocess, 90.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.6ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.9ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 116.3ms\n",
      "Speed: 0.8ms preprocess, 116.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.4ms\n",
      "Speed: 1.3ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 0.7ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.9ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 0.8ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 1.1ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 2 persons, 44.8ms\n",
      "Speed: 0.7ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.6ms\n",
      "Speed: 0.7ms preprocess, 47.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.5ms\n",
      "Speed: 1.6ms preprocess, 75.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.6ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.8ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.8ms preprocess, 52.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 87.6ms\n",
      "Speed: 1.1ms preprocess, 87.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.1ms\n",
      "Speed: 0.9ms preprocess, 52.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Average Score for the Video: 3.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Hordenlopen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Hordelopen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Hordenlopen/segment_001695.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.7957 - mae: 2.5806 - val_loss: 8.6824 - val_mae: 2.7881\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5307 - mae: 0.9772 - val_loss: 7.9351 - val_mae: 2.6581\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1255 - mae: 0.8272 - val_loss: 6.4055 - val_mae: 2.3625\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0359 - mae: 0.7898 - val_loss: 5.2709 - val_mae: 2.1336\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9907 - mae: 0.7929 - val_loss: 4.1137 - val_mae: 1.8765\n",
      "Epoch 6/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7679 - mae: 0.6714 - val_loss: 2.7977 - val_mae: 1.5357\n",
      "Epoch 7/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6970 - mae: 0.6451 - val_loss: 2.0612 - val_mae: 1.2954\n",
      "Epoch 8/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6998 - mae: 0.6459 - val_loss: 1.5963 - val_mae: 1.1317\n",
      "Epoch 9/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6534 - mae: 0.6158 - val_loss: 1.2567 - val_mae: 0.9834\n",
      "Epoch 10/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6918 - mae: 0.6340 - val_loss: 0.9522 - val_mae: 0.8267\n",
      "Epoch 11/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5590 - mae: 0.5703 - val_loss: 0.6142 - val_mae: 0.6188\n",
      "Epoch 12/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5776 - mae: 0.5718 - val_loss: 0.5637 - val_mae: 0.5997\n",
      "Epoch 13/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5102 - mae: 0.5415 - val_loss: 0.4701 - val_mae: 0.5377\n",
      "Epoch 14/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5137 - mae: 0.5399 - val_loss: 0.4853 - val_mae: 0.5462\n",
      "Epoch 15/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4946 - mae: 0.5292 - val_loss: 0.3057 - val_mae: 0.3950\n",
      "Epoch 16/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4484 - mae: 0.5077 - val_loss: 0.3833 - val_mae: 0.4608\n",
      "Epoch 17/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4120 - mae: 0.4915 - val_loss: 0.3028 - val_mae: 0.3852\n",
      "Epoch 18/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4080 - mae: 0.4799 - val_loss: 0.2680 - val_mae: 0.3655\n",
      "Epoch 19/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5390 - mae: 0.5473 - val_loss: 0.3110 - val_mae: 0.4007\n",
      "Epoch 20/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4423 - mae: 0.4810 - val_loss: 0.3097 - val_mae: 0.3968\n",
      "Epoch 21/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4157 - mae: 0.4974 - val_loss: 0.2910 - val_mae: 0.3848\n",
      "Epoch 22/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3766 - mae: 0.4694 - val_loss: 0.2833 - val_mae: 0.3700\n",
      "Epoch 23/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4290 - mae: 0.4766 - val_loss: 0.3111 - val_mae: 0.3901\n",
      "Epoch 24/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3536 - mae: 0.4349 - val_loss: 0.4155 - val_mae: 0.5009\n",
      "Epoch 25/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3928 - mae: 0.4746 - val_loss: 0.2974 - val_mae: 0.3855\n",
      "Epoch 26/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3276 - mae: 0.4182 - val_loss: 0.3093 - val_mae: 0.3898\n",
      "Epoch 27/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3928 - mae: 0.4604 - val_loss: 0.2777 - val_mae: 0.3592\n",
      "Epoch 28/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3719 - mae: 0.4499 - val_loss: 0.4235 - val_mae: 0.4920\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4776 - mae: 0.4975\n",
      "Test Loss: 0.4235, Test MAE: 0.4920\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5620\n",
      "\n",
      "0: 384x640 3 persons, 53.2ms\n",
      "Speed: 0.8ms preprocess, 53.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 61.2ms\n",
      "Speed: 1.4ms preprocess, 61.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.7ms\n",
      "Speed: 1.1ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.7ms\n",
      "Speed: 0.8ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 0.9ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.1ms\n",
      "Speed: 0.9ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 64.2ms\n",
      "Speed: 0.7ms preprocess, 64.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.8ms\n",
      "Speed: 0.8ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.6ms\n",
      "Speed: 1.3ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.5ms\n",
      "Speed: 0.7ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.5ms\n",
      "Speed: 1.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.4ms\n",
      "Speed: 1.2ms preprocess, 44.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.2ms\n",
      "Speed: 0.8ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.3ms\n",
      "Speed: 0.9ms preprocess, 43.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.2ms\n",
      "Speed: 0.8ms preprocess, 43.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.3ms\n",
      "Speed: 0.8ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.6ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.7ms\n",
      "Speed: 0.7ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 76.9ms\n",
      "Speed: 4.3ms preprocess, 76.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.0ms\n",
      "Speed: 0.7ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.8ms\n",
      "Speed: 0.6ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.0ms\n",
      "Speed: 0.6ms preprocess, 56.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.4ms\n",
      "Speed: 0.7ms preprocess, 50.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 3 persons, 187.2ms\n",
      "Speed: 4.7ms preprocess, 187.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 384x640 3 persons, 108.9ms\n",
      "Speed: 2.3ms preprocess, 108.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 1.4ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 0.8ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 0.9ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.1ms\n",
      "Speed: 0.6ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.4ms\n",
      "Speed: 0.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.3ms\n",
      "Speed: 3.2ms preprocess, 51.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.1ms\n",
      "Speed: 0.7ms preprocess, 48.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 1.0ms preprocess, 39.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.0ms\n",
      "Speed: 0.9ms preprocess, 40.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.0ms\n",
      "Speed: 0.6ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 82.2ms\n",
      "Speed: 0.9ms preprocess, 82.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "0: 384x640 3 persons, 73.9ms\n",
      "Speed: 2.3ms preprocess, 73.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 51.1ms\n",
      "Speed: 0.8ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 1.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.3ms\n",
      "Speed: 1.0ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 0.9ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "0: 384x640 3 persons, 70.6ms\n",
      "Speed: 1.3ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.5ms\n",
      "Speed: 1.3ms preprocess, 47.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.4ms\n",
      "Speed: 0.8ms preprocess, 50.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.3ms\n",
      "Speed: 0.8ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.2ms\n",
      "Speed: 0.8ms preprocess, 49.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 48.9ms\n",
      "Speed: 0.8ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.9ms\n",
      "Speed: 0.7ms preprocess, 56.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.8ms\n",
      "Speed: 1.1ms preprocess, 47.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 50.6ms\n",
      "Speed: 0.8ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.7ms\n",
      "Speed: 0.9ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.7ms\n",
      "Speed: 0.7ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.3ms\n",
      "Speed: 1.2ms preprocess, 49.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.9ms\n",
      "Speed: 0.7ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 100.0ms\n",
      "Speed: 0.7ms preprocess, 100.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.3ms\n",
      "Speed: 0.8ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.1ms\n",
      "Speed: 0.6ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.4ms\n",
      "Speed: 0.8ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.7ms\n",
      "Speed: 0.6ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.7ms\n",
      "Speed: 1.0ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.8ms\n",
      "Speed: 0.6ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 88.2ms\n",
      "Speed: 3.3ms preprocess, 88.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 384x640 3 persons, 53.4ms\n",
      "Speed: 2.9ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 56.4ms\n",
      "Speed: 0.7ms preprocess, 56.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.1ms\n",
      "Speed: 0.6ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.0ms\n",
      "Speed: 0.7ms preprocess, 43.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 0.8ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.0ms\n",
      "Speed: 0.7ms preprocess, 46.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.4ms\n",
      "Speed: 0.8ms preprocess, 45.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 47.0ms\n",
      "Speed: 1.0ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.8ms\n",
      "Speed: 0.8ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 0.6ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.4ms\n",
      "Speed: 0.9ms preprocess, 54.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 46.4ms\n",
      "Speed: 0.8ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 54.4ms\n",
      "Speed: 1.0ms preprocess, 54.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 52.3ms\n",
      "Speed: 0.7ms preprocess, 52.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 3 persons, 44.9ms\n",
      "Speed: 0.7ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 49.7ms\n",
      "Speed: 0.7ms preprocess, 49.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 3 persons, 87.5ms\n",
      "Speed: 2.1ms preprocess, 87.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 3 persons, 187.2ms\n",
      "Speed: 5.0ms preprocess, 187.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 3 persons, 39.4ms\n",
      "Speed: 0.5ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 40.6ms\n",
      "Speed: 1.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.5ms\n",
      "Speed: 0.6ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 35.7ms\n",
      "Speed: 1.0ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.9ms\n",
      "Speed: 0.8ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.6ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.0ms\n",
      "Speed: 1.0ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.6ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.1ms\n",
      "Speed: 0.8ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.2ms\n",
      "Speed: 1.1ms preprocess, 36.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.8ms\n",
      "Speed: 0.8ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.6ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 72.6ms\n",
      "Speed: 1.6ms preprocess, 72.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.4ms\n",
      "Speed: 0.6ms preprocess, 36.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.6ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.3ms\n",
      "Speed: 1.0ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.8ms\n",
      "Speed: 0.8ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 384x640 4 persons, 48.3ms\n",
      "Speed: 0.9ms preprocess, 48.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 67.4ms\n",
      "Speed: 1.9ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 0.8ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.9ms\n",
      "Speed: 0.6ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.6ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 81.6ms\n",
      "Speed: 1.6ms preprocess, 81.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.2ms\n",
      "Speed: 1.6ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.6ms\n",
      "Speed: 0.8ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 41.7ms\n",
      "Speed: 0.9ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.9ms\n",
      "Speed: 0.6ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.6ms\n",
      "Speed: 0.9ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.1ms\n",
      "Speed: 0.6ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 70.4ms\n",
      "Speed: 2.7ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.9ms\n",
      "Speed: 0.8ms preprocess, 37.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.0ms\n",
      "Speed: 0.9ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.8ms\n",
      "Speed: 0.7ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 4 persons, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.9ms\n",
      "Speed: 0.6ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 66.0ms\n",
      "Speed: 1.1ms preprocess, 66.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.5ms\n",
      "Speed: 1.0ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 0.6ms preprocess, 38.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.2ms\n",
      "Speed: 1.1ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 4 persons, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 38.8ms\n",
      "Speed: 0.8ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 4 persons, 37.7ms\n",
      "Speed: 1.0ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 3 persons, 35.6ms\n",
      "Speed: 0.7ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 68.7ms\n",
      "Speed: 1.6ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.6ms\n",
      "Speed: 0.6ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 61.5ms\n",
      "Speed: 11.1ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.7ms preprocess, 35.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.6ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Average Score for the Video: 3.49\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Kogelstoten.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Kogelstoten.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Kogelstoten/segment_000265.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.5470 - mae: 2.2111 - val_loss: 7.8280 - val_mae: 2.5890\n",
      "Epoch 2/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5873 - mae: 1.0306 - val_loss: 6.1945 - val_mae: 2.2785\n",
      "Epoch 3/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3856 - mae: 0.9501 - val_loss: 4.8441 - val_mae: 2.0252\n",
      "Epoch 4/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2058 - mae: 0.8803 - val_loss: 3.5392 - val_mae: 1.7292\n",
      "Epoch 5/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9864 - mae: 0.7907 - val_loss: 2.9282 - val_mae: 1.5856\n",
      "Epoch 6/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8844 - mae: 0.7533 - val_loss: 2.3039 - val_mae: 1.4008\n",
      "Epoch 7/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8963 - mae: 0.7413 - val_loss: 1.7285 - val_mae: 1.2115\n",
      "Epoch 8/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8331 - mae: 0.7128 - val_loss: 1.7492 - val_mae: 1.2050\n",
      "Epoch 9/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8734 - mae: 0.7381 - val_loss: 1.1147 - val_mae: 0.9265\n",
      "Epoch 10/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8038 - mae: 0.6914 - val_loss: 0.8282 - val_mae: 0.7798\n",
      "Epoch 11/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7862 - mae: 0.6865 - val_loss: 0.7330 - val_mae: 0.7230\n",
      "Epoch 12/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7323 - mae: 0.6578 - val_loss: 0.7001 - val_mae: 0.7177\n",
      "Epoch 13/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7267 - mae: 0.6696 - val_loss: 0.6040 - val_mae: 0.6258\n",
      "Epoch 14/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7005 - mae: 0.6528 - val_loss: 0.6486 - val_mae: 0.6708\n",
      "Epoch 15/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6567 - mae: 0.6282 - val_loss: 0.5441 - val_mae: 0.5187\n",
      "Epoch 16/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6833 - mae: 0.6362 - val_loss: 0.5253 - val_mae: 0.5407\n",
      "Epoch 17/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6612 - mae: 0.6253 - val_loss: 0.4040 - val_mae: 0.4680\n",
      "Epoch 18/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6031 - mae: 0.5938 - val_loss: 0.8836 - val_mae: 0.7446\n",
      "Epoch 19/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6439 - mae: 0.6161 - val_loss: 0.6032 - val_mae: 0.5487\n",
      "Epoch 20/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7105 - mae: 0.6330 - val_loss: 0.4425 - val_mae: 0.4885\n",
      "Epoch 21/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5703 - mae: 0.5847 - val_loss: 0.5563 - val_mae: 0.5576\n",
      "Epoch 22/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5875 - mae: 0.5878 - val_loss: 0.4788 - val_mae: 0.4938\n",
      "Epoch 23/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5425 - mae: 0.5645 - val_loss: 0.3742 - val_mae: 0.4648\n",
      "Epoch 24/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5369 - mae: 0.5677 - val_loss: 0.3985 - val_mae: 0.4720\n",
      "Epoch 25/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5584 - mae: 0.5656 - val_loss: 0.3935 - val_mae: 0.4309\n",
      "Epoch 26/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6268 - mae: 0.5892 - val_loss: 0.4240 - val_mae: 0.4801\n",
      "Epoch 27/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5170 - mae: 0.5371 - val_loss: 0.4126 - val_mae: 0.4606\n",
      "Epoch 28/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4819 - mae: 0.5288 - val_loss: 0.3421 - val_mae: 0.4246\n",
      "Epoch 29/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5501 - mae: 0.5534 - val_loss: 0.3964 - val_mae: 0.4661\n",
      "Epoch 30/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5662 - mae: 0.5585 - val_loss: 0.3608 - val_mae: 0.4126\n",
      "Epoch 31/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4980 - mae: 0.5167 - val_loss: 0.3506 - val_mae: 0.4536\n",
      "Epoch 32/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4730 - mae: 0.5174 - val_loss: 0.3576 - val_mae: 0.4218\n",
      "Epoch 33/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4995 - mae: 0.5163 - val_loss: 0.3536 - val_mae: 0.4144\n",
      "Epoch 34/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4804 - mae: 0.5281 - val_loss: 0.3827 - val_mae: 0.4426\n",
      "Epoch 35/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4904 - mae: 0.5206 - val_loss: 0.3121 - val_mae: 0.3797\n",
      "Epoch 36/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4850 - mae: 0.4974 - val_loss: 0.3227 - val_mae: 0.4036\n",
      "Epoch 37/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4751 - mae: 0.5022 - val_loss: 0.4633 - val_mae: 0.4578\n",
      "Epoch 38/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4630 - mae: 0.4854 - val_loss: 0.3643 - val_mae: 0.4364\n",
      "Epoch 39/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5008 - mae: 0.5126 - val_loss: 0.3075 - val_mae: 0.3830\n",
      "Epoch 40/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4306 - mae: 0.4739 - val_loss: 0.4164 - val_mae: 0.4505\n",
      "Epoch 41/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4487 - mae: 0.4846 - val_loss: 0.3618 - val_mae: 0.4236\n",
      "Epoch 42/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4224 - mae: 0.4655 - val_loss: 0.2925 - val_mae: 0.3850\n",
      "Epoch 43/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3869 - mae: 0.4570 - val_loss: 0.3744 - val_mae: 0.4266\n",
      "Epoch 44/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3740 - mae: 0.4308 - val_loss: 0.3074 - val_mae: 0.4239\n",
      "Epoch 45/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4369 - mae: 0.4633 - val_loss: 0.3638 - val_mae: 0.4436\n",
      "Epoch 46/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4235 - mae: 0.4587 - val_loss: 0.2905 - val_mae: 0.3824\n",
      "Epoch 47/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4306 - mae: 0.4564 - val_loss: 0.4959 - val_mae: 0.4770\n",
      "Epoch 48/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4287 - mae: 0.4697 - val_loss: 0.4820 - val_mae: 0.4489\n",
      "Epoch 49/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4365 - mae: 0.4685 - val_loss: 0.3745 - val_mae: 0.4262\n",
      "Epoch 50/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3815 - mae: 0.4502 - val_loss: 0.5489 - val_mae: 0.5363\n",
      "Epoch 51/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4190 - mae: 0.4613 - val_loss: 0.3746 - val_mae: 0.4085\n",
      "Epoch 52/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4010 - mae: 0.4407 - val_loss: 0.3328 - val_mae: 0.4095\n",
      "Epoch 53/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3526 - mae: 0.4231 - val_loss: 0.3089 - val_mae: 0.3842\n",
      "Epoch 54/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4240 - mae: 0.4456 - val_loss: 0.2824 - val_mae: 0.3645\n",
      "Epoch 55/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4157 - mae: 0.4494 - val_loss: 0.5132 - val_mae: 0.4653\n",
      "Epoch 56/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3853 - mae: 0.4383 - val_loss: 0.3187 - val_mae: 0.4097\n",
      "Epoch 57/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3809 - mae: 0.4364 - val_loss: 0.4282 - val_mae: 0.4495\n",
      "Epoch 58/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3994 - mae: 0.4398 - val_loss: 0.4472 - val_mae: 0.4372\n",
      "Epoch 59/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3752 - mae: 0.4308 - val_loss: 0.3645 - val_mae: 0.4047\n",
      "Epoch 60/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3964 - mae: 0.4358 - val_loss: 0.4852 - val_mae: 0.4936\n",
      "Epoch 61/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3930 - mae: 0.4356 - val_loss: 0.5180 - val_mae: 0.4474\n",
      "Epoch 62/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3432 - mae: 0.4096 - val_loss: 0.2749 - val_mae: 0.3569\n",
      "Epoch 63/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3856 - mae: 0.4196 - val_loss: 0.2888 - val_mae: 0.3619\n",
      "Epoch 64/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3529 - mae: 0.4176 - val_loss: 0.3633 - val_mae: 0.3903\n",
      "Epoch 65/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4094 - mae: 0.4454 - val_loss: 0.2625 - val_mae: 0.3386\n",
      "Epoch 66/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2803 - mae: 0.3699 - val_loss: 0.2310 - val_mae: 0.3351\n",
      "Epoch 67/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3646 - mae: 0.4110 - val_loss: 0.3514 - val_mae: 0.3956\n",
      "Epoch 68/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3374 - mae: 0.4017 - val_loss: 0.3965 - val_mae: 0.4307\n",
      "Epoch 69/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3432 - mae: 0.4094 - val_loss: 0.2828 - val_mae: 0.3633\n",
      "Epoch 70/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3488 - mae: 0.4109 - val_loss: 0.2800 - val_mae: 0.3581\n",
      "Epoch 71/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3258 - mae: 0.3837 - val_loss: 0.3265 - val_mae: 0.3637\n",
      "Epoch 72/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3053 - mae: 0.3777 - val_loss: 0.2743 - val_mae: 0.3640\n",
      "Epoch 73/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3181 - mae: 0.3919 - val_loss: 0.2885 - val_mae: 0.3349\n",
      "Epoch 74/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3143 - mae: 0.3817 - val_loss: 0.3328 - val_mae: 0.3486\n",
      "Epoch 75/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3308 - mae: 0.3771 - val_loss: 0.3982 - val_mae: 0.3993\n",
      "Epoch 76/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3190 - mae: 0.3794 - val_loss: 0.2561 - val_mae: 0.3353\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2423 - mae: 0.3272\n",
      "Test Loss: 0.2561, Test MAE: 0.3353\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.7772\n",
      "\n",
      "0: 384x640 2 persons, 54.0ms\n",
      "Speed: 0.9ms preprocess, 54.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 55.5ms\n",
      "Speed: 1.0ms preprocess, 55.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 64.8ms\n",
      "Speed: 1.4ms preprocess, 64.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 49.8ms\n",
      "Speed: 0.8ms preprocess, 49.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.6ms\n",
      "Speed: 1.0ms preprocess, 54.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.6ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.8ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.8ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.9ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.3ms\n",
      "Speed: 0.8ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 54.6ms\n",
      "Speed: 0.7ms preprocess, 54.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 110.9ms\n",
      "Speed: 2.7ms preprocess, 110.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.0ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.8ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.8ms\n",
      "Speed: 0.7ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.2ms\n",
      "Speed: 0.8ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.9ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.7ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.8ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.7ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 1.2ms preprocess, 38.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.7ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.7ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 56.1ms\n",
      "Speed: 0.8ms preprocess, 56.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\n",
      "0: 384x640 1 person, 119.6ms\n",
      "Speed: 3.2ms preprocess, 119.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.0ms\n",
      "Speed: 1.1ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.7ms\n",
      "Speed: 0.8ms preprocess, 48.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.2ms\n",
      "Speed: 1.0ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.8ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.9ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.8ms\n",
      "Speed: 0.9ms preprocess, 47.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.9ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.8ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.7ms preprocess, 41.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.6ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 1.3ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 1.2ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.8ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.8ms\n",
      "Speed: 0.8ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.8ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.8ms preprocess, 35.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 2.5ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.7ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.7ms\n",
      "Speed: 1.6ms preprocess, 74.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 1.1ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.9ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.7ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.6ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.6ms\n",
      "Speed: 12.0ms preprocess, 78.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.9ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 1.1ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.8ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.7ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.8ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.9ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.8ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 93.1ms\n",
      "Speed: 1.7ms preprocess, 93.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.8ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 1.4ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 2.1ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.9ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.9ms\n",
      "Speed: 0.7ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.8ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.8ms preprocess, 36.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 0.8ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.7ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.1ms\n",
      "Speed: 3.8ms preprocess, 79.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.8ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.6ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 0.8ms preprocess, 46.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.7ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.8ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.9ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.8ms preprocess, 45.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.8ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.9ms\n",
      "Speed: 2.9ms preprocess, 79.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.7ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 0.8ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.9ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.7ms\n",
      "Speed: 0.7ms preprocess, 34.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.6ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.9ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 0.9ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 1.4ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 9.7ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.8ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.8ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.7ms\n",
      "Speed: 1.2ms preprocess, 45.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.4ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.8ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 2.5ms preprocess, 80.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.7ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.7ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.8ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.8ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.6ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.9ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.9ms\n",
      "Speed: 3.0ms preprocess, 77.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.8ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.8ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.8ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.8ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.2ms\n",
      "Speed: 0.8ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.0ms\n",
      "Speed: 2.9ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.7ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.8ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.4ms\n",
      "Speed: 0.7ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.8ms preprocess, 39.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.7ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.4ms\n",
      "Speed: 0.8ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.6ms\n",
      "Speed: 0.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.1ms\n",
      "Speed: 2.9ms preprocess, 68.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.1ms\n",
      "Speed: 0.7ms preprocess, 35.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.8ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.4ms\n",
      "Speed: 0.7ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.8ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.6ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.8ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.7ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.2ms\n",
      "Speed: 0.9ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.8ms\n",
      "Speed: 0.7ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.7ms\n",
      "Speed: 2.4ms preprocess, 78.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 1.1ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 50.1ms\n",
      "Speed: 0.8ms preprocess, 50.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.9ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.9ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.8ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 2.2ms preprocess, 73.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 1.3ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 1.2ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.8ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.7ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.6ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 69.0ms\n",
      "Speed: 2.0ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.5ms\n",
      "Speed: 1.6ms preprocess, 73.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.8ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.4ms\n",
      "Speed: 0.7ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.6ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.8ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.8ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 1.5ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.9ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.9ms\n",
      "Speed: 0.7ms preprocess, 49.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.8ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.1ms\n",
      "Speed: 7.5ms preprocess, 71.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.6ms\n",
      "Speed: 0.8ms preprocess, 47.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.8ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.5ms\n",
      "Speed: 0.8ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.1ms\n",
      "Speed: 0.7ms preprocess, 45.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.8ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.3ms\n",
      "Speed: 0.8ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 52.9ms\n",
      "Speed: 1.4ms preprocess, 52.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.9ms\n",
      "Speed: 0.9ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.6ms\n",
      "Speed: 2.1ms preprocess, 81.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.7ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.8ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.8ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 0.9ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.8ms preprocess, 41.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.7ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.8ms\n",
      "Speed: 1.9ms preprocess, 74.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.2ms\n",
      "Speed: 0.8ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.7ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 2 persons, 36.8ms\n",
      "Speed: 0.6ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 0.9ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 42.7ms\n",
      "Speed: 0.9ms preprocess, 42.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 45.8ms\n",
      "Speed: 0.8ms preprocess, 45.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 68.3ms\n",
      "Speed: 1.1ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 0.7ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 0.8ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.8ms\n",
      "Speed: 0.7ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.2ms\n",
      "Speed: 2.2ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.2ms\n",
      "Speed: 1.2ms preprocess, 35.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 0.6ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.7ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.9ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.9ms\n",
      "Speed: 0.8ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 1.4ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.2ms\n",
      "Speed: 2.2ms preprocess, 71.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.6ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.6ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.7ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 1.2ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.5ms\n",
      "Speed: 0.8ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.6ms\n",
      "Speed: 2.0ms preprocess, 76.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.9ms\n",
      "Speed: 1.3ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.0ms\n",
      "Speed: 0.7ms preprocess, 38.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.7ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 1.1ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 1.5ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.6ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.5ms\n",
      "Speed: 0.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 1.2ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.9ms\n",
      "Speed: 1.3ms preprocess, 39.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 2 persons, 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.8ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 78.2ms\n",
      "Speed: 4.2ms preprocess, 78.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 1.5ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 0.8ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 1.5ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.7ms\n",
      "Speed: 0.8ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.8ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "0: 384x640 1 person, 58.2ms\n",
      "Speed: 1.0ms preprocess, 58.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 79.7ms\n",
      "Speed: 2.5ms preprocess, 79.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.0ms\n",
      "Speed: 0.9ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.8ms preprocess, 43.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 1.5ms preprocess, 42.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.1ms\n",
      "Speed: 0.7ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.6ms\n",
      "Speed: 0.6ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.8ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.4ms\n",
      "Speed: 4.4ms preprocess, 67.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 1.2ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.8ms\n",
      "Speed: 0.7ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.8ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 0.8ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 67.0ms\n",
      "Speed: 2.7ms preprocess, 67.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.7ms preprocess, 35.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.9ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.8ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.6ms\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.8ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.5ms\n",
      "Speed: 0.7ms preprocess, 45.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 1.0ms preprocess, 44.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.3ms\n",
      "Speed: 2.0ms preprocess, 74.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.8ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.3ms\n",
      "Speed: 0.7ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.7ms\n",
      "Speed: 0.7ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 71.7ms\n",
      "Speed: 1.1ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.8ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.2ms\n",
      "Speed: 0.7ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 55.9ms\n",
      "Speed: 0.7ms preprocess, 55.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.8ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.8ms\n",
      "Speed: 1.3ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "0: 384x640 1 person, 48.8ms\n",
      "Speed: 0.9ms preprocess, 48.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.7ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.8ms\n",
      "Speed: 0.6ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.0ms\n",
      "Speed: 0.8ms preprocess, 37.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.5ms\n",
      "Speed: 0.7ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.8ms\n",
      "Speed: 1.5ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.8ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.7ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.5ms\n",
      "Speed: 0.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.6ms\n",
      "Speed: 0.8ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.7ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 73.1ms\n",
      "Speed: 1.6ms preprocess, 73.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.1ms\n",
      "Speed: 0.8ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.8ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.3ms\n",
      "Speed: 2.4ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.9ms\n",
      "Speed: 1.1ms preprocess, 35.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.1ms\n",
      "Speed: 0.7ms preprocess, 36.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.2ms\n",
      "Speed: 0.8ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.4ms\n",
      "Speed: 0.7ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 81.0ms\n",
      "Speed: 1.4ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.2ms\n",
      "Speed: 0.7ms preprocess, 49.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.8ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.9ms\n",
      "Speed: 0.7ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.0ms\n",
      "Speed: 1.2ms preprocess, 35.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.8ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 70.0ms\n",
      "Speed: 1.7ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.8ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.0ms\n",
      "Speed: 0.8ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.8ms\n",
      "Speed: 0.8ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.0ms\n",
      "Speed: 0.7ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.7ms\n",
      "Speed: 1.3ms preprocess, 36.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.0ms\n",
      "Speed: 0.8ms preprocess, 43.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 74.4ms\n",
      "Speed: 3.0ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.2ms\n",
      "Speed: 0.7ms preprocess, 43.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.1ms\n",
      "Speed: 0.7ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.8ms\n",
      "Speed: 0.8ms preprocess, 44.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.0ms\n",
      "Speed: 0.8ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.4ms\n",
      "Speed: 1.8ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 46.1ms\n",
      "Speed: 1.3ms preprocess, 46.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 384x640 1 person, 57.9ms\n",
      "Speed: 1.2ms preprocess, 57.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.1ms\n",
      "Speed: 0.8ms preprocess, 41.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.0ms\n",
      "Speed: 0.7ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 1.2ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.3ms\n",
      "Speed: 0.7ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.5ms\n",
      "Speed: 1.1ms preprocess, 34.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.1ms\n",
      "Speed: 0.8ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 76.7ms\n",
      "Speed: 1.5ms preprocess, 76.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.0ms\n",
      "Speed: 0.7ms preprocess, 36.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 34.6ms\n",
      "Speed: 0.7ms preprocess, 34.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 0.8ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.7ms\n",
      "Speed: 0.7ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.4ms\n",
      "Speed: 0.8ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.6ms\n",
      "Speed: 0.7ms preprocess, 36.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 90.6ms\n",
      "Speed: 4.1ms preprocess, 90.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.6ms\n",
      "Speed: 0.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 47.7ms\n",
      "Speed: 0.8ms preprocess, 47.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.2ms\n",
      "Speed: 0.7ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 41.4ms\n",
      "Speed: 0.9ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.6ms\n",
      "Speed: 1.1ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.9ms\n",
      "Speed: 0.8ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 43.3ms\n",
      "Speed: 0.7ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.9ms\n",
      "Speed: 1.1ms preprocess, 75.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.2ms\n",
      "Speed: 0.8ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.6ms\n",
      "Speed: 1.2ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.5ms\n",
      "Speed: 0.7ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.8ms\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.3ms\n",
      "Speed: 0.8ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 1.7ms preprocess, 77.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.5ms\n",
      "Speed: 0.7ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.1ms\n",
      "Speed: 0.7ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 384x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 45.6ms\n",
      "Speed: 0.7ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 75.0ms\n",
      "Speed: 1.1ms preprocess, 75.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.6ms\n",
      "Speed: 0.8ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 44.3ms\n",
      "Speed: 0.7ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 36.3ms\n",
      "Speed: 0.8ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 35.4ms\n",
      "Speed: 0.8ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 384x640 1 person, 40.6ms\n",
      "Speed: 0.7ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 38.5ms\n",
      "Speed: 0.6ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 42.5ms\n",
      "Speed: 0.7ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 384x640 1 person, 59.1ms\n",
      "Speed: 1.3ms preprocess, 59.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 72.5ms\n",
      "Speed: 2.5ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Average Score for the Video: 3.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Speerwerpen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Speerwerpen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Speerwerpen/segment_000734.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 10.0793 - mae: 2.8154 - val_loss: 10.5913 - val_mae: 3.1897\n",
      "Epoch 2/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1250 - mae: 0.8297 - val_loss: 7.6764 - val_mae: 2.6883\n",
      "Epoch 3/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9721 - mae: 0.7676 - val_loss: 4.7291 - val_mae: 2.0643\n",
      "Epoch 4/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8672 - mae: 0.7441 - val_loss: 2.9651 - val_mae: 1.5753\n",
      "Epoch 5/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6563 - mae: 0.6421 - val_loss: 1.6940 - val_mae: 1.1279\n",
      "Epoch 6/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5828 - mae: 0.6061 - val_loss: 0.9799 - val_mae: 0.8616\n",
      "Epoch 7/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5771 - mae: 0.5979 - val_loss: 0.6284 - val_mae: 0.6408\n",
      "Epoch 8/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4699 - mae: 0.5417 - val_loss: 0.4440 - val_mae: 0.5441\n",
      "Epoch 9/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4958 - mae: 0.5592 - val_loss: 0.3186 - val_mae: 0.4441\n",
      "Epoch 10/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4598 - mae: 0.5400 - val_loss: 0.3032 - val_mae: 0.4326\n",
      "Epoch 11/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4144 - mae: 0.5062 - val_loss: 0.2648 - val_mae: 0.3999\n",
      "Epoch 12/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4048 - mae: 0.5071 - val_loss: 0.2564 - val_mae: 0.3827\n",
      "Epoch 13/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4018 - mae: 0.4991 - val_loss: 0.2625 - val_mae: 0.3857\n",
      "Epoch 14/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3798 - mae: 0.4814 - val_loss: 0.2384 - val_mae: 0.3521\n",
      "Epoch 15/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3747 - mae: 0.4734 - val_loss: 0.2494 - val_mae: 0.3820\n",
      "Epoch 16/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3552 - mae: 0.4634 - val_loss: 0.2738 - val_mae: 0.3861\n",
      "Epoch 17/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3511 - mae: 0.4612 - val_loss: 0.2442 - val_mae: 0.3645\n",
      "Epoch 18/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3461 - mae: 0.4626 - val_loss: 0.2378 - val_mae: 0.3604\n",
      "Epoch 19/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3334 - mae: 0.4492 - val_loss: 0.2350 - val_mae: 0.3386\n",
      "Epoch 20/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3081 - mae: 0.4323 - val_loss: 0.2300 - val_mae: 0.3310\n",
      "Epoch 21/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3039 - mae: 0.4295 - val_loss: 0.2514 - val_mae: 0.3733\n",
      "Epoch 22/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3107 - mae: 0.4344 - val_loss: 0.2304 - val_mae: 0.3525\n",
      "Epoch 23/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2885 - mae: 0.4203 - val_loss: 0.2330 - val_mae: 0.3520\n",
      "Epoch 24/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2820 - mae: 0.4184 - val_loss: 0.2176 - val_mae: 0.3240\n",
      "Epoch 25/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2709 - mae: 0.3999 - val_loss: 0.2289 - val_mae: 0.3421\n",
      "Epoch 26/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2631 - mae: 0.4026 - val_loss: 0.2858 - val_mae: 0.4057\n",
      "Epoch 27/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2640 - mae: 0.4018 - val_loss: 0.2297 - val_mae: 0.3367\n",
      "Epoch 28/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2556 - mae: 0.3887 - val_loss: 0.2145 - val_mae: 0.3284\n",
      "Epoch 29/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2634 - mae: 0.4058 - val_loss: 0.2130 - val_mae: 0.3153\n",
      "Epoch 30/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2534 - mae: 0.3894 - val_loss: 0.2261 - val_mae: 0.3497\n",
      "Epoch 31/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2577 - mae: 0.3858 - val_loss: 0.2443 - val_mae: 0.3387\n",
      "Epoch 32/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2309 - mae: 0.3625 - val_loss: 0.2560 - val_mae: 0.3540\n",
      "Epoch 33/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2385 - mae: 0.3794 - val_loss: 0.3013 - val_mae: 0.4045\n",
      "Epoch 34/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2428 - mae: 0.3794 - val_loss: 0.2522 - val_mae: 0.3622\n",
      "Epoch 35/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2283 - mae: 0.3654 - val_loss: 0.2395 - val_mae: 0.3590\n",
      "Epoch 36/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2158 - mae: 0.3605 - val_loss: 0.2430 - val_mae: 0.3584\n",
      "Epoch 37/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2156 - mae: 0.3529 - val_loss: 0.2061 - val_mae: 0.3046\n",
      "Epoch 38/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2091 - mae: 0.3414 - val_loss: 0.2059 - val_mae: 0.3038\n",
      "Epoch 39/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2345 - mae: 0.3696 - val_loss: 0.2233 - val_mae: 0.3001\n",
      "Epoch 40/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2009 - mae: 0.3373 - val_loss: 0.2082 - val_mae: 0.3099\n",
      "Epoch 41/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2194 - mae: 0.3529 - val_loss: 0.1920 - val_mae: 0.2957\n",
      "Epoch 42/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2048 - mae: 0.3393 - val_loss: 0.2314 - val_mae: 0.3636\n",
      "Epoch 43/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2061 - mae: 0.3428 - val_loss: 0.2058 - val_mae: 0.3023\n",
      "Epoch 44/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1839 - mae: 0.3241 - val_loss: 0.2132 - val_mae: 0.2882\n",
      "Epoch 45/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1756 - mae: 0.3105 - val_loss: 0.2046 - val_mae: 0.2972\n",
      "Epoch 46/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1815 - mae: 0.3206 - val_loss: 0.2457 - val_mae: 0.3687\n",
      "Epoch 47/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1927 - mae: 0.3247 - val_loss: 0.2504 - val_mae: 0.3617\n",
      "Epoch 48/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1869 - mae: 0.3218 - val_loss: 0.1966 - val_mae: 0.2928\n",
      "Epoch 49/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1771 - mae: 0.3125 - val_loss: 0.2180 - val_mae: 0.3260\n",
      "Epoch 50/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1820 - mae: 0.3154 - val_loss: 0.2148 - val_mae: 0.3245\n",
      "Epoch 51/100\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1868 - mae: 0.3213 - val_loss: 0.2238 - val_mae: 0.3126\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.2398 - mae: 0.3184\n",
      "Test Loss: 0.2238, Test MAE: 0.3126\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5205\n",
      "\n",
      "0: 640x640 1 person, 86.3ms\n",
      "Speed: 3.1ms preprocess, 86.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.9ms\n",
      "Speed: 2.1ms preprocess, 76.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 1.8ms preprocess, 68.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.8ms\n",
      "Speed: 1.4ms preprocess, 58.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.3ms\n",
      "Speed: 1.3ms preprocess, 60.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.7ms\n",
      "Speed: 1.3ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.1ms\n",
      "Speed: 1.6ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.7ms\n",
      "Speed: 1.4ms preprocess, 62.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.3ms\n",
      "Speed: 1.4ms preprocess, 60.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.4ms\n",
      "Speed: 1.9ms preprocess, 73.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.8ms\n",
      "Speed: 1.5ms preprocess, 64.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.3ms\n",
      "Speed: 1.5ms preprocess, 61.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.6ms preprocess, 71.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.8ms\n",
      "Speed: 1.4ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.3ms\n",
      "Speed: 1.4ms preprocess, 62.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.0ms\n",
      "Speed: 1.4ms preprocess, 61.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.6ms\n",
      "Speed: 1.4ms preprocess, 77.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.2ms\n",
      "Speed: 1.4ms preprocess, 77.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "0: 640x640 1 person, 78.6ms\n",
      "Speed: 5.0ms preprocess, 78.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.3ms\n",
      "Speed: 1.4ms preprocess, 67.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.5ms\n",
      "Speed: 1.4ms preprocess, 72.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.3ms\n",
      "Speed: 1.4ms preprocess, 65.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 139.5ms\n",
      "Speed: 2.6ms preprocess, 139.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.6ms\n",
      "Speed: 1.7ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.5ms\n",
      "Speed: 1.6ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.5ms\n",
      "Speed: 1.5ms preprocess, 63.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.2ms\n",
      "Speed: 1.4ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.1ms\n",
      "Speed: 1.4ms preprocess, 58.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.5ms\n",
      "Speed: 1.4ms preprocess, 64.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.7ms\n",
      "Speed: 1.4ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.3ms\n",
      "Speed: 1.4ms preprocess, 58.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.4ms\n",
      "Speed: 1.4ms preprocess, 59.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.0ms\n",
      "Speed: 1.4ms preprocess, 59.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.5ms\n",
      "Speed: 1.4ms preprocess, 58.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.9ms\n",
      "Speed: 1.4ms preprocess, 58.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.3ms\n",
      "Speed: 1.7ms preprocess, 101.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.9ms\n",
      "Speed: 1.8ms preprocess, 95.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 79.7ms\n",
      "Speed: 1.9ms preprocess, 79.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.5ms\n",
      "Speed: 1.5ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 82.4ms\n",
      "Speed: 1.5ms preprocess, 82.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.8ms\n",
      "Speed: 1.7ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.4ms\n",
      "Speed: 1.9ms preprocess, 76.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.4ms\n",
      "Speed: 1.4ms preprocess, 73.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 146.3ms\n",
      "Speed: 1.7ms preprocess, 146.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.4ms\n",
      "Speed: 2.0ms preprocess, 102.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "0: 640x640 1 person, 84.9ms\n",
      "Speed: 2.3ms preprocess, 84.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 88.1ms\n",
      "Speed: 1.6ms preprocess, 88.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 148.8ms\n",
      "Speed: 1.8ms preprocess, 148.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.9ms\n",
      "Speed: 1.5ms preprocess, 113.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.5ms\n",
      "Speed: 1.7ms preprocess, 76.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.7ms\n",
      "Speed: 1.6ms preprocess, 74.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.9ms\n",
      "Speed: 1.4ms preprocess, 68.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.9ms\n",
      "Speed: 1.5ms preprocess, 65.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.5ms\n",
      "Speed: 1.9ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.5ms\n",
      "Speed: 1.7ms preprocess, 75.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.2ms\n",
      "Speed: 1.4ms preprocess, 68.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.6ms\n",
      "Speed: 1.4ms preprocess, 64.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.6ms\n",
      "Speed: 1.4ms preprocess, 64.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.8ms\n",
      "Speed: 1.4ms preprocess, 61.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.6ms\n",
      "Speed: 1.4ms preprocess, 63.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.1ms\n",
      "Speed: 1.4ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.2ms\n",
      "Speed: 1.3ms preprocess, 60.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.8ms\n",
      "Speed: 1.8ms preprocess, 68.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.8ms\n",
      "Speed: 1.5ms preprocess, 62.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.2ms\n",
      "Speed: 1.5ms preprocess, 69.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.2ms\n",
      "Speed: 1.6ms preprocess, 73.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.2ms\n",
      "Speed: 1.4ms preprocess, 77.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 138.1ms\n",
      "Speed: 1.4ms preprocess, 138.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.2ms\n",
      "Speed: 1.5ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.0ms\n",
      "Speed: 1.4ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.6ms\n",
      "Speed: 1.4ms preprocess, 66.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 119.6ms\n",
      "Speed: 3.5ms preprocess, 119.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.8ms\n",
      "Speed: 1.4ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.3ms\n",
      "Speed: 1.9ms preprocess, 63.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.7ms\n",
      "Speed: 1.3ms preprocess, 60.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.8ms\n",
      "Speed: 1.5ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 58.0ms\n",
      "Speed: 1.4ms preprocess, 58.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.4ms\n",
      "Speed: 1.4ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.1ms\n",
      "Speed: 1.4ms preprocess, 60.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.8ms\n",
      "Speed: 1.4ms preprocess, 60.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.9ms\n",
      "Speed: 1.5ms preprocess, 65.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.0ms\n",
      "Speed: 1.4ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.3ms\n",
      "Speed: 1.4ms preprocess, 61.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.4ms\n",
      "Speed: 1.4ms preprocess, 61.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.1ms\n",
      "Speed: 1.4ms preprocess, 67.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.8ms\n",
      "Speed: 1.5ms preprocess, 66.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 2 persons, 72.6ms\n",
      "Speed: 1.5ms preprocess, 72.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.1ms\n",
      "Speed: 1.3ms preprocess, 60.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.5ms\n",
      "Speed: 1.5ms preprocess, 64.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.5ms\n",
      "Speed: 1.6ms preprocess, 68.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.5ms\n",
      "Speed: 1.4ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.4ms\n",
      "Speed: 1.3ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.7ms\n",
      "Speed: 1.5ms preprocess, 59.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.9ms\n",
      "Speed: 2.0ms preprocess, 66.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 1.4ms preprocess, 72.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.4ms\n",
      "Speed: 1.4ms preprocess, 70.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.1ms\n",
      "Speed: 1.5ms preprocess, 74.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.5ms\n",
      "Speed: 1.5ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.6ms\n",
      "Speed: 1.4ms preprocess, 66.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.1ms\n",
      "Speed: 1.4ms preprocess, 65.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.7ms\n",
      "Speed: 1.4ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.2ms\n",
      "Speed: 1.4ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.4ms\n",
      "Speed: 1.4ms preprocess, 70.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.8ms\n",
      "Speed: 1.6ms preprocess, 67.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.6ms\n",
      "Speed: 1.8ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.6ms\n",
      "Speed: 1.4ms preprocess, 66.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.5ms\n",
      "Speed: 1.5ms preprocess, 68.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 93.0ms\n",
      "Speed: 2.3ms preprocess, 93.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.9ms\n",
      "Speed: 1.4ms preprocess, 63.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.1ms\n",
      "Speed: 1.5ms preprocess, 67.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.9ms\n",
      "Speed: 1.5ms preprocess, 70.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.3ms\n",
      "Speed: 1.5ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.4ms\n",
      "Speed: 1.4ms preprocess, 70.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.9ms\n",
      "Speed: 1.4ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.4ms\n",
      "Speed: 1.4ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.6ms\n",
      "Speed: 1.5ms preprocess, 68.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.9ms\n",
      "Speed: 1.5ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.6ms\n",
      "Speed: 1.4ms preprocess, 60.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.5ms\n",
      "Speed: 1.8ms preprocess, 61.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.4ms\n",
      "Speed: 1.5ms preprocess, 63.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.9ms\n",
      "Speed: 1.4ms preprocess, 64.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.3ms\n",
      "Speed: 1.4ms preprocess, 62.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.6ms\n",
      "Speed: 1.4ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.9ms\n",
      "Speed: 1.3ms preprocess, 62.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.5ms preprocess, 71.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.9ms\n",
      "Speed: 1.5ms preprocess, 65.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.5ms\n",
      "Speed: 1.7ms preprocess, 66.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.4ms\n",
      "Speed: 1.4ms preprocess, 63.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.1ms\n",
      "Speed: 1.5ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.9ms\n",
      "Speed: 1.4ms preprocess, 64.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.7ms\n",
      "Speed: 1.5ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.2ms\n",
      "Speed: 2.0ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.4ms\n",
      "Speed: 1.4ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.7ms\n",
      "Speed: 1.4ms preprocess, 67.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.3ms\n",
      "Speed: 3.7ms preprocess, 99.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.6ms\n",
      "Speed: 1.5ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.7ms\n",
      "Speed: 1.4ms preprocess, 62.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.0ms\n",
      "Speed: 1.4ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.4ms\n",
      "Speed: 1.4ms preprocess, 62.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.9ms\n",
      "Speed: 1.4ms preprocess, 61.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.9ms\n",
      "Speed: 1.5ms preprocess, 66.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.5ms\n",
      "Speed: 1.4ms preprocess, 62.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 2 persons, 67.5ms\n",
      "Speed: 1.4ms preprocess, 67.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.6ms\n",
      "Speed: 1.4ms preprocess, 66.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.6ms\n",
      "Speed: 1.3ms preprocess, 61.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.6ms\n",
      "Speed: 1.4ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.8ms\n",
      "Speed: 1.5ms preprocess, 69.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.4ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.3ms\n",
      "Speed: 1.4ms preprocess, 73.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 2 persons, 69.7ms\n",
      "Speed: 1.4ms preprocess, 69.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.8ms\n",
      "Speed: 1.6ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.8ms\n",
      "Speed: 1.6ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 2 persons, 69.8ms\n",
      "Speed: 1.5ms preprocess, 69.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.5ms\n",
      "Speed: 1.4ms preprocess, 72.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.5ms\n",
      "Speed: 1.4ms preprocess, 64.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.8ms\n",
      "Speed: 1.4ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.6ms\n",
      "Speed: 1.4ms preprocess, 61.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.9ms\n",
      "Speed: 1.4ms preprocess, 61.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.9ms\n",
      "Speed: 1.4ms preprocess, 74.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.1ms\n",
      "Speed: 1.4ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.0ms\n",
      "Speed: 1.5ms preprocess, 71.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 96.0ms\n",
      "Speed: 11.9ms preprocess, 96.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.1ms\n",
      "Speed: 1.5ms preprocess, 69.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 3.4ms preprocess, 69.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.8ms\n",
      "Speed: 1.5ms preprocess, 73.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.6ms\n",
      "Speed: 1.4ms preprocess, 65.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.8ms\n",
      "Speed: 1.4ms preprocess, 59.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.2ms\n",
      "Speed: 1.4ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.8ms\n",
      "Speed: 1.5ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.6ms\n",
      "Speed: 1.3ms preprocess, 59.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.2ms\n",
      "Speed: 1.4ms preprocess, 59.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.6ms\n",
      "Speed: 1.3ms preprocess, 62.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.0ms\n",
      "Speed: 1.4ms preprocess, 62.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.0ms\n",
      "Speed: 1.4ms preprocess, 62.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.3ms\n",
      "Speed: 1.4ms preprocess, 70.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.2ms\n",
      "Speed: 1.5ms preprocess, 75.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.5ms\n",
      "Speed: 1.5ms preprocess, 76.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.5ms\n",
      "Speed: 1.6ms preprocess, 74.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.6ms\n",
      "Speed: 1.5ms preprocess, 72.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.4ms\n",
      "Speed: 1.6ms preprocess, 72.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 78.1ms\n",
      "Speed: 1.6ms preprocess, 78.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.5ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 81.0ms\n",
      "Speed: 1.4ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.0ms\n",
      "Speed: 1.4ms preprocess, 74.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 79.8ms\n",
      "Speed: 1.7ms preprocess, 79.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.6ms\n",
      "Speed: 8.7ms preprocess, 98.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.2ms\n",
      "Speed: 1.4ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.7ms\n",
      "Speed: 1.4ms preprocess, 70.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.1ms\n",
      "Speed: 1.5ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.4ms preprocess, 71.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.4ms preprocess, 71.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.6ms\n",
      "Speed: 1.4ms preprocess, 64.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.5ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.2ms\n",
      "Speed: 1.4ms preprocess, 70.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.8ms\n",
      "Speed: 2.0ms preprocess, 72.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.4ms\n",
      "Speed: 1.6ms preprocess, 66.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.0ms\n",
      "Speed: 1.6ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.8ms\n",
      "Speed: 1.5ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.6ms\n",
      "Speed: 1.4ms preprocess, 67.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 1.5ms preprocess, 72.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.8ms\n",
      "Speed: 1.5ms preprocess, 67.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.1ms\n",
      "Speed: 1.4ms preprocess, 73.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.3ms\n",
      "Speed: 1.5ms preprocess, 63.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.4ms\n",
      "Speed: 1.9ms preprocess, 62.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.9ms\n",
      "Speed: 1.5ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.8ms\n",
      "Speed: 1.4ms preprocess, 68.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.9ms\n",
      "Speed: 1.6ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 98.6ms\n",
      "Speed: 2.6ms preprocess, 98.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.7ms\n",
      "Speed: 1.4ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.5ms\n",
      "Speed: 1.5ms preprocess, 73.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.7ms\n",
      "Speed: 1.5ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.3ms\n",
      "Speed: 1.4ms preprocess, 67.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.9ms\n",
      "Speed: 1.6ms preprocess, 69.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.0ms\n",
      "Speed: 1.4ms preprocess, 63.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.3ms\n",
      "Speed: 1.4ms preprocess, 63.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.6ms preprocess, 72.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.4ms\n",
      "Speed: 1.5ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.4ms\n",
      "Speed: 1.8ms preprocess, 70.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.3ms\n",
      "Speed: 1.4ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.8ms\n",
      "Speed: 1.4ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.3ms\n",
      "Speed: 1.6ms preprocess, 73.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.3ms\n",
      "Speed: 1.5ms preprocess, 68.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.3ms\n",
      "Speed: 1.7ms preprocess, 73.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.5ms preprocess, 71.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.4ms\n",
      "Speed: 1.6ms preprocess, 74.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.5ms\n",
      "Speed: 1.4ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "0: 640x640 1 person, 94.9ms\n",
      "Speed: 2.6ms preprocess, 94.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.8ms\n",
      "Speed: 1.5ms preprocess, 67.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.0ms\n",
      "Speed: 1.4ms preprocess, 101.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.8ms\n",
      "Speed: 1.4ms preprocess, 61.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.8ms\n",
      "Speed: 1.5ms preprocess, 69.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.5ms\n",
      "Speed: 1.4ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.6ms\n",
      "Speed: 1.5ms preprocess, 64.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.5ms\n",
      "Speed: 1.5ms preprocess, 76.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.7ms\n",
      "Speed: 1.4ms preprocess, 63.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.2ms\n",
      "Speed: 1.4ms preprocess, 63.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.5ms\n",
      "Speed: 1.6ms preprocess, 73.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.6ms\n",
      "Speed: 1.6ms preprocess, 73.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.4ms\n",
      "Speed: 1.4ms preprocess, 66.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.2ms\n",
      "Speed: 1.6ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.3ms\n",
      "Speed: 1.4ms preprocess, 66.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.8ms\n",
      "Speed: 1.4ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.8ms\n",
      "Speed: 1.4ms preprocess, 62.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 1.4ms preprocess, 72.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.2ms\n",
      "Speed: 1.4ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.5ms preprocess, 72.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 1.7ms preprocess, 72.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 95.4ms\n",
      "Speed: 8.8ms preprocess, 95.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.5ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.4ms\n",
      "Speed: 1.4ms preprocess, 63.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.5ms\n",
      "Speed: 1.4ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.0ms\n",
      "Speed: 1.5ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.5ms\n",
      "Speed: 1.4ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.2ms\n",
      "Speed: 1.5ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.2ms\n",
      "Speed: 1.5ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.1ms\n",
      "Speed: 1.4ms preprocess, 73.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.4ms\n",
      "Speed: 1.4ms preprocess, 66.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.2ms\n",
      "Speed: 1.3ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.7ms\n",
      "Speed: 1.4ms preprocess, 66.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.4ms\n",
      "Speed: 1.3ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.2ms\n",
      "Speed: 1.4ms preprocess, 64.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 76.5ms\n",
      "Speed: 1.4ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.2ms\n",
      "Speed: 1.4ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.6ms\n",
      "Speed: 1.5ms preprocess, 63.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.7ms\n",
      "Speed: 1.4ms preprocess, 63.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.8ms\n",
      "Speed: 1.4ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 90.3ms\n",
      "Speed: 2.8ms preprocess, 90.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.8ms\n",
      "Speed: 1.6ms preprocess, 77.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.2ms\n",
      "Speed: 1.4ms preprocess, 68.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.9ms\n",
      "Speed: 1.4ms preprocess, 71.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.7ms\n",
      "Speed: 1.4ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.4ms\n",
      "Speed: 1.4ms preprocess, 62.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.6ms\n",
      "Speed: 1.4ms preprocess, 61.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.9ms\n",
      "Speed: 1.4ms preprocess, 67.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.1ms\n",
      "Speed: 1.4ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.7ms\n",
      "Speed: 1.5ms preprocess, 63.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.1ms\n",
      "Speed: 1.5ms preprocess, 70.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.4ms\n",
      "Speed: 1.4ms preprocess, 62.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 2.1ms preprocess, 68.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.8ms\n",
      "Speed: 1.4ms preprocess, 62.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.1ms\n",
      "Speed: 1.3ms preprocess, 63.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.2ms\n",
      "Speed: 1.6ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.2ms\n",
      "Speed: 1.8ms preprocess, 66.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 92.6ms\n",
      "Speed: 4.0ms preprocess, 92.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 77.0ms\n",
      "Speed: 1.5ms preprocess, 77.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.2ms\n",
      "Speed: 1.4ms preprocess, 62.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.3ms\n",
      "Speed: 1.4ms preprocess, 71.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.9ms\n",
      "Speed: 1.5ms preprocess, 63.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.3ms\n",
      "Speed: 1.4ms preprocess, 64.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.8ms\n",
      "Speed: 1.6ms preprocess, 65.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.7ms\n",
      "Speed: 1.7ms preprocess, 66.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.6ms\n",
      "Speed: 1.4ms preprocess, 62.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.2ms\n",
      "Speed: 1.4ms preprocess, 62.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.7ms\n",
      "Speed: 1.9ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.8ms\n",
      "Speed: 1.5ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.2ms\n",
      "Speed: 1.5ms preprocess, 71.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.9ms\n",
      "Speed: 1.4ms preprocess, 62.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.4ms\n",
      "Speed: 1.4ms preprocess, 63.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.9ms\n",
      "Speed: 1.5ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.6ms\n",
      "Speed: 3.3ms preprocess, 70.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.7ms\n",
      "Speed: 1.4ms preprocess, 65.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.6ms\n",
      "Speed: 1.5ms preprocess, 67.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 101.8ms\n",
      "Speed: 7.5ms preprocess, 101.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.4ms\n",
      "Speed: 2.0ms preprocess, 74.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.9ms\n",
      "Speed: 1.4ms preprocess, 70.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.5ms\n",
      "Speed: 1.6ms preprocess, 67.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 143.0ms\n",
      "Speed: 1.3ms preprocess, 143.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.8ms\n",
      "Speed: 1.4ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.7ms\n",
      "Speed: 1.4ms preprocess, 68.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.6ms preprocess, 72.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.4ms\n",
      "Speed: 1.4ms preprocess, 63.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.5ms\n",
      "Speed: 1.5ms preprocess, 70.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.4ms\n",
      "Speed: 1.5ms preprocess, 64.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.6ms\n",
      "Speed: 1.6ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.6ms\n",
      "Speed: 1.4ms preprocess, 75.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.8ms\n",
      "Speed: 1.7ms preprocess, 72.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.6ms\n",
      "Speed: 1.6ms preprocess, 72.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.3ms\n",
      "Speed: 1.4ms preprocess, 64.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 88.8ms\n",
      "Speed: 12.0ms preprocess, 88.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.0ms\n",
      "Speed: 1.7ms preprocess, 64.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.8ms\n",
      "Speed: 1.6ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.2ms\n",
      "Speed: 1.6ms preprocess, 83.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.5ms\n",
      "Speed: 1.4ms preprocess, 70.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.1ms\n",
      "Speed: 1.4ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.5ms\n",
      "Speed: 1.5ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.2ms\n",
      "Speed: 1.4ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.7ms\n",
      "Speed: 1.5ms preprocess, 65.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.4ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.0ms\n",
      "Speed: 1.5ms preprocess, 69.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.6ms\n",
      "Speed: 1.5ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.1ms\n",
      "Speed: 1.4ms preprocess, 65.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.3ms\n",
      "Speed: 1.4ms preprocess, 65.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.3ms\n",
      "Speed: 1.5ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.1ms\n",
      "Speed: 1.7ms preprocess, 64.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.9ms\n",
      "Speed: 1.4ms preprocess, 62.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 99.9ms\n",
      "Speed: 2.9ms preprocess, 99.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.4ms\n",
      "Speed: 1.5ms preprocess, 65.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.0ms\n",
      "Speed: 1.5ms preprocess, 69.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 73.0ms\n",
      "Speed: 1.4ms preprocess, 73.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.4ms\n",
      "Speed: 1.4ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.5ms\n",
      "Speed: 1.4ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.1ms\n",
      "Speed: 1.5ms preprocess, 64.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.8ms\n",
      "Speed: 1.4ms preprocess, 66.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.9ms\n",
      "Speed: 1.4ms preprocess, 65.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.5ms\n",
      "Speed: 1.3ms preprocess, 71.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.3ms\n",
      "Speed: 1.4ms preprocess, 63.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.5ms\n",
      "Speed: 1.3ms preprocess, 63.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.5ms\n",
      "Speed: 1.5ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 97.7ms\n",
      "Speed: 3.3ms preprocess, 97.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.0ms\n",
      "Speed: 1.6ms preprocess, 70.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.6ms\n",
      "Speed: 1.4ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.1ms\n",
      "Speed: 1.4ms preprocess, 72.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.6ms\n",
      "Speed: 1.9ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.8ms\n",
      "Speed: 1.8ms preprocess, 70.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.9ms\n",
      "Speed: 2.1ms preprocess, 64.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.2ms\n",
      "Speed: 1.5ms preprocess, 72.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.3ms\n",
      "Speed: 1.4ms preprocess, 65.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.9ms\n",
      "Speed: 1.4ms preprocess, 66.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.0ms\n",
      "Speed: 1.4ms preprocess, 63.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.3ms\n",
      "Speed: 2.2ms preprocess, 71.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.4ms preprocess, 69.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.0ms\n",
      "Speed: 8.3ms preprocess, 102.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.2ms\n",
      "Speed: 1.4ms preprocess, 71.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.0ms\n",
      "Speed: 1.4ms preprocess, 70.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.4ms\n",
      "Speed: 1.5ms preprocess, 70.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.2ms\n",
      "Speed: 1.4ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.4ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.9ms\n",
      "Speed: 1.5ms preprocess, 68.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.2ms\n",
      "Speed: 1.5ms preprocess, 70.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.8ms\n",
      "Speed: 1.5ms preprocess, 83.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.8ms\n",
      "Speed: 1.5ms preprocess, 74.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.5ms\n",
      "Speed: 1.5ms preprocess, 63.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.9ms\n",
      "Speed: 1.4ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.9ms\n",
      "Speed: 1.5ms preprocess, 74.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 104.0ms\n",
      "Speed: 2.8ms preprocess, 104.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.6ms\n",
      "Speed: 1.9ms preprocess, 69.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 1.5ms preprocess, 72.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.5ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.8ms\n",
      "Speed: 1.4ms preprocess, 64.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.7ms\n",
      "Speed: 1.5ms preprocess, 64.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.5ms preprocess, 66.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.9ms\n",
      "Speed: 1.4ms preprocess, 60.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.5ms\n",
      "Speed: 1.7ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 1.5ms preprocess, 68.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.0ms\n",
      "Speed: 1.5ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.3ms\n",
      "Speed: 1.4ms preprocess, 60.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.2ms\n",
      "Speed: 1.4ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 82.7ms\n",
      "Speed: 2.0ms preprocess, 82.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 70.3ms\n",
      "Speed: 1.6ms preprocess, 70.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 102.0ms\n",
      "Speed: 4.2ms preprocess, 102.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.6ms\n",
      "Speed: 1.5ms preprocess, 63.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.6ms\n",
      "Speed: 1.9ms preprocess, 61.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.9ms\n",
      "Speed: 1.4ms preprocess, 67.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 67.0ms\n",
      "Speed: 1.4ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 149.1ms\n",
      "Speed: 1.4ms preprocess, 149.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.3ms\n",
      "Speed: 1.5ms preprocess, 71.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 79.8ms\n",
      "Speed: 1.5ms preprocess, 79.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 79.7ms\n",
      "Speed: 1.6ms preprocess, 79.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.3ms\n",
      "Speed: 1.7ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.1ms\n",
      "Speed: 1.4ms preprocess, 65.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 113.3ms\n",
      "Speed: 3.3ms preprocess, 113.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.6ms\n",
      "Speed: 1.4ms preprocess, 69.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 71.3ms\n",
      "Speed: 1.4ms preprocess, 71.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 74.5ms\n",
      "Speed: 1.4ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.1ms\n",
      "Speed: 1.5ms preprocess, 65.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.8ms\n",
      "Speed: 1.4ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.2ms\n",
      "Speed: 1.6ms preprocess, 66.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.7ms\n",
      "Speed: 1.4ms preprocess, 61.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.4ms\n",
      "Speed: 1.4ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 64.1ms\n",
      "Speed: 1.5ms preprocess, 64.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Average Score for the Video: 4.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Sprint_Start.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Sprint_Start.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/sprint_start/segment_000091.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 9.0018 - mae: 2.6548 - val_loss: 11.0403 - val_mae: 3.1718\n",
      "Epoch 2/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8415 - mae: 1.0687 - val_loss: 8.9501 - val_mae: 2.8419\n",
      "Epoch 3/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5177 - mae: 0.9672 - val_loss: 7.7660 - val_mae: 2.6401\n",
      "Epoch 4/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4378 - mae: 0.9612 - val_loss: 5.5853 - val_mae: 2.1848\n",
      "Epoch 5/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1954 - mae: 0.8617 - val_loss: 4.0846 - val_mae: 1.8142\n",
      "Epoch 6/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0949 - mae: 0.8246 - val_loss: 3.5878 - val_mae: 1.6930\n",
      "Epoch 7/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1045 - mae: 0.8215 - val_loss: 2.6179 - val_mae: 1.4105\n",
      "Epoch 8/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9452 - mae: 0.7457 - val_loss: 2.1790 - val_mae: 1.2786\n",
      "Epoch 9/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8674 - mae: 0.7399 - val_loss: 1.6502 - val_mae: 1.0928\n",
      "Epoch 10/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6802 - mae: 0.6466 - val_loss: 1.5930 - val_mae: 1.0833\n",
      "Epoch 11/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8700 - mae: 0.7343 - val_loss: 1.4080 - val_mae: 1.0140\n",
      "Epoch 12/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7359 - mae: 0.6826 - val_loss: 1.1290 - val_mae: 0.8953\n",
      "Epoch 13/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6100 - mae: 0.6110 - val_loss: 0.7958 - val_mae: 0.7463\n",
      "Epoch 14/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4979 - mae: 0.5454 - val_loss: 0.7935 - val_mae: 0.7534\n",
      "Epoch 15/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5473 - mae: 0.5733 - val_loss: 0.7981 - val_mae: 0.7553\n",
      "Epoch 16/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5365 - mae: 0.5666 - val_loss: 0.7195 - val_mae: 0.7304\n",
      "Epoch 17/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4986 - mae: 0.5386 - val_loss: 0.5791 - val_mae: 0.6279\n",
      "Epoch 18/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4333 - mae: 0.5115 - val_loss: 0.5287 - val_mae: 0.6094\n",
      "Epoch 19/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4785 - mae: 0.5308 - val_loss: 0.4858 - val_mae: 0.5404\n",
      "Epoch 20/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4753 - mae: 0.5364 - val_loss: 0.4300 - val_mae: 0.5262\n",
      "Epoch 21/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4487 - mae: 0.5175 - val_loss: 0.4622 - val_mae: 0.5590\n",
      "Epoch 22/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4009 - mae: 0.4910 - val_loss: 0.3742 - val_mae: 0.4824\n",
      "Epoch 23/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3453 - mae: 0.4468 - val_loss: 0.3684 - val_mae: 0.4781\n",
      "Epoch 24/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3704 - mae: 0.4654 - val_loss: 0.3455 - val_mae: 0.4486\n",
      "Epoch 25/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4259 - mae: 0.5008 - val_loss: 0.4035 - val_mae: 0.4517\n",
      "Epoch 26/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3935 - mae: 0.4743 - val_loss: 0.3388 - val_mae: 0.4033\n",
      "Epoch 27/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4099 - mae: 0.4894 - val_loss: 0.4051 - val_mae: 0.4096\n",
      "Epoch 28/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3830 - mae: 0.4716 - val_loss: 0.3396 - val_mae: 0.4215\n",
      "Epoch 29/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3880 - mae: 0.4780 - val_loss: 0.3108 - val_mae: 0.3743\n",
      "Epoch 30/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3439 - mae: 0.4496 - val_loss: 0.3531 - val_mae: 0.3914\n",
      "Epoch 31/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3234 - mae: 0.4351 - val_loss: 0.3048 - val_mae: 0.3753\n",
      "Epoch 32/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3651 - mae: 0.4745 - val_loss: 0.3254 - val_mae: 0.3942\n",
      "Epoch 33/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3622 - mae: 0.4635 - val_loss: 0.2989 - val_mae: 0.3334\n",
      "Epoch 34/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3483 - mae: 0.4471 - val_loss: 0.4222 - val_mae: 0.4492\n",
      "Epoch 35/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3587 - mae: 0.4610 - val_loss: 0.2814 - val_mae: 0.3376\n",
      "Epoch 36/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3114 - mae: 0.4124 - val_loss: 0.3120 - val_mae: 0.3428\n",
      "Epoch 37/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3750 - mae: 0.4635 - val_loss: 0.3096 - val_mae: 0.3508\n",
      "Epoch 38/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3438 - mae: 0.4466 - val_loss: 0.3228 - val_mae: 0.3648\n",
      "Epoch 39/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3693 - mae: 0.4577 - val_loss: 0.3314 - val_mae: 0.3749\n",
      "Epoch 40/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3148 - mae: 0.4266 - val_loss: 0.3105 - val_mae: 0.3655\n",
      "Epoch 41/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3289 - mae: 0.4282 - val_loss: 0.5215 - val_mae: 0.5221\n",
      "Epoch 42/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3621 - mae: 0.4578 - val_loss: 0.2931 - val_mae: 0.3272\n",
      "Epoch 43/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2869 - mae: 0.3957 - val_loss: 0.3163 - val_mae: 0.3689\n",
      "Epoch 44/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3694 - mae: 0.4523 - val_loss: 0.2868 - val_mae: 0.3355\n",
      "Epoch 45/100\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2819 - mae: 0.3986 - val_loss: 0.3243 - val_mae: 0.3850\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3077 - mae: 0.3781\n",
      "Test Loss: 0.3243, Test MAE: 0.3850\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.6088\n",
      "\n",
      "0: 384x640 1 person, 49.6ms\n",
      "Speed: 0.8ms preprocess, 49.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 384x640 1 person, 49.2ms\n",
      "Speed: 1.1ms preprocess, 49.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Average Score for the Video: 4.50\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Sprint.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Sprint.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/sprint/segment_000671.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at /Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/athletes/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 6.8859 - mae: 2.2983 - val_loss: 10.2453 - val_mae: 3.1130\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5475 - mae: 0.9127 - val_loss: 8.3449 - val_mae: 2.7919\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0948 - mae: 0.8239 - val_loss: 6.8892 - val_mae: 2.5224\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0465 - mae: 0.7848 - val_loss: 5.5357 - val_mae: 2.2234\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8600 - mae: 0.7254 - val_loss: 3.7000 - val_mae: 1.7640\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7552 - mae: 0.6811 - val_loss: 2.7870 - val_mae: 1.4695\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7087 - mae: 0.6776 - val_loss: 1.9117 - val_mae: 1.1644\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7056 - mae: 0.6726 - val_loss: 1.3542 - val_mae: 0.9439\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5535 - mae: 0.5765 - val_loss: 1.1491 - val_mae: 0.8742\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5738 - mae: 0.5823 - val_loss: 0.9416 - val_mae: 0.7965\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5853 - mae: 0.5915 - val_loss: 0.8498 - val_mae: 0.7543\n",
      "Epoch 12/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5445 - mae: 0.5748 - val_loss: 0.8582 - val_mae: 0.7564\n",
      "Epoch 13/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4890 - mae: 0.5479 - val_loss: 0.9500 - val_mae: 0.7958\n",
      "Epoch 14/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5526 - mae: 0.5710 - val_loss: 0.9303 - val_mae: 0.7842\n",
      "Epoch 15/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4969 - mae: 0.5590 - val_loss: 0.8686 - val_mae: 0.7544\n",
      "Epoch 16/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5105 - mae: 0.5475 - val_loss: 0.6939 - val_mae: 0.6677\n",
      "Epoch 17/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4491 - mae: 0.5098 - val_loss: 0.5400 - val_mae: 0.5880\n",
      "Epoch 18/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4553 - mae: 0.5088 - val_loss: 0.4397 - val_mae: 0.5169\n",
      "Epoch 19/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4345 - mae: 0.5030 - val_loss: 0.4066 - val_mae: 0.5004\n",
      "Epoch 20/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4196 - mae: 0.4959 - val_loss: 0.5143 - val_mae: 0.5849\n",
      "Epoch 21/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4397 - mae: 0.5140 - val_loss: 0.4967 - val_mae: 0.5674\n",
      "Epoch 22/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4437 - mae: 0.5135 - val_loss: 0.3813 - val_mae: 0.4681\n",
      "Epoch 23/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4210 - mae: 0.4943 - val_loss: 0.5634 - val_mae: 0.6050\n",
      "Epoch 24/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4433 - mae: 0.5065 - val_loss: 0.3688 - val_mae: 0.4477\n",
      "Epoch 25/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4969 - mae: 0.5341 - val_loss: 0.3622 - val_mae: 0.4582\n",
      "Epoch 26/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3861 - mae: 0.4839 - val_loss: 0.3698 - val_mae: 0.4496\n",
      "Epoch 27/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4835 - mae: 0.5294 - val_loss: 0.3568 - val_mae: 0.4495\n",
      "Epoch 28/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4158 - mae: 0.4875 - val_loss: 0.3779 - val_mae: 0.4792\n",
      "Epoch 29/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4244 - mae: 0.5025 - val_loss: 0.3888 - val_mae: 0.4725\n",
      "Epoch 30/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3920 - mae: 0.4852 - val_loss: 0.5049 - val_mae: 0.5657\n",
      "Epoch 31/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4162 - mae: 0.4918 - val_loss: 0.4383 - val_mae: 0.5144\n",
      "Epoch 32/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3990 - mae: 0.4728 - val_loss: 0.4278 - val_mae: 0.4906\n",
      "Epoch 33/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4131 - mae: 0.4886 - val_loss: 0.3527 - val_mae: 0.4355\n",
      "Epoch 34/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4244 - mae: 0.4979 - val_loss: 0.4403 - val_mae: 0.5160\n",
      "Epoch 35/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3792 - mae: 0.4592 - val_loss: 0.3613 - val_mae: 0.4382\n",
      "Epoch 36/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3946 - mae: 0.4710 - val_loss: 0.4071 - val_mae: 0.4819\n",
      "Epoch 37/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3674 - mae: 0.4532 - val_loss: 0.3589 - val_mae: 0.4369\n",
      "Epoch 38/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3732 - mae: 0.4616 - val_loss: 0.3682 - val_mae: 0.4520\n",
      "Epoch 39/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3513 - mae: 0.4523 - val_loss: 0.3571 - val_mae: 0.4419\n",
      "Epoch 40/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3561 - mae: 0.4479 - val_loss: 0.4450 - val_mae: 0.5113\n",
      "Epoch 41/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3803 - mae: 0.4525 - val_loss: 0.3751 - val_mae: 0.4536\n",
      "Epoch 42/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3880 - mae: 0.4734 - val_loss: 0.3546 - val_mae: 0.4304\n",
      "Epoch 43/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3560 - mae: 0.4407 - val_loss: 0.3695 - val_mae: 0.4478\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3852 - mae: 0.4492 \n",
      "Test Loss: 0.3695, Test MAE: 0.4478\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.3487\n",
      "\n",
      "0: 640x640 1 person, 75.2ms\n",
      "Speed: 2.3ms preprocess, 75.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 83.1ms\n",
      "Speed: 2.1ms preprocess, 83.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.1ms\n",
      "Speed: 1.4ms preprocess, 75.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.8ms\n",
      "Speed: 1.4ms preprocess, 69.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 62.3ms\n",
      "Speed: 1.4ms preprocess, 62.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.7ms\n",
      "Speed: 1.5ms preprocess, 66.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 60.8ms\n",
      "Speed: 1.3ms preprocess, 60.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 2 persons, 59.4ms\n",
      "Speed: 1.5ms preprocess, 59.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 1.4ms preprocess, 66.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.1ms\n",
      "Speed: 1.3ms preprocess, 61.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 63.1ms\n",
      "Speed: 1.4ms preprocess, 63.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "0: 640x640 1 person, 68.5ms\n",
      "Speed: 1.3ms preprocess, 68.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 72.9ms\n",
      "Speed: 1.5ms preprocess, 72.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.7ms\n",
      "Speed: 1.5ms preprocess, 61.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 69.4ms\n",
      "Speed: 1.5ms preprocess, 69.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 84.4ms\n",
      "Speed: 1.4ms preprocess, 84.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 75.7ms\n",
      "Speed: 1.6ms preprocess, 75.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 61.6ms\n",
      "Speed: 1.3ms preprocess, 61.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 (no detections), 70.9ms\n",
      "Speed: 1.4ms preprocess, 70.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 65.7ms\n",
      "Speed: 1.2ms preprocess, 65.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "0: 640x640 1 person, 65.5ms\n",
      "Speed: 1.4ms preprocess, 65.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "0: 640x640 1 person, 59.9ms\n",
      "Speed: 1.4ms preprocess, 59.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Average Score for the Video: 4.88\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "\n",
    "# Load JSON data for scoring model\n",
    "file_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonScore/Verspringen.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the expected number of keypoints (e.g., 17 keypoints, each with x, y, and visibility)\n",
    "expected_keypoints = 17\n",
    "expected_size = expected_keypoints * 2  # Each keypoint has x, y, visibility (3 values)\n",
    "\n",
    "# Extract features (keypoints) and labels (scores)\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "for segment, details in data['segments'].items():\n",
    "    for annotation in details['annotations']:\n",
    "        keypoints = annotation['keypoints']  # Flatten keypoints\n",
    "        normalized_keypoints = []\n",
    "\n",
    "        # Normalize using bbox dimensions\n",
    "        bbox = annotation['bbox']\n",
    "        \n",
    "        # Ensure that keypoints contain a valid number of elements (x, y, and visibility for each keypoint)\n",
    "        if len(keypoints) % 3 == 0:  # Check that each keypoint has x, y, and visibility\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "                y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "                visibility = keypoints[i + 2]  # Keep visibility as-is\n",
    "                normalized_keypoints.extend([x, y, visibility])\n",
    "\n",
    "            # Pad or trim the keypoints list to the expected size\n",
    "            if len(normalized_keypoints) > expected_size:\n",
    "                normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "            elif len(normalized_keypoints) < expected_size:\n",
    "                normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "            features.append(normalized_keypoints)\n",
    "            scores.append(details['score'])\n",
    "        else:\n",
    "            print(f\"Skipping annotation with invalid keypoint length: {len(keypoints)}\")\n",
    "\n",
    "# Convert to NumPy arrays (after ensuring consistency)\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(scores, dtype=np.float32)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler and save the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train)\n",
    "scaler_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/JsonKeypoints/Testing/scoring_scaler7.pkl\"  # Correct path for saving the scaler\n",
    "joblib.dump(scaler, scaler_path)  # Save the scaler\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# Define a more complex model with added LSTM layers for sequential data\n",
    "def create_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Reshape the input to be 3D for LSTM\n",
    "        tf.keras.layers.Reshape((1, input_dim), input_shape=(input_dim,)),  # Reshape to (1, features)\n",
    "        \n",
    "        # LSTM layer for sequential data (even though you have one timestep)\n",
    "        tf.keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dense layers for feature learning\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize and train the model with early stopping\n",
    "model = create_model(X_train.shape[1])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Calculate R² score\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"Verspringen.h5\")\n",
    "\n",
    "# Load YOLO model for pose estimation\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"/Users/alessiacolumban/Desktop/TeamProject-GradindSysAthletes/Athletes/KeypointDetection/exercises/Verspringen/segment_001406.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_count = 0\n",
    "frame_scores = []\n",
    "\n",
    "# Load the saved scaler for normalization of keypoints\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler_scoring = joblib.load(scaler_path)\n",
    "else:\n",
    "    # If the scaler doesn't exist, fit a new one (this case should not happen if you have already saved it)\n",
    "    scaler_scoring = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_scoring.fit(X)  # Use `X` as the keypoints data\n",
    "    joblib.dump(scaler_scoring, scaler_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Step 1: Extract Keypoints\n",
    "    results = pose_model(frame)\n",
    "    if len(results) == 0 or results[0].keypoints is None:\n",
    "        continue\n",
    "\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy().flatten()  # x, y coordinates\n",
    "    if len(keypoints) == 0:\n",
    "        continue  # Skip frames without valid keypoints\n",
    "\n",
    "    # Step 2: Normalize keypoints using bounding box dimensions\n",
    "    bbox = results[0].boxes.xyxy.cpu().numpy()[0]  # x_min, y_min, x_max, y_max\n",
    "    if bbox is None or len(bbox) != 4:\n",
    "        continue  # Skip frames without valid bbox\n",
    "\n",
    "    normalized_keypoints = []\n",
    "    for i in range(0, len(keypoints), 2):  # Assuming keypoints are in x, y pairs\n",
    "        x = keypoints[i] / bbox[2]  # Normalize by width\n",
    "        y = keypoints[i + 1] / bbox[3]  # Normalize by height\n",
    "        normalized_keypoints.extend([x, y])\n",
    "\n",
    "    # Pad or trim the normalized keypoints to match the expected feature size (51 values)\n",
    "    if len(normalized_keypoints) > expected_size:\n",
    "        normalized_keypoints = normalized_keypoints[:expected_size]  # Trim if too long\n",
    "    elif len(normalized_keypoints) < expected_size:\n",
    "        normalized_keypoints += [0] * (expected_size - len(normalized_keypoints))  # Pad if too short\n",
    "\n",
    "    # Step 3: Predict Score using the trained scoring model\n",
    "    normalized_keypoints = np.array([normalized_keypoints], dtype=np.float32)\n",
    "\n",
    "    # Normalize the features using the saved scaler\n",
    "    normalized_keypoints = scaler_scoring.transform(normalized_keypoints)\n",
    "\n",
    "    # Predict score from the model\n",
    "    score = model.predict(normalized_keypoints).flatten()[0]\n",
    "\n",
    "    # Clip the predicted score to ensure it's between 1 and 5\n",
    "    score = np.clip(score, 1, 5)\n",
    "\n",
    "    # Round the score to the nearest 0.5\n",
    "    rounded_score = round(score * 2) / 2\n",
    "    frame_scores.append(rounded_score)\n",
    "\n",
    "    # Step 4: Annotate the frame with the score\n",
    "    cv2.putText(frame, f\"Score: {rounded_score:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    frame_count += 1\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Step 5: Aggregate Results\n",
    "average_score = np.mean(frame_scores) if frame_scores else 0\n",
    "print(f\"Average Score for the Video: {average_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athletes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
